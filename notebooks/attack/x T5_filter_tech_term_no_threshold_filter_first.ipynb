{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfdb9c8",
   "metadata": {},
   "source": [
    "## Setting up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215d7bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:43.608914Z",
     "start_time": "2022-01-14T10:50:39.501031Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f27280a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:44.775695Z",
     "start_time": "2022-01-14T10:50:43.611328Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "from random import choices\n",
    "import os\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import traceback\n",
    "import gc\n",
    "from enum import Enum \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "# from trl.ppo import PPOTrainer\n",
    "#from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "from IPython.core.display import Markdown,display, HTML, Latex\n",
    "import qgrid\n",
    "\n",
    "from verisci.covid import AbstractRetriever, RationaleSelector, LabelPredictor\n",
    "from verisci.evaluate.lib.data import GoldDataset\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2404d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:47.613263Z",
     "start_time": "2022-01-14T10:50:44.778314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/ratulalahy/scifact-paraphrase-T5-evo/e/SCIF3-78\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "nep_run = neptune.init(\n",
    "    project=\"ratulalahy/scifact-paraphrase-T5-evo\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2NWQwMGIyZi1mNzM5LTRiMjEtOTg2MC1mNTc4ODRiMWU2ZGYifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# neptune.create_experiment(name = 'org_refute_gen_sup_threshold_0.7', description = 'Trained with org refute gen support , but having \\\n",
    "#                           threshold grater or equal 0.7. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ef31ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:47.620863Z",
     "start_time": "2022-01-14T10:50:47.617045Z"
    }
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"Scifact_paraphrase_T5_per_evo_general_parasci_sup_to_ref_2_ft_0.0.1\", entity=\"qratulalahy\")\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1645da9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:47.627506Z",
     "start_time": "2022-01-14T10:50:47.623526Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "cur_date_time = datetime.today().strftime('%Y_%m_%d_%H_%M')\n",
    "log_dir = \"../../logs/\"\n",
    "project_name = 'Scifact_paraphrase_T5_scifive_per_evo_filter_tech_mmli_v1'\n",
    "version = '0.0.2'\n",
    "log_file_dir_name = log_dir+project_name+'_'+version+'.log'\n",
    "\n",
    "import logging\n",
    "  \n",
    "#Create and configure logger\n",
    "logging.basicConfig(filename=log_file_dir_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "log_file_fine_tune_callback = '../../logs/log_results.txt'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c342eb42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:47.634077Z",
     "start_time": "2022-01-14T10:50:47.629445Z"
    }
   },
   "source": [
    "import logging\n",
    "  \n",
    "#Create and configure logger\n",
    "logging.basicConfig(filename=log_file_dir_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "log_file_fine_tune_callback = '../../logs/log_results.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256b2fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.056626Z",
     "start_time": "2022-01-14T10:50:47.636215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Endpoint: <samp>https://notify.run/DJ0nGmzotY093BsODY8Z</samp></p>\n",
       "<p>To subscribe, open: <a href=\"https://notify.run/c/DJ0nGmzotY093BsODY8Z\">https://notify.run/c/DJ0nGmzotY093BsODY8Z</a></p>\n",
       "<p>Or scan this QR code:</p>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"222\" width=\"222\" class=\"pyqrcode\"><path transform=\"scale(6)\" stroke=\"#000\" class=\"pyqrline\" d=\"M4 4.5h7m2 0h1m3 0h1m1 0h4m1 0h1m1 0h7m-29 1h1m5 0h1m1 0h2m1 0h8m3 0h1m5 0h1m-29 1h1m1 0h3m1 0h1m2 0h3m2 0h1m3 0h1m1 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m1 0h1m2 0h5m3 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m3 0h1m1 0h1m3 0h1m3 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h1m1 0h1m3 0h2m2 0h1m1 0h1m1 0h1m5 0h1m-29 1h7m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h7m-17 1h1m5 0h3m-21 1h5m1 0h5m1 0h1m1 0h2m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m-28 1h2m1 0h2m3 0h2m3 0h7m1 0h4m3 0h1m-29 1h1m4 0h3m1 0h1m1 0h2m2 0h3m2 0h1m2 0h2m-25 1h1m1 0h1m4 0h1m1 0h3m2 0h1m3 0h2m1 0h2m2 0h1m1 0h1m-28 1h1m1 0h1m1 0h1m1 0h1m2 0h1m1 0h1m1 0h1m1 0h2m8 0h2m-26 1h3m5 0h2m1 0h3m3 0h3m1 0h3m3 0h1m-26 1h4m1 0h1m1 0h1m4 0h3m2 0h3m1 0h3m-25 1h4m2 0h3m1 0h1m2 0h1m3 0h2m2 0h2m2 0h1m-28 1h1m2 0h4m2 0h1m2 0h1m1 0h1m1 0h2m1 0h1m1 0h1m1 0h1m1 0h2m-27 1h1m2 0h3m1 0h1m1 0h2m2 0h2m2 0h3m1 0h4m1 0h1m1 0h1m-29 1h1m2 0h1m1 0h2m4 0h3m1 0h1m6 0h2m2 0h1m-27 1h1m1 0h2m1 0h1m1 0h5m3 0h1m3 0h5m3 0h1m-28 1h1m1 0h2m1 0h2m3 0h2m3 0h3m1 0h6m1 0h3m-21 1h3m1 0h2m3 0h1m2 0h1m3 0h5m-29 1h7m1 0h1m1 0h1m2 0h8m1 0h1m1 0h3m-27 1h1m5 0h1m5 0h2m1 0h1m2 0h3m3 0h1m-25 1h1m1 0h3m1 0h1m1 0h3m1 0h1m1 0h1m1 0h2m2 0h5m1 0h3m-29 1h1m1 0h3m1 0h1m1 0h1m6 0h5m5 0h4m-29 1h1m1 0h3m1 0h1m1 0h1m2 0h2m2 0h1m1 0h1m1 0h1m2 0h6m-28 1h1m5 0h1m1 0h2m1 0h1m2 0h3m2 0h3m3 0h1m1 0h1m-28 1h7m1 0h4m3 0h3m1 0h1m1 0h3m2 0h1\"/></svg>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "Endpoint: https://notify.run/DJ0nGmzotY093BsODY8Z\n",
       "To subscribe, open: https://notify.run/c/DJ0nGmzotY093BsODY8Z\n",
       "Or scan this QR code:\n",
       "\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notify_run import Notify\n",
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6823c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.062538Z",
     "start_time": "2022-01-14T10:50:48.059660Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2902c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.068187Z",
     "start_time": "2022-01-14T10:50:48.065306Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22bd4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.074933Z",
     "start_time": "2022-01-14T10:50:48.069927Z"
    }
   },
   "outputs": [],
   "source": [
    "class ParaphraseTargetDirection(Enum):\n",
    "    org_support_to_gen_refute = 0\n",
    "    org_refute_to_gen_support = 1\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(37)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5cb4bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.084057Z",
     "start_time": "2022-01-14T10:50:48.076786Z"
    }
   },
   "outputs": [],
   "source": [
    "PARAPHRASE_PROJECT_SETTINGS={\n",
    "    'file_and_dirs': {\n",
    "        'file_paraphrased_no_tune_all_model_full' : '../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl', # can be deleted\n",
    "        'file_org_claims_by_scifact' : '../../dfs_generated/scifact/org_claim_ext_label_roberta_large_fever.pkl',\n",
    "    },\n",
    "    'config_scifact' : {\n",
    "        'cls_model_name': '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/label_roberta_large_fever_scifact',\n",
    "        'rationale_model_name': '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/rationale_roberta_large_fever_scifact',\n",
    "        'loc_gold_ds_corpus' : '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/data/corpus.jsonl', \n",
    "        'loc_gold_ds_train' : '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/data/claims_train.jsonl', \n",
    "        'loc_gold_ds_dev' : '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/data/claims_dev.jsonl', \n",
    "\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'paraphrase_model' :\n",
    "    {\n",
    "        'list_potential_paraphrase_models' : \n",
    "            [\n",
    "                {'model_name' : 'parasci_base_no_fine_tune' , 'model_path_or_url' : 'HelloRusk/t5-base-parasci', 'is_selected' : False},\n",
    "                {'model_name' : 'parrot_base_no_fine_tune' , 'model_path_or_url' : 'prithivida/parrot_paraphraser_on_T5', 'is_selected' : False},\n",
    "                {'model_name' : 'parrot_div_base_no_fine_tune' , 'model_path_or_url' : 'prithivida/parrot_paraphraser_on_T5', 'is_selected' : False},\n",
    "                {'model_name' : 'pegasus_base_no_fine_tune' , 'model_path_or_url' : 'tuner007/pegasus_paraphrase', 'is_selected' : False},\n",
    "                {'model_name' : 'paws_base_no_fine_tune' , 'model_path_or_url' : 'Vamsi/T5_Paraphrase_Paws', 'is_selected' : True},\n",
    "                {'model_name' : 'tapaco_base_no_fine_tune' , 'model_path_or_url' : 'hetpandya/t5-base-tapaco', 'is_selected' : False},\n",
    "                {'model_name' : 'sci_five_pubmed' , 'model_path_or_url' : 'razent/SciFive-large-Pubmed_PMC', 'is_selected' : False}\n",
    "            ],\n",
    "        't5_paraphrase_model_params':\n",
    "        {\n",
    "            'max_length':256,\n",
    "            'do_sample':True,\n",
    "            'top_k':50,\n",
    "            'top_p': 0.99,\n",
    "            'repetition_penalty':3.5,\n",
    "            'early_stopping':True,\n",
    "            'num_return_sequences':10\n",
    "        }\n",
    "    },\n",
    "    'entailment_model':\n",
    "    {\n",
    "        'model_path' : 'pytorch/fairseq',\n",
    "        'model_name' : 'roberta.large.mnli',\n",
    "    },\n",
    "    'labels_multi_nli' :\n",
    "    {\n",
    "        0: 'contradiction', \n",
    "        1 : 'neutral', \n",
    "        2 : 'entailment'\n",
    "    },\n",
    "    \n",
    "    'run_settings':\n",
    "    {\n",
    "        'PARAPHRASE_FT_TRAIN_SPLIT' : 0.1,\n",
    "        'PARAPHRASE_FT_DATASET_DIRECTION' : ParaphraseTargetDirection.org_refute_to_gen_support,#ParaphraseTargetDirection.org_support_to_gen_refute,#ParaphraseTargetDirection.org_refute_to_gen_support,\n",
    "        'NUM_OF_EPOCH_REQ_FT' : 2,\n",
    "        'FILTER_BY' : 'TECH_TERMS',\n",
    "        'SIMILARITY_THRESHOLD' : -100\n",
    "        #'CUR_MODEL_NAME_PATHS' : (lambda: [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True])(),\n",
    "    },\n",
    "}\n",
    "\n",
    "CUR_NO_OF_EPOCH_FT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb26e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.088625Z",
     "start_time": "2022-01-14T10:50:48.085888Z"
    }
   },
   "outputs": [],
   "source": [
    "loc_df_scispacy_sentence_word_unq_ner_abr_filtered ='../../dfs_generated/linguistic/df_scispacy_sentence_word_unq_ner_abr_cust_1.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ee541",
   "metadata": {},
   "source": [
    "##  Load paraphrased dataset for selected `paraphrase_models`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e7353a",
   "metadata": {},
   "source": [
    "def gen_paraphrased_dataframe_selected_model_no_fine_tuned(df_org_claim_evid_label, paraphrase_model_name):\n",
    "    \n",
    "    dic_key_gen_sent = paraphrase_model_name+\"_sentences_info\"\n",
    "    for index_df, cur_row in tqdm(df_org_claim_evid_label.iloc[:,:].iterrows(), total=df_org_claim_evid_label.shape[0]):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim\n",
    "\n",
    "        try:\n",
    "            list_pws_sentences = get_t5_pws_gen_sentences(cur_row[\"claim\"])\n",
    "            list_dic_pws_info = []\n",
    "            for cur_pws_sent in tqdm(list_pws_sentences, desc = paraphrase_model_name):\n",
    "                cur_dic_pws_info = get_results_by_gen_claim(cur_pws_sent, dic_info_org_claim)\n",
    "                cur_dic_pws_info[\"model\"] = paraphrase_model_name\n",
    "                list_dic_pws_info.append(cur_dic_pws_info)\n",
    "            \n",
    "            cur_res[dic_key_gen_sent] = list_dic_pws_info\n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception pws genereted claim >>> \")\n",
    "            logging.info(cur_row[\"claim\"])\n",
    "            logging.info(e)\n",
    "    \n",
    "    result_as_proper_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecc8565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.093407Z",
     "start_time": "2022-01-14T10:50:48.090789Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_dataframe_all_model_no_fine_tuned():\n",
    "    return pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_paraphrased_no_tune_all_model_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3e450a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.099158Z",
     "start_time": "2022-01-14T10:50:48.096112Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_dataframe_selected_models(df_all_model_paraphrased, list_model_names):\n",
    "    return df_all_model_paraphrased[df_all_model_paraphrased['model'].isin(list_model_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f440b6",
   "metadata": {},
   "source": [
    "## Prepare dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2322136b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.108736Z",
     "start_time": "2022-01-14T10:50:48.103872Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframes_by_majority_org_claim(df_all_paraphrased_org_claim):\n",
    "    df_all_paraphrased_org_success = df_all_paraphrased_org_claim[df_all_paraphrased_org_claim['org_comment'] == 'success']\n",
    "    \n",
    "    # Select claims with majority\n",
    "    df_paraphrased_org_support_major = df_all_paraphrased_org_success[\n",
    "        df_all_paraphrased_org_success['org_count_support'] > df_all_paraphrased_org_success['org_count_refute']\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df_paraphrased_org_refute_major = df_all_paraphrased_org_success[\n",
    "        df_all_paraphrased_org_success['org_count_support'] < df_all_paraphrased_org_success['org_count_refute']\n",
    "    ]\n",
    "    \n",
    "    return df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_org_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45682cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.113934Z",
     "start_time": "2022-01-14T10:50:48.111087Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_html_tag = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be0e01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.120876Z",
     "start_time": "2022-01-14T10:50:48.116291Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_org_success):\n",
    "    count_org_claim_support_major = len(df_paraphrased_org_support_major['org_claim'].unique())\n",
    "    count_org_claim_refute_major = len(df_paraphrased_org_refute_major['org_claim'].unique())\n",
    "    count_successful_org_claim = len(df_all_paraphrased_org_success['org_claim'].unique())\n",
    "    ## Report majority\n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#0080ff\">' + 'Original Claim Stat for current SciFact model' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim with Support majority : '+str(count_org_claim_support_major)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim with Refute majority : '+str(count_org_claim_refute_major)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim : '+str(count_successful_org_claim)+'</h4>'\n",
    "    display(HTML(tmp_html_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6198d60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.126883Z",
     "start_time": "2022-01-14T10:50:48.123213Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_succesfully_attacked_claim(df_paraphrased_support_major, df_paraphrased_refute_major):\n",
    "    df_org_refute_gen_support = df_paraphrased_refute_major[\n",
    "    df_paraphrased_refute_major['gen_count_support'] > df_paraphrased_refute_major['gen_count_refute']\n",
    "    ]\n",
    "\n",
    "    df_org_support_gen_refute = df_paraphrased_support_major[\n",
    "        df_paraphrased_support_major['gen_count_support'] < df_paraphrased_support_major['gen_count_refute']\n",
    "    ]\n",
    "    \n",
    "    return df_org_support_gen_refute, df_org_refute_gen_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15628496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.133557Z",
     "start_time": "2022-01-14T10:50:48.128826Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch):\n",
    "    \n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#0080ff\">' + 'Succesfully attacked Claim Stat ' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of toal org refute to gen support : '+str(len(df_org_refute_gen_support))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of total org support to gen refute : '+str(len(df_org_support_gen_refute))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique org refute to support : '+str(len(df_org_refute_gen_support['org_claim'].unique()))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique org support to refute : '+str(len(df_org_support_gen_refute['org_claim'].unique()))+'</h4>'\n",
    "    display(HTML(tmp_html_tag))\n",
    "    \n",
    "    dict_no_ft_org_gen_count = {\n",
    "    '# of toal org refute to gen support': len(df_org_refute_gen_support),\n",
    "    '# of total org support to gen refute' : len(df_org_support_gen_refute), \n",
    "    '# of unique org refute to support' : len(df_org_refute_gen_support['org_claim'].unique()),\n",
    "    '# of unique org support to refute' : len(df_org_support_gen_refute['org_claim'].unique()),\n",
    "    }\n",
    "\n",
    "    nep_run['no_ft_org_gen_count'] = dict_no_ft_org_gen_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08bafe0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.139890Z",
     "start_time": "2022-01-14T10:50:48.135612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org_sup_gen_ref'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'org SUP gen REF'.lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95e70563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.149639Z",
     "start_time": "2022-01-14T10:50:48.141675Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_df_filter(df_cur_analyze_filter, name_analyzed_df ,cur_epoch):\n",
    "    \n",
    "    name_analyzed_df_formatted = name_analyzed_df.lower().replace(' ', '_')\n",
    "    num_mlnli_ent_org_gen = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment'])\n",
    "    num_mlnli_ent_gen_org = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment'])\n",
    "    num_mlnli_ent_both = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment')])\n",
    "    num_passed_ner_abr_filter_ic = len(df_cur_analyze_filter[df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True])\n",
    "    \n",
    "    num_both_ent_ner_passed = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True)\n",
    "    ])\n",
    "    #unique \n",
    "    num_unique_mlnli_ent_org_gen = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment']['org_claim'].unique())\n",
    "    \n",
    "    num_unique_mlnli_ent_both = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment')]['org_claim'].unique())\n",
    "    \n",
    "    num_unique_both_ent_ner_passed = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True)\n",
    "    ]['org_claim'].unique())\n",
    "    \n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#004f11\">' + 'Filtered for '+name_analyzed_df+' ' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_org_gen : '+str(num_mlnli_ent_org_gen)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_gen_org : '+str(num_mlnli_ent_gen_org)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_both : '+str(num_mlnli_ent_both)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# passed_ner_abr_filter_ic : '+str(num_passed_ner_abr_filter_ic)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# both_ent_ner_passed : '+str(num_both_ent_ner_passed)+'</h4>'\n",
    "    \n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_mlnli_ent_org_gen : '+str(num_unique_mlnli_ent_org_gen)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_mlnli_ent_both : '+str(num_unique_mlnli_ent_both)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_both_ent_ner_passed : '+str(num_unique_both_ent_ner_passed)+'</h4>'\n",
    "    display(HTML(tmp_html_tag))\n",
    "    \n",
    "    dict_no_ft_org_gen_count = {\n",
    "    name_analyzed_df_formatted+'_mlnli_ent_org_gen': num_mlnli_ent_org_gen,\n",
    "    name_analyzed_df_formatted+'_mlnli_ent_gen_org' : num_mlnli_ent_gen_org, \n",
    "    name_analyzed_df_formatted+'_mlnli_ent_both' : num_mlnli_ent_both,\n",
    "    name_analyzed_df_formatted+'_passed_ner_abr_filter_ic' : num_passed_ner_abr_filter_ic,\n",
    "    name_analyzed_df_formatted+'_both_ent_ner_passed' : num_both_ent_ner_passed,\n",
    "    name_analyzed_df_formatted+'_unique_mlnli_ent_org_gen' : num_unique_mlnli_ent_org_gen,\n",
    "    name_analyzed_df_formatted+'_unique_mlnli_ent_both' : num_unique_mlnli_ent_both,\n",
    "    name_analyzed_df_formatted+'_unique_both_ent_ner_passed' : num_unique_both_ent_ner_passed,\n",
    "    }\n",
    "\n",
    "    nep_run['no_ft_org_gen_count'] = dict_no_ft_org_gen_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c529f",
   "metadata": {},
   "source": [
    "## Load Abbraviation NER dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fcef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-01T02:09:42.609626Z",
     "start_time": "2022-01-01T02:09:42.607487Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95e5ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.156391Z",
     "start_time": "2022-01-14T10:50:48.151138Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered = pd.read_pickle(loc_df_scispacy_sentence_word_unq_ner_abr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2abba92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.167495Z",
     "start_time": "2022-01-14T10:50:48.160473Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text_stripped'] = df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text'].apply(lambda x: re.sub('[^a-z]+', ' ', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25108f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.187428Z",
     "start_time": "2022-01-14T10:50:48.169854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_text</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>ner_model</th>\n",
       "      <th>claim</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "      <th>org_label</th>\n",
       "      <th>list_rationales</th>\n",
       "      <th>data_source</th>\n",
       "      <th>ner_text_stripped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK</td>\n",
       "      <td>GENE_OR_GENE_PRODUCT</td>\n",
       "      <td>en_ner_bionlp13cg_md</td>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PrP</td>\n",
       "      <td>GENE_OR_GENE_PRODUCT</td>\n",
       "      <td>en_ner_bionlp13cg_md</td>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "      <td>prp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genomes</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>genomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genetic sequence</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>genetic sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>variants</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>139</td>\n",
       "      <td>147</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>variants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>A20</td>\n",
       "      <td>PROTEIN</td>\n",
       "      <td>en_ner_jnlpba_md</td>\n",
       "      <td>siRNA knockdown of A20 slows tumor progression...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The tumorigenic potential of GSCs was decreas...</td>\n",
       "      <td>train</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>tumor</td>\n",
       "      <td>CANCER</td>\n",
       "      <td>en_ner_bionlp13cg_md</td>\n",
       "      <td>siRNA knockdown of A20 slows tumor progression...</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The tumorigenic potential of GSCs was decreas...</td>\n",
       "      <td>train</td>\n",
       "      <td>tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>murine</td>\n",
       "      <td>TAXON</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>siRNA knockdown of A20 slows tumor progression...</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The tumorigenic potential of GSCs was decreas...</td>\n",
       "      <td>train</td>\n",
       "      <td>murine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>Î²-sheet</td>\n",
       "      <td>PROTEIN</td>\n",
       "      <td>en_ner_jnlpba_md</td>\n",
       "      <td>Î²-sheet opening occurs during pleurotolysin po...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The major conformational changes in PlyB are ...</td>\n",
       "      <td>train</td>\n",
       "      <td>sheet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>pleurotolysin</td>\n",
       "      <td>PROTEIN</td>\n",
       "      <td>en_ner_jnlpba_md</td>\n",
       "      <td>Î²-sheet opening occurs during pleurotolysin po...</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The major conformational changes in PlyB are ...</td>\n",
       "      <td>train</td>\n",
       "      <td>pleurotolysin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1729 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ner_text             ner_label             ner_model  \\\n",
       "0                   UK  GENE_OR_GENE_PRODUCT  en_ner_bionlp13cg_md   \n",
       "1                  PrP  GENE_OR_GENE_PRODUCT  en_ner_bionlp13cg_md   \n",
       "2              genomes                    SO       en_ner_craft_md   \n",
       "3     genetic sequence                    SO       en_ner_craft_md   \n",
       "4             variants                    SO       en_ner_craft_md   \n",
       "...                ...                   ...                   ...   \n",
       "1724               A20               PROTEIN      en_ner_jnlpba_md   \n",
       "1725             tumor                CANCER  en_ner_bionlp13cg_md   \n",
       "1726            murine                 TAXON       en_ner_craft_md   \n",
       "1727           Î²-sheet               PROTEIN      en_ner_jnlpba_md   \n",
       "1728     pleurotolysin               PROTEIN      en_ner_jnlpba_md   \n",
       "\n",
       "                                                  claim start_char end_char  \\\n",
       "0     1 in 5 million in UK have abnormal PrP positiv...         18       20   \n",
       "1     1 in 5 million in UK have abnormal PrP positiv...         35       38   \n",
       "2     1,000 genomes project enables mapping of genet...          6       13   \n",
       "3     1,000 genomes project enables mapping of genet...         41       57   \n",
       "4     1,000 genomes project enables mapping of genet...        139      147   \n",
       "...                                                 ...        ...      ...   \n",
       "1724  siRNA knockdown of A20 slows tumor progression...         19       22   \n",
       "1725  siRNA knockdown of A20 slows tumor progression...         29       34   \n",
       "1726  siRNA knockdown of A20 slows tumor progression...         61       77   \n",
       "1727  Î²-sheet opening occurs during pleurotolysin po...          0        7   \n",
       "1728  Î²-sheet opening occurs during pleurotolysin po...         30       48   \n",
       "\n",
       "     org_label                                    list_rationales data_source  \\\n",
       "0      REFUTES  [RESULTS Of the 32,441 appendix samples 16 wer...       train   \n",
       "1      REFUTES  [RESULTS Of the 32,441 appendix samples 16 wer...       train   \n",
       "2     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "3     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "4     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "...        ...                                                ...         ...   \n",
       "1724  SUPPORTS  [The tumorigenic potential of GSCs was decreas...       train   \n",
       "1725  SUPPORTS  [The tumorigenic potential of GSCs was decreas...       train   \n",
       "1726  SUPPORTS  [The tumorigenic potential of GSCs was decreas...       train   \n",
       "1727  SUPPORTS  [The major conformational changes in PlyB are ...       train   \n",
       "1728  SUPPORTS  [The major conformational changes in PlyB are ...       train   \n",
       "\n",
       "     ner_text_stripped  \n",
       "0                   uk  \n",
       "1                  prp  \n",
       "2              genomes  \n",
       "3     genetic sequence  \n",
       "4             variants  \n",
       "...                ...  \n",
       "1724                a   \n",
       "1725             tumor  \n",
       "1726            murine  \n",
       "1727             sheet  \n",
       "1728     pleurotolysin  \n",
       "\n",
       "[1729 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24cd6abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:54:43.868737Z",
     "start_time": "2021-12-20T01:54:42.026813Z"
    }
   },
   "source": [
    "for cur_tmp in df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text'].values:\n",
    "    print(cur_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941162a",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07b484",
   "metadata": {},
   "source": [
    "### Setting up Fine tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fecb2011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.192401Z",
     "start_time": "2022-01-14T10:50:48.188985Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_fine_tune, val_fine_tune = train_test_split(df_paraphrase_fine_tune_dataset[['org_claim', 'gen_claim']], \n",
    "#                                                   test_size=paraphraser_split_size)\n",
    "\n",
    "def get_train_test_dataset(df_to_be_splitted, split_size): \n",
    "    df_train, df_val = train_test_split(df_to_be_splitted, \n",
    "                                                      test_size=split_size)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_val.reset_index(drop=True, inplace=True)\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd7679a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.201070Z",
     "start_time": "2022-01-14T10:50:48.194174Z"
    }
   },
   "outputs": [],
   "source": [
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, target_dataframe, max_len=512, truncation=True):\n",
    "        #self.path = os.path.join(data_dir, type_path + '.csv')\n",
    "\n",
    "        self.source_column = \"org_claim\"\n",
    "        self.target_column = \"gen_claim\"\n",
    "        self.data = target_dataframe#pd.read_csv(self.path)\n",
    "        #print(self.data)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
    "\n",
    "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
    "            target = target + \" </s>\"\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8c58239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.205486Z",
     "start_time": "2022-01-14T10:50:48.202822Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, target_dataframe, args):\n",
    "    return ParaphraseDataset(tokenizer=tokenizer, target_dataframe = target_dataframe,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0af1167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.221960Z",
     "start_time": "2022-01-14T10:50:48.207812Z"
    }
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        # Calling the super constructer\n",
    "        super(T5FineTuner,self).__init__()\n",
    "\n",
    "        self.hparams.update(vars(hparams))\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None):        \n",
    "        return self.model(input_ids, attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                labels=labels,)\n",
    "     \n",
    "    def is_logger(self):\n",
    "        return True\n",
    "        #return self.trainer.proc_rank <= 0        \n",
    "    \n",
    "    def _step(self, batch):\n",
    "        labels = batch[\"target_ids\"]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100 #########\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return None#{\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]    \n",
    "    \n",
    "#     def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=None):\n",
    "#         if self.trainer.use_tpu:\n",
    "#             print('why tpu!')\n",
    "#             xm.optimizer_step(optimizer)\n",
    "#         else:\n",
    "#             print('here!')\n",
    "#             optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         self.lr_scheduler.step()    \n",
    "     \n",
    "    def optimizer_step(self, epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None,\n",
    "                       optimizer_closure=None, on_tpu=None, using_native_amp=None, using_lbfgs=None):\n",
    "        optimizer.step(closure=optimizer_closure) # remove 'closure=optimizer_closure' here\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, target_dataframe=self.hparams.df_train, args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, target_dataframe=self.hparams.df_val, args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db259cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.229304Z",
     "start_time": "2022-01-14T10:50:48.224078Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            output_test_results_file = log_file_fine_tune_callback\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe0bd76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.236934Z",
     "start_time": "2022-01-14T10:50:48.231298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "class FineTuneHyperParams:\n",
    "    def __init__(self,model_name_path, num_train_epochs, df_train, df_val, df_train_val):\n",
    "        self.args_dict_fine_tune = dict(\n",
    "            #data_dir='./tmp_data/', # path for data files\n",
    "            #output_dir='./tmp_data/', # path to save the checkpoints\n",
    "            #temp_train_file_name = 'train.csv',\n",
    "            #temp_validation_file_name = 'val.csv',\n",
    "            #temp_train_val_file_name = 'all.csv',\n",
    "            df_train = df_train,\n",
    "            df_val = df_val,\n",
    "            df_train_val = df_train_val,\n",
    "            model_name_or_path= model_name_path,#'HelloRusk/t5-base-parasci',\n",
    "            tokenizer_name_or_path= model_name_path,#'HelloRusk/t5-base-parasci',\n",
    "            max_seq_length=512,\n",
    "            learning_rate=3e-4,\n",
    "            weight_decay=0.0,\n",
    "            adam_epsilon=1e-8,\n",
    "            warmup_steps=0,\n",
    "            train_batch_size=4,\n",
    "            eval_batch_size=4,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            gradient_accumulation_steps=16,\n",
    "            n_gpu=1,\n",
    "            early_stop_callback=False,\n",
    "            fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "            opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "            max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "            seed=37,\n",
    "        )\n",
    "\n",
    "        self.args_fine_tune_ns = argparse.Namespace(**self.args_dict_fine_tune)\n",
    "\n",
    "        self.checkpoint_callback_fine_tune = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=5,\n",
    "            verbose=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\")\n",
    "\n",
    "        self.train_params_fine_tune = dict(\n",
    "            accumulate_grad_batches=self.args_fine_tune_ns.gradient_accumulation_steps,\n",
    "            gpus=self.args_fine_tune_ns.n_gpu,\n",
    "            max_epochs=self.args_fine_tune_ns.num_train_epochs,\n",
    "            #early_stop_callback=False, #\n",
    "            precision=32,\n",
    "            #amp_level=self.args_fine_tune_ns.opt_level, #\n",
    "            gradient_clip_val=self.args_fine_tune_ns.max_grad_norm,\n",
    "            #logger=wandb_logger,\n",
    "            callbacks=[self.checkpoint_callback_fine_tune, LoggingCallback()],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c9407",
   "metadata": {},
   "source": [
    "## Scifact Functinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2f14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249e379e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-26T02:58:23.723272Z",
     "start_time": "2021-12-26T02:58:23.720406Z"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7258213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.242569Z",
     "start_time": "2022-01-14T10:50:48.238945Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_claim_label_from_jsonl(dataset_jsonl):\n",
    "    claim_label_list_train = []\n",
    "\n",
    "\n",
    "    for cur_claim in dataset_jsonl:\n",
    "        claim_txt = cur_claim.claim\n",
    "\n",
    "        for doc_id, evidence in cur_claim.evidence.items():\n",
    "\n",
    "            ev_doc = cur_claim.release.corpus.get_document(doc_id)\n",
    "\n",
    "            claim_label = evidence.label.name\n",
    "\n",
    "            tmp_dic = {\"claim\" : claim_txt, \"label\" : claim_label}\n",
    "\n",
    "            claim_label_list_train.append(tmp_dic)\n",
    "    return claim_label_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9e198ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.249077Z",
     "start_time": "2022-01-14T10:50:48.244575Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_claim_label_evidence_from_jsonl(dataset_jsonl, source):\n",
    "    claim_label_list_train = []\n",
    "\n",
    "\n",
    "    for cur_claim in dataset_jsonl:\n",
    "        claim_txt = cur_claim.claim\n",
    "\n",
    "        for doc_id, evidence in cur_claim.evidence.items():\n",
    "\n",
    "            ev_doc = claim_train.release.corpus.get_document(doc_id)\n",
    "\n",
    "            claim_label = evidence.label.name\n",
    "            \n",
    "            list_rationales = []\n",
    "            for i, sents in enumerate(evidence.rationales):\n",
    "                list_rationales = [sent for i, sent in enumerate(ev_doc.sentences) if i in sents]\n",
    "\n",
    "            tmp_dic = {\"claim\" : claim_txt, \"label\" : claim_label, \"list_rationales\" :list_rationales, \"source\" :source}\n",
    "\n",
    "            claim_label_list_train.append(tmp_dic)\n",
    "    return claim_label_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fec0f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.739136Z",
     "start_time": "2022-01-14T10:50:48.251049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 39: A diminished ovarian reserve does not solely indicate infertility in an a priori non-infertile population.\n",
      "\n",
      "Evidence sets:\n",
      "\n",
      "####################\n",
      "\n",
      "13497630: SUPPORTS\n",
      "Set 0:\n",
      "\t- After adjusting for age, body mass index, race, current smoking status, and recent hormonal contraceptive use, women with low AMH values (<0.7 ng/mL [n = 84]) did not have a significantly different predicted probability of conceiving by 6 cycles of attempt (65%; 95% CI, 50%-75%) compared with women (n = 579) with normal values (62%; 95% CI, 57%-66%) or by 12 cycles of attempt (84% [95% CI, 70%-91%] vs 75% [95% CI, 70%-79%], respectively).\n",
      "Set 1:\n",
      "\t- Women with high serum FSH values (>10 mIU/mL [n = 83]) did not have a significantly different predicted probability of conceiving after 6 cycles of attempt (63%; 95% CI, 50%-73%) compared with women (n = 654) with normal values (62%; 95% CI, 57%-66%) or after 12 cycles of attempt (82% [95% CI, 70%-89%] vs 75% [95% CI, 70%-78%], respectively).\n",
      "Set 2:\n",
      "\t- Women with high urinary FSH values (>11.5 mIU/mg creatinine [n = 69]) did not have a significantly different predicted probability of conceiving after 6 cycles of attempt (61%; 95% CI, 46%-74%) compared with women (n = 660) with normal values (62%; 95% CI, 58%-66%) or after 12 cycles of attempt (70% [95% CI, 54%-80%] vs 76% [95% CI, 72%-80%], respectively).\n",
      "Set 3:\n",
      "\t- Inhibin B levels (n = 737) were not associated with the probability of conceiving in a given cycle (hazard ratio per 1-pg/mL increase, 0.999; 95% CI, 0.997-1.001).\n",
      "Set 4:\n",
      "\t- Conclusions and Relevance Among women aged 30 to 44 years without a history of infertility who had been trying to conceive for 3 months or less, biomarkers indicating diminished ovarian reserve compared with normal ovarian reserve were not associated with reduced fertility.\n"
     ]
    }
   ],
   "source": [
    "ds_train = GoldDataset(PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_corpus'],\n",
    "                       PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_train'])\n",
    "claim_train = ds_train.get_claim(39)\n",
    "claim_train.pretty_print()\n",
    "\n",
    "dic_train = get_claim_label_evidence_from_jsonl(ds_train, source = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "098083b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.963508Z",
     "start_time": "2022-01-14T10:50:48.741047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 42: A high microerythrocyte count raises vulnerability to severe anemia in homozygous alpha (+)- thalassemia trait subjects.\n",
      "\n",
      "Evidence sets:\n",
      "\n",
      "####################\n",
      "\n",
      "18174210: REFUTES\n",
      "Set 0:\n",
      "\t- Individuals homozygous for alpha(+)-thalassaemia have microcytosis and an increased erythrocyte count.\n",
      "\t- We estimated that the haematological profile in children homozygous for alpha(+)-thalassaemia reduces the risk of SMA during acute malaria compared to children of normal genotype (relative risk 0.52; 95% confidence interval [CI] 0.24-1.12, p = 0.09).   \n",
      "\n",
      "Set 1:\n",
      "\t- CONCLUSIONS The increased erythrocyte count and microcytosis in children homozygous for alpha(+)-thalassaemia may contribute substantially to their protection against SMA.\n"
     ]
    }
   ],
   "source": [
    "ds_valid = GoldDataset(PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_corpus'],\n",
    "                       PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_dev'])\n",
    "claim_valid = ds_valid.get_claim(42)\n",
    "claim_valid.pretty_print()\n",
    "\n",
    "dic_valid = get_claim_label_evidence_from_jsonl(ds_valid, source = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d35d8f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.972810Z",
     "start_time": "2022-01-14T10:50:48.965973Z"
    }
   },
   "outputs": [],
   "source": [
    "df_claim_evid_label = pd.concat([pd.DataFrame(dic_train), pd.DataFrame(dic_valid)], ignore_index=True)\n",
    "\n",
    "#df_claim_evid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10f77b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.987234Z",
     "start_time": "2022-01-14T10:50:48.974938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>list_rationales</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32% of liver transplantation programs required...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Policies requiring discontinuation of methado...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40mg/day dosage of folic acid and 2mg/day dosa...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[CONCLUSION Treatment with high doses of folic...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Although disorder severity was correlated wit...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A T helper 2 cell (Th2) environment impedes di...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Thus, in Lyn(-/-) mice, basophils and IgE aut...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Women with a higher birth weight are more like...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Increased risk of breast cancer was noted wit...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Women with a higher birth weight are more like...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[RESULTS We found that heavier birth weights w...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>aPKCz causes tumour enhancement by affecting g...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Taken together, this demonstrates that PKCÎ¶ i...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>cSMAC formation enhances weak ligand signalling.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[This conclusion was supported by experiments ...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>mTORC2 regulates intracellular cysteine levels...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[mTORC2 phosphorylates serine 26 at the cytoso...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 claim     label  \\\n",
       "0    1 in 5 million in UK have abnormal PrP positiv...   REFUTES   \n",
       "1    32% of liver transplantation programs required...  SUPPORTS   \n",
       "2    40mg/day dosage of folic acid and 2mg/day dosa...  SUPPORTS   \n",
       "3    76-85% of people with severe mental disorder r...  SUPPORTS   \n",
       "4    A T helper 2 cell (Th2) environment impedes di...   REFUTES   \n",
       "..                                                 ...       ...   \n",
       "768  Women with a higher birth weight are more like...  SUPPORTS   \n",
       "769  Women with a higher birth weight are more like...  SUPPORTS   \n",
       "770  aPKCz causes tumour enhancement by affecting g...   REFUTES   \n",
       "771   cSMAC formation enhances weak ligand signalling.  SUPPORTS   \n",
       "772  mTORC2 regulates intracellular cysteine levels...  SUPPORTS   \n",
       "\n",
       "                                       list_rationales source  \n",
       "0    [RESULTS Of the 32,441 appendix samples 16 wer...  train  \n",
       "1    [Policies requiring discontinuation of methado...  train  \n",
       "2    [CONCLUSION Treatment with high doses of folic...  train  \n",
       "3    [Although disorder severity was correlated wit...  train  \n",
       "4    [Thus, in Lyn(-/-) mice, basophils and IgE aut...  train  \n",
       "..                                                 ...    ...  \n",
       "768  [Increased risk of breast cancer was noted wit...    dev  \n",
       "769  [RESULTS We found that heavier birth weights w...    dev  \n",
       "770  [Taken together, this demonstrates that PKCÎ¶ i...    dev  \n",
       "771  [This conclusion was supported by experiments ...    dev  \n",
       "772  [mTORC2 phosphorylates serine 26 at the cytoso...    dev  \n",
       "\n",
       "[773 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claim_evid_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed2043",
   "metadata": {},
   "source": [
    "### Scifact Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc9d048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:50:48.995661Z",
     "start_time": "2022-01-14T10:50:48.989379Z"
    }
   },
   "outputs": [],
   "source": [
    "class ArgsScifact:\n",
    "    def __init__(self, claim):\n",
    "        self.claim = claim\n",
    "        self.report_file = \"/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/results/covid/report\" #not needed\n",
    "        self.n_documents = 100\n",
    "        self.rationale_selection_method = \"topk\"\n",
    "        self.output_format = \"markdown\"\n",
    "        self.rationale_threshold = 0.5\n",
    "        self.label_threshold = 0.5\n",
    "        self.keep_nei = False\n",
    "        self.full_abstract = True\n",
    "        self.verbose = True\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        ##\n",
    "class PretrainedModelsForScifact:\n",
    "    def __init__(self, args):\n",
    "        if args.device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(args.device)\n",
    "            \n",
    "        #self.rationale_selection_model = '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/rationale_roberta_large_scifact'\n",
    "        self.rationale_selection_model = PARAPHRASE_PROJECT_SETTINGS['config_scifact']['rationale_model_name']\n",
    "        self.label_prediction_model = PARAPHRASE_PROJECT_SETTINGS['config_scifact']['cls_model_name']\n",
    "        self.abstract_retriever = AbstractRetriever()\n",
    "        self.rationale_selector = RationaleSelector(self.rationale_selection_model,\n",
    "                                               args.rationale_selection_method,\n",
    "                                               args.rationale_threshold,\n",
    "                                               self.device)\n",
    "        self.label_predictor = LabelPredictor(self.label_prediction_model,\n",
    "                                         args.keep_nei,\n",
    "                                         args.label_threshold,\n",
    "                                         self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1fa2fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:03.641473Z",
     "start_time": "2022-01-14T10:50:48.997735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/rationale_roberta_large_fever_scifact were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/label_roberta_large_fever_scifact were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args_sci = ArgsScifact(\"\")\n",
    "\n",
    "pretrained_models_config = PretrainedModelsForScifact(args_sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ad4cd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:03.653477Z",
     "start_time": "2022-01-14T10:51:03.644738Z"
    }
   },
   "outputs": [],
   "source": [
    "log_failed_claim = []\n",
    "def inference(args, pretraind_models_config):\n",
    "\n",
    "    try:\n",
    "#         if args.verbose:\n",
    "#         print(\"Retrieving abstracts. inference > \", args.claim)\n",
    "        results = pretraind_models_config.abstract_retriever(args.claim, k=args.n_documents)\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'abstract retrival', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "        #print(\"abstract_retriever >> \", results)\n",
    "\n",
    "#         if args.verbose:\n",
    "#             print(\"Selecting rationales. inference > \", args.claim)\n",
    "        results = pretraind_models_config.rationale_selector(args.claim, results)\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'Rationale selection', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "#         if args.verbose:\n",
    "#             print(\"Label predictions. inference > \", args.claim)\n",
    "        results = pretraind_models_config.label_predictor(args.claim, results)\n",
    "\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'Label Prediction', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "        results.sort(key=lambda r: r['label_confidence'], reverse=True)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"Exception :: Inference cant retrive info for >> \", args.claim)\n",
    "        print(sys.exc_info()[0])\n",
    "        print(traceback.format_exc())\n",
    "        temp_dic = {'failed_in' : sys.exc_info()[0], 'claim': args.claim}\n",
    "        log_failed_claim.append(temp_dic)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0456c634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:03.662133Z",
     "start_time": "2022-01-14T10:51:03.655891Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_result(result, full_abstract):\n",
    "    all_msg = \"\"\n",
    "    all_msg = f\"#### [{result['title']}]({result['url']}) \\n\"\n",
    "    #print(msg, file=f)\n",
    "    #all_msg = all_msg+msg\n",
    "    ev_scores = [f\"{x:0.2f}\" for x in result[\"evidence_confidence\"]]\n",
    "    ev_scores = \", \".join(ev_scores)\n",
    "    if result['label'].lower() == \"support\":\n",
    "        msg = f\"ðŸŸ© **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    elif result['label'].lower() == \"refute\":\n",
    "        msg = f\"ðŸŸ¥ **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    else:\n",
    "        msg = f\"âº **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    #print(msg, file=f)\n",
    "    all_msg = all_msg+msg \n",
    "    \n",
    "    for i, line in enumerate(result[\"abstract\"]):\n",
    "        # If we're showing the full abstract, show evidence in green.\n",
    "        if full_abstract:\n",
    "            if result['label'].lower() == \"support\":\n",
    "                msg = (f\"- <span style='color:green'>{line}</span>\"\n",
    "                       if i in result[\"evidence\"]\n",
    "                       else f\"- {line}\")\n",
    "            elif result['label'].lower() == \"refute\":\n",
    "                msg = (f\"- <span style='color:red'>{line}</span>\"\n",
    "                       if i in result[\"evidence\"]\n",
    "                       else f\"- {line}\")                \n",
    "            #print(msg, file=f)\n",
    "            all_msg = all_msg+msg + \" \\n\"\n",
    "        else:\n",
    "            if i in result[\"evidence\"]:\n",
    "                msg = f\"- {line}\"\n",
    "                #print(msg, file=f)\n",
    "                all_msg = all_msg+msg + \" \\n\" \n",
    "    \n",
    "    #print(file=f)\n",
    "    #print(40 * \"-\", file=f)\n",
    "    #print(file=f)\n",
    "    all_msg = all_msg+msg \n",
    "    return all_msg + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96b77196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:03.669996Z",
     "start_time": "2022-01-14T10:51:03.664242Z"
    }
   },
   "outputs": [],
   "source": [
    "def export(args, results):\n",
    "    all_msg = \"\"\n",
    "    claim = args.claim\n",
    "    #report_file = args.report_file\n",
    "    #f = open(f\"{report_file}.md\", \"w\")\n",
    "    msg = f\"### Claim \\n > **{claim}** \\n \"\n",
    "    #print(msg, file=f)\n",
    "    #print(file=f)\n",
    "    all_msg = all_msg +msg\n",
    "    \n",
    "    #support_confs = [], refute_confs = []\n",
    "    confs = []\n",
    "    for result in results:\n",
    "        if result['label'].lower() == \"support\":\n",
    "            tmp_dic = {'label' : 'Support', 'label_confidence' : result[\"label_confidence\"], \"no_of_evidence\" : len(result['evidence_confidence'])}\n",
    "            confs.append(tmp_dic)\n",
    "        elif result['label'].lower() == \"refute\":\n",
    "            tmp_dic = {'label' : 'Refute', 'label_confidence' : -result[\"label_confidence\"], \"no_of_evidence\" : len(result['evidence_confidence'])}\n",
    "            confs.append(tmp_dic)\n",
    "        \n",
    "    \n",
    "    tpm_df = pd.DataFrame(confs)\n",
    "    #HTML(tpm_df.style.bar(align='mid', color=['#d65f5f', '#5fba7d']))\n",
    "    display(HTML(tpm_df.style.bar(subset=[\"label_confidence\"], align='mid', color=['#ffa1a1', '#bfffcf']).render()))\n",
    "    \n",
    "    msg = \"### Evidence \\n \"\n",
    "    all_msg = all_msg +msg\n",
    "    for result in results:\n",
    "        cur_msg = write_result(result, args.full_abstract)\n",
    "        all_msg = all_msg +cur_msg+\"\\n\"\n",
    "\n",
    "    return all_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfa5914e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:09.775467Z",
     "start_time": "2022-01-14T10:51:03.672032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7850c_row0_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 100.0%, transparent 100.0%);\n",
       "}\n",
       "#T_7850c_row1_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 92.9%, transparent 92.9%);\n",
       "}\n",
       "#T_7850c_row2_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 78.5%, transparent 78.5%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7850c_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >label</th>\n",
       "      <th class=\"col_heading level0 col1\" >label_confidence</th>\n",
       "      <th class=\"col_heading level0 col2\" >no_of_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7850c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7850c_row0_col0\" class=\"data row0 col0\" >Support</td>\n",
       "      <td id=\"T_7850c_row0_col1\" class=\"data row0 col1\" >0.69</td>\n",
       "      <td id=\"T_7850c_row0_col2\" class=\"data row0 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7850c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7850c_row1_col0\" class=\"data row1 col0\" >Support</td>\n",
       "      <td id=\"T_7850c_row1_col1\" class=\"data row1 col1\" >0.64</td>\n",
       "      <td id=\"T_7850c_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7850c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7850c_row2_col0\" class=\"data row2 col0\" >Support</td>\n",
       "      <td id=\"T_7850c_row2_col1\" class=\"data row2 col1\" >0.54</td>\n",
       "      <td id=\"T_7850c_row2_col2\" class=\"data row2 col2\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claim \n",
       " > **ART substantially reduces infectiveness of HIV-positive people.** \n",
       " ### Evidence \n",
       " #### [Autonomous Targeting of Infectious Superspreaders Using Engineered Transmissible Therapies](https://api.semanticscholar.org/10.1371/journal.pcbi.1002015) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.69, evidence scores=0.11, 0.06, 0.01)\n",
       "- Infectious disease treatments, both pharmaceutical and vaccine, face three universal challenges: the difficulty of targeting treatments to high-risk â€˜superspreaderâ€™ populations who drive the great majority of disease spread, behavioral barriers in the host population (such as poor compliance and risk disinhibition), and the evolution of pathogen resistance. \n",
       "- Here, we describe a proposed intervention that would overcome these challenges by capitalizing upon Therapeutic Interfering Particles (TIPs) that are engineered to replicate conditionally in the presence of the pathogen and spread between individuals â€” analogous to â€˜transmissible immunizationâ€™ that occurs with live-attenuated vaccines (but without the potential for reversion to virulence). \n",
       "- Building on analyses of HIV field data from sub-Saharan Africa, we construct a multi-scale model, beginning at the single-cell level, to predict the effect of TIPs on individual patient viral loads and ultimately population-level disease prevalence. \n",
       "- <span style='color:green'>Our results show that a TIP, engineered with properties based on a recent HIV gene-therapy trial, could stably lower HIV/AIDS prevalence by âˆ¼30-fold within 50 years and could complement current therapies.</span> \n",
       "- <span style='color:green'>In contrast, optimistic antiretroviral therapy or vaccination campaigns alone could only lower HIV/AIDS prevalence by <2-fold over 50 years.</span> \n",
       "- The TIP's efficacy arises from its exploitation of the same risk factors as the pathogen, allowing it to autonomously penetrate superspreader populations, maintain efficacy despite behavioral disinhibition, and limit viral resistance. \n",
       "- <span style='color:green'>While demonstrated here for HIV, the TIP concept could apply broadly to many viral infectious diseases and would represent a new paradigm for disease control, away from pathogen eradication but toward robust disease suppression.</span> \n",
       "- <span style='color:green'>While demonstrated here for HIV, the TIP concept could apply broadly to many viral infectious diseases and would represent a new paradigm for disease control, away from pathogen eradication but toward robust disease suppression.</span>\n",
       "\n",
       "#### [HIV: Biology to Treatment](https://api.semanticscholar.org/10.1007/978-981-32-9898-9_7) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.64, evidence scores=0.67, 0.01, 0.37)\n",
       "- AIDS is one of the most dreaded diseases of the twenty-first century caused by human immunodeficiency virus (HIV). \n",
       "- <span style='color:green'>Recently, there are reports which show decline in new infections due to better access to anti-retroviral drugs.</span> \n",
       "- <span style='color:green'>Still on a daily basis, ~2356 new HIV infections are being reported globally.</span> \n",
       "- New treatments and anti-HIV drugs are being continuously developed with the aim to control and cure AIDS. \n",
       "- The anti-HIV drugs that are in use usually target HIV entry and replication inside the host cells. \n",
       "- <span style='color:green'>However, these drugs are only partially effective in slowing the rate of HIV replication.</span> \n",
       "- Nevertheless, the virus manages to replicate at much slower rates even when anti-retroviral treatment is ongoing. \n",
       "- The HIV seropositives who are on anti-retroviral treatment for long periods of time are now developing different kinds of other complications including neuroAIDS. \n",
       "- The latest development in HIV therapy is a novel kind of bone marrow transplantation from donors who have a homozygous mutation in CCR5 gene. \n",
       "- The latest development in HIV therapy is a novel kind of bone marrow transplantation from donors who have a homozygous mutation in CCR5 gene.\n",
       "\n",
       "#### [Human Immunodeficiency Virus-Associated Diarrhea: Still an Issue in the Era of Antiretroviral Therapy](https://api.semanticscholar.org/10.1007/s10620-015-3615-y) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.54, evidence scores=0.01, 0.99, 0.02)\n",
       "- <span style='color:green'>Over half of patients with human immunodeficiency virus (HIV) experience diarrhea that contributes negatively to quality of life and adherence to antiretroviral therapy (ART).</span> \n",
       "- Opportunistic infectious agents that cause diarrhea in patients with HIV span the array of protozoa, fungi, viruses, and bacteria. \n",
       "- <span style='color:green'>With global use of ART, the incidence of diarrhea because of opportunistic infections has decreased; however, the incidence of noninfectious diarrhea has increased.</span> \n",
       "- <span style='color:green'>The etiology of noninfectious diarrhea in patients with HIV is multifactorial and includes ART-associated diarrhea and gastrointestinal damage related to HIV infection (i.e., HIV enteropathy).</span> \n",
       "- A basic algorithm for the diagnosis of diarrhea in patients with HIV includes physical examination, a review of medical history, assessment of HIV viral load and CD4+ T cell count, stool microbiologic assessment, and endoscopic evaluation, if needed. \n",
       "- For patients with negative diagnostic results, the diagnosis of noninfectious diarrhea may be considered. \n",
       "- Pharmacologic options for the treatment of noninfectious diarrhea are primarily supportive; however, the use of many unapproved agents is based on unstudied and anecdotal information. \n",
       "- In addition, these agents can be associated with treatment-limiting adverse events (AEs), such as drugâ€“drug interactions with ART regimens, abuse liability, and additional gastrointestinal AEs. \n",
       "- Currently, crofelemer, an antisecretory agent, is the only therapy approved in the USA for the symptomatic relief of noninfectious diarrhea in patients with HIV on ART. \n",
       "- Currently, crofelemer, an antisecretory agent, is the only therapy approved in the USA for the symptomatic relief of noninfectious diarrhea in patients with HIV on ART.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claim_to_check = \"ART substantially reduces infectiveness of HIV-positive people.\"#df_claim_evid_label.iloc[18, :][\"claim\"]\n",
    "args_sci = ArgsScifact(claim_to_check)\n",
    "\n",
    "#pretrained_models_config = pretrained_models_for_scifact(args_sci)\n",
    "\n",
    "results_raw = inference(args_sci, pretrained_models_config)\n",
    "\n",
    "if results_raw!= []:\n",
    "    result_md = export(args_sci, results_raw)\n",
    "    #result_md = export(args_sci, results_raw)\n",
    "    display(Markdown(result_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83ad2623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:09.780116Z",
     "start_time": "2022-01-14T10:51:09.777541Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(pretraind_models_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56524194",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895bab2",
   "metadata": {},
   "source": [
    "### Tech term"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78c3110c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:50:57.373595Z",
     "start_time": "2022-01-12T03:50:57.366209Z"
    }
   },
   "source": [
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    print('\\n\\n\\n>>>>> ')\n",
    "    print('claim_original : ', claim_original)\n",
    "    print('claim_paraphrased : ', claim_paraphrased)\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'\\b' + re.escape(cur_term_row.ner_text) + r'\\b'\n",
    "        if cur_term_row_formatted not in claim_paraphrased:\n",
    "            print('# [ ' , cur_term_row_formatted, '  ] not found ' )\n",
    "            return False        \n",
    "#         if cur_term_row.ner_text.lower() not in claim_paraphrased.lower():\n",
    "#             return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4311b512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:09.785335Z",
     "start_time": "2022-01-14T10:51:09.781914Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29996079/match-a-whole-word-in-a-string-using-dynamic-regex\n",
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'(?<!\\S){}(?!\\S)'.format(re.escape(cur_term_row.ner_text))\n",
    "        res_num = re.findall(cur_term_row_formatted, claim_paraphrased)\n",
    "        if res_num == []:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fbe2b",
   "metadata": {},
   "source": [
    "### Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68dac4c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.414980Z",
     "start_time": "2022-01-14T10:51:09.787182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/qudratealahyratu/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (mnli): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_neg_checker_roberta = torch.hub.load(PARAPHRASE_PROJECT_SETTINGS['entailment_model']['model_path'], \n",
    "                                           PARAPHRASE_PROJECT_SETTINGS['entailment_model']['model_name'])\n",
    "\n",
    "model_neg_checker_roberta.to(device)\n",
    "#model_neg_checker_roberta.cuda()\n",
    "\n",
    "model_neg_checker_roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3ec0b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.421828Z",
     "start_time": "2022-01-14T10:51:26.416888Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mlnli_label(org_claim, gen_claim):    \n",
    "    tokens_sentences_org_gen = model_neg_checker_roberta.encode(org_claim, gen_claim)\n",
    "    logprobs_sentences_org_gen = model_neg_checker_roberta.predict('mnli', tokens_sentences_org_gen)      \n",
    "    cal_val_mlnli_org_gen = logprobs_sentences_org_gen.argmax(dim=1).item()\n",
    "    cal_label_mlnli_org_gen = PARAPHRASE_PROJECT_SETTINGS['labels_multi_nli'][cal_val_mlnli_org_gen]\n",
    "    \n",
    "    tokens_sentences_gen_org = model_neg_checker_roberta.encode(gen_claim, org_claim)\n",
    "    logprobs_sentences_gen_org = model_neg_checker_roberta.predict('mnli', tokens_sentences_gen_org)      \n",
    "    cal_val_mlnli_gen_org = logprobs_sentences_gen_org.argmax(dim=1).item()\n",
    "    cal_label_mlnli_gen_org = PARAPHRASE_PROJECT_SETTINGS['labels_multi_nli'][cal_val_mlnli_gen_org]    \n",
    "#     return {'val_mlnli_org_gen' : cal_val_mlnli_org_gen, \n",
    "#             'label_mlnli_org_gen': cal_label_mlnli_org_gen, \n",
    "#             'val_mlnli_gen_org': cal_val_mlnli_gen_org, \n",
    "#             'label_mlnli_gen_org': cal_label_mlnli_gen_org}\n",
    "    \n",
    "    return pd.Series([cal_val_mlnli_org_gen, cal_label_mlnli_org_gen, cal_val_mlnli_gen_org, cal_label_mlnli_gen_org])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd46de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d13eb3",
   "metadata": {},
   "source": [
    "## Apply Finetuned Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e2de9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T09:11:48.857656Z",
     "start_time": "2021-11-11T09:11:48.822407Z"
    }
   },
   "source": [
    "tokenizer_t5 = AutoTokenizer.from_pretrained(model_name_or_path)  \n",
    "_ = model_t5_fine_tuned.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffc7509b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.432627Z",
     "start_time": "2022-01-14T10:51:26.424040Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_t5_gen_sentences(org_sentence, model_t5, tokenizer_t5):\n",
    "    text =  \"paraphrase: \" + org_sentence + \" </s>\"\n",
    "\n",
    "    encoding = tokenizer_t5.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    #PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']\n",
    "    outputs = []\n",
    "    if type(model_t5) == T5ForConditionalGeneration:\n",
    "        outputs = model_t5.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['max_length'],\n",
    "            do_sample=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['do_sample'],\n",
    "            top_k=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_k'],\n",
    "            top_p=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_p'],\n",
    "            repetition_penalty=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['repetition_penalty'],\n",
    "            early_stopping=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['early_stopping'],\n",
    "            num_return_sequences=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['num_return_sequences']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        outputs = model_t5.model.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['max_length'],\n",
    "            do_sample=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['do_sample'],\n",
    "            top_k=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_k'],\n",
    "            top_p=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_p'],\n",
    "            repetition_penalty=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['repetition_penalty'],\n",
    "            early_stopping=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['early_stopping'],\n",
    "            num_return_sequences=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['num_return_sequences']\n",
    "        )\n",
    "        \n",
    "    gen_sentences_t5 = []\n",
    "    for output in outputs:\n",
    "        line = tokenizer_t5.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "        gen_sentences_t5.append(line)\n",
    "    \n",
    "    #print('gen_sentences_t5_tapaco >> ', gen_sentences_t5_tapaco)\n",
    "    return list(set(gen_sentences_t5))#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23e2a7f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.442821Z",
     "start_time": "2022-01-14T10:51:26.434682Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stat_of_original_claim(row_org_claim):\n",
    "    claim =  row_org_claim[\"org_claim\"]\n",
    "    logging.info(\"#### \\n\\n>>> Original claim >>> \")\n",
    "    logging.info(claim)\n",
    "\n",
    "    args_sci = ArgsScifact(claim)\n",
    "    dic_info = {}\n",
    "    \n",
    "    dic_info[\"org_claim\"] = row_org_claim[\"org_claim\"]\n",
    "    dic_info[\"ground_label\"] = row_org_claim[\"ground_label\"]\n",
    "    dic_info[\"ground_list_rationales\"] = row_org_claim[\"ground_list_rationales\"]\n",
    "    dic_info[\"source\"] = row_org_claim[\"source\"]\n",
    "    dic_info[\"org_count_support\"] = 0\n",
    "    dic_info[\"org_count_refute\"] = 0\n",
    "    dic_info[\"org_list_supported_ids\"] = []\n",
    "    dic_info[\"org_list_refuted_ids\"] = []\n",
    "    dic_info[\"org_list_supported_confidence\"] = []\n",
    "    dic_info[\"org_list_refuted_confidence\"] = []\n",
    "    dic_info[\"org_list_supported_confidence_mean\"] = 0.0\n",
    "    dic_info[\"org_list_refuted_confidence_mean\"] = 0.0\n",
    "    dic_info[\"org_comment\"] = \"\"\n",
    "    \n",
    "    try:\n",
    "        results_raw_org = inference(args_sci, pretrained_models_config)  \n",
    "        \n",
    "        if results_raw_org == []:\n",
    "            dic_info[\"org_comment\"] = \"no result\"\n",
    "            \n",
    "        else:\n",
    "            list_supported_ids = [cur_result['id'] for cur_result in results_raw_org if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_ids = [cur_result['id'] for cur_result in results_raw_org if cur_result['label'] == 'REFUTE']\n",
    "            list_supported_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_org if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_org if cur_result['label'] == 'REFUTE']\n",
    "            \n",
    "            \n",
    "            dic_info[\"org_count_support\"] = len(list_supported_ids)\n",
    "            dic_info[\"org_count_refute\"] = len(list_refuted_ids)\n",
    "            dic_info[\"org_list_supported_ids\"] = list_supported_ids\n",
    "            dic_info[\"org_list_refuted_ids\"] = list_refuted_ids\n",
    "            dic_info[\"org_list_supported_confidence\"] = list_supported_label_confidence\n",
    "            dic_info[\"org_list_refuted_confidence\"] = list_refuted_label_confidence\n",
    "            if len(list_supported_label_confidence) > 0:\n",
    "                dic_info[\"org_list_supported_confidence_mean\"] = mean(list_supported_label_confidence)\n",
    "            if len(list_refuted_label_confidence) > 0:\n",
    "                dic_info[\"org_list_refuted_confidence_mean\"] = mean(list_refuted_label_confidence)\n",
    "            dic_info[\"org_comment\"] = \"success\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        dic_info[\"org_comment\"] = \"exception : \"+e\n",
    "        logging.info(\">>> Exception original claim >>> \") \n",
    "        logging.info(claim) \n",
    "        logging.info(e)\n",
    "    \n",
    "    finally:\n",
    "        return dic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b73fe3ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.455882Z",
     "start_time": "2022-01-14T10:51:26.444699Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results_by_gen_claim(gen_claim, dic_original_claim_info):\n",
    "    logging.info(\":: Generated claim :: \")\n",
    "    logging.info(gen_claim)\n",
    "    #print(gen_claim)\n",
    "    \n",
    "    args_gen = ArgsScifact(gen_claim)\n",
    "    gen_dic_info = {}\n",
    "    \n",
    "    #gen_dic_info[\"gen_claim\"] = gen_claim\n",
    "    gen_dic_info[\"gen_count_support\"] = 0\n",
    "    gen_dic_info[\"gen_count_refute\"] = 0\n",
    "    gen_dic_info[\"gen_list_supported_ids\"] = []\n",
    "    gen_dic_info[\"gen_list_refuted_ids\"] = []\n",
    "    gen_dic_info[\"gen_list_supported_confidence\"] = []\n",
    "    gen_dic_info[\"gen_list_refuted_confidence\"] = []\n",
    "    gen_dic_info[\"gen_list_supported_confidence_mean\"] = 0.0\n",
    "    gen_dic_info[\"gen_list_refuted_confidence_mean\"] = 0.0\n",
    "    gen_dic_info[\"gen_comment\"] = \"\"    \n",
    "    \n",
    "    gen_dic_info[\"common_all\"] = 0\n",
    "    gen_dic_info[\"common_support_refute\"] = 0\n",
    "    gen_dic_info[\"common_refute_support\"] = 0\n",
    "    gen_dic_info[\"common_support_support\"] = 0\n",
    "    gen_dic_info[\"common_refute_refute\"] = 0\n",
    "    \n",
    "    try:\n",
    "        results_raw_gen = inference(args_gen, pretrained_models_config)  \n",
    "\n",
    "        if results_raw_gen == []:\n",
    "            gen_dic_info[\"gen_comment\"] = \"no result\"\n",
    "            \n",
    "        else:\n",
    "            list_supported_ids = [cur_result['id'] for cur_result in results_raw_gen if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_ids = [cur_result['id'] for cur_result in results_raw_gen if cur_result['label'] == 'REFUTE']\n",
    "            list_supported_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_gen if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_gen if cur_result['label'] == 'REFUTE']\n",
    "            \n",
    "            \n",
    "            gen_dic_info[\"gen_count_support\"] = len(list_supported_ids)\n",
    "            gen_dic_info[\"gen_count_refute\"] = len(list_refuted_ids)\n",
    "            gen_dic_info[\"gen_list_supported_ids\"] = list_supported_ids\n",
    "            gen_dic_info[\"gen_list_refuted_ids\"] = list_refuted_ids\n",
    "            gen_dic_info[\"gen_list_supported_confidence\"] = list_supported_label_confidence\n",
    "            gen_dic_info[\"gen_list_refuted_confidence\"] = list_refuted_label_confidence\n",
    "            if len(list_supported_label_confidence) > 0 :\n",
    "                gen_dic_info[\"gen_list_supported_confidence_mean\"] = mean(list_supported_label_confidence)\n",
    "            if len(list_refuted_label_confidence) > 0:\n",
    "                gen_dic_info[\"gen_list_refuted_confidence_mean\"] = mean(list_refuted_label_confidence)\n",
    "            gen_dic_info[\"gen_comment\"] = \"success\"      \n",
    "            \n",
    "            \n",
    "            common_all = (set(gen_dic_info[\"gen_list_supported_ids\"]) | set(gen_dic_info[\"gen_list_refuted_ids\"])) & \\\n",
    "                (set(dic_original_claim_info[\"org_list_supported_ids\"]) | set(dic_original_claim_info[\"org_list_refuted_ids\"]))\n",
    "            \n",
    "            common_support_refute = set(dic_original_claim_info[\"org_list_supported_ids\"]) & set(gen_dic_info[\"gen_list_refuted_ids\"])\n",
    "            common_refute_support = set(dic_original_claim_info[\"org_list_refuted_ids\"]) & set(gen_dic_info[\"gen_list_supported_ids\"])\n",
    "            common_support_support = set(dic_original_claim_info[\"org_list_supported_ids\"]) & set(gen_dic_info[\"gen_list_supported_ids\"])\n",
    "            common_refute_refute = set(dic_original_claim_info[\"org_list_refuted_ids\"]) & set(gen_dic_info[\"gen_list_refuted_ids\"])\n",
    "            \n",
    "            gen_dic_info[\"common_all\"] = len(common_all)\n",
    "            gen_dic_info[\"common_support_refute\"] = len(common_support_refute)\n",
    "            gen_dic_info[\"common_refute_support\"] = len(common_refute_support)\n",
    "            gen_dic_info[\"common_support_support\"] = len(common_support_support)\n",
    "            gen_dic_info[\"common_refute_refute\"] = len(common_refute_refute)\n",
    "            \n",
    "            gen_dic_info[\"gen_comment\"] = \"success\" \n",
    "            \n",
    "    except Exception as e:\n",
    "        dic_info[\"gen_comment\"] = \"exception : \"+e\n",
    "        logging.info(\">>> Exception gen claim >>> \") \n",
    "        logging.info(claim) \n",
    "        logging.info(e)\n",
    "        \n",
    "    finally:\n",
    "        return gen_dic_info           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bb8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c483efe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.464295Z",
     "start_time": "2022-01-14T10:51:26.457899Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_no_ft_with_detail_stat(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "\n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info\n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "294c1060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.473205Z",
     "start_time": "2022-01-14T10:51:26.466510Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            list_paraphrased_claims_with_sim_threshold = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                ## Enable if need to measure similarity score                \n",
    "                #                 cur_similarity_score = get_sentence_similarity_score(model_sim_diltillroberta_base,\n",
    "                #                                                                     cur_row['org_claim'], \n",
    "                #                                                                     cur_paraphrased_sent)\n",
    "                cur_similarity_score = 1.0\n",
    "                if cur_similarity_score >= PARAPHRASE_PROJECT_SETTINGS['run_settings']['SIMILARITY_THRESHOLD']:\n",
    "                    list_paraphrased_claims_with_sim_threshold.append(cur_paraphrased_sent)\n",
    "            #Filter paraphrased sentences with tech terms\n",
    "                \n",
    "            #for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info      \n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2856f43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.481923Z",
     "start_time": "2022-01-14T10:51:26.475227Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact = pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_org_claims_by_scifact'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8b64a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:43:13.538419Z",
     "start_time": "2022-01-11T21:43:13.532482Z"
    }
   },
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9247214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d401e519",
   "metadata": {},
   "source": [
    "### Filter first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98dff6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.488268Z",
     "start_time": "2022-01-14T10:51:26.483709Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in df_dataset_to_be_paraphrased.iloc[:,:].iterrows():\n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                cur_tmp_dict = {'gen_claim' : cur_paraphrased_sent,\n",
    "                                               'model_paraphrase' : model_name_t5}\n",
    "                dict_cur_row = cur_row.to_dict()\n",
    "                cur_tmp_dict.update(dict_cur_row)\n",
    "                list_results_fine_tuned.append(cur_tmp_dict)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)      \n",
    "            \n",
    "    return pd.DataFrame(list_results_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87be8218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.492791Z",
     "start_time": "2022-01-14T10:51:26.490147Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_stat_no_ft(cur_row_org_detail_paraphrased_sent):\n",
    "    dict_cur_org_claim = cur_row_org_detail_paraphrased_sent.to_dict()\n",
    "    cur_paraphrased_sent = cur_row_org_detail_paraphrased_sent['gen_claim']\n",
    "    dict_all_results = get_results_by_gen_claim(cur_paraphrased_sent, dict_cur_org_claim)\n",
    "    return pd.Series(dict_all_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cef49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.499642Z",
     "start_time": "2022-01-14T10:51:26.494679Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in df_dataset_to_be_paraphrased.iloc[:,:].iterrows():\n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                cur_tmp_dict = {'gen_claim' : cur_paraphrased_sent,\n",
    "                                               'model_paraphrase' : model_name_t5}\n",
    "                dict_cur_row = cur_row.to_dict()\n",
    "                cur_tmp_dict.update(dict_cur_row)\n",
    "                list_results_fine_tuned.append(cur_tmp_dict)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)      \n",
    "            \n",
    "    return pd.DataFrame(list_results_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4ef76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf5cff06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.503565Z",
     "start_time": "2022-01-14T10:51:26.501400Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_org_support_major[10:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0735aa00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.511843Z",
     "start_time": "2022-01-14T10:51:26.505134Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            list_paraphrased_claims_with_sim_threshold = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                ## Enable if need to measure similarity score                \n",
    "                #                 cur_similarity_score = get_sentence_similarity_score(model_sim_diltillroberta_base,\n",
    "                #                                                                     cur_row['org_claim'], \n",
    "                #                                                                     cur_paraphrased_sent)\n",
    "                cur_similarity_score = 1.0\n",
    "                if cur_similarity_score >= PARAPHRASE_PROJECT_SETTINGS['run_settings']['SIMILARITY_THRESHOLD']:\n",
    "                    list_paraphrased_claims_with_sim_threshold.append(cur_paraphrased_sent)\n",
    "            #Filter paraphrased sentences with tech terms\n",
    "                \n",
    "            #for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info      \n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5084305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c2264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T00:29:37.063389Z",
     "start_time": "2022-01-14T00:29:37.054846Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1b55b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:51:26.526569Z",
     "start_time": "2022-01-14T10:51:26.513552Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact = pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_org_claims_by_scifact'])\n",
    "df_org_claims_by_scifact_support_major, df_org_claims_by_scifact_refute_major, df_org_claims_by_scifact_sci_success = get_dataframes_by_majority_org_claim(df_org_claims_by_scifact)\n",
    "df_org_claims_by_scifact_majority = pd.concat([df_org_claims_by_scifact_support_major, df_org_claims_by_scifact_refute_major], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b729c8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T10:50:41.649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2387</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2344</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2226</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1024</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 918</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 320</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 214</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 134</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 80</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 214</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56b388539a141288e6c0a79fe9f693e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/verisci/covid/abstract_retriever.py:53: UserWarning: abstract selector :: returning empty list \n",
      "  warnings.warn(\"abstract selector :: returning empty list \")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d028ad6ff0548a59a00d3120b124cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Succesfully attacked Claim Stat </h3><h4 style=\"color:#0080ff\"># of toal org refute to gen support : 384</h4><h4 style=\"color:#0080ff\"># of total org support to gen refute : 534</h4><h4 style=\"color:#0080ff\"># of unique org refute to support : 80</h4><h4 style=\"color:#0080ff\"># of unique org support to refute : 134</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:86: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding`LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch(rather, they are called on every optimization step).\n",
      "  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e67c11888c4dcda855f50ee7825cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:406: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 11: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2572</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2280</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2163</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1120</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 972</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 319</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 229</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 141</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 88</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 229</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8dad85e7054010ab06795e19fadcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39227ae5538a4a64a5c1a43d103ad1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Succesfully attacked Claim Stat </h3><h4 style=\"color:#0080ff\"># of toal org refute to gen support : 415</h4><h4 style=\"color:#0080ff\"># of total org support to gen refute : 557</h4><h4 style=\"color:#0080ff\"># of unique org refute to support : 88</h4><h4 style=\"color:#0080ff\"># of unique org support to refute : 141</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6e34589ff3469893d1e3a9aaef8cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 11: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2670</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2129</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2042</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1150</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 969</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 321</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 231</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 144</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 87</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 231</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62ab8d680f543418aab202adc45ec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399cb3a1245b462196c23b3ec45fe5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Succesfully attacked Claim Stat </h3><h4 style=\"color:#0080ff\"># of toal org refute to gen support : 412</h4><h4 style=\"color:#0080ff\"># of total org support to gen refute : 557</h4><h4 style=\"color:#0080ff\"># of unique org refute to support : 87</h4><h4 style=\"color:#0080ff\"># of unique org support to refute : 144</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ede8f290d4429686800e929e20603d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 11: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2725</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2188</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2112</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1189</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 1010</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 317</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 231</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 140</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 91</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 231</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba16a6bad9c4727883f6b4d78644abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6188d6e5104ed792459999d9641677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Succesfully attacked Claim Stat </h3><h4 style=\"color:#0080ff\"># of toal org refute to gen support : 441</h4><h4 style=\"color:#0080ff\"># of total org support to gen refute : 569</h4><h4 style=\"color:#0080ff\"># of unique org refute to support : 91</h4><h4 style=\"color:#0080ff\"># of unique org support to refute : 140</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11591e118b274ca787936831fadf7406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 6: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 13: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2703</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2179</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2101</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1264</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 1060</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 319</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 226</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 135</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 91</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 226</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7187c0fc7be4e29a5ba9e4f05b9bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed50ad09952149dc9296553b6bef6def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load experiment setup\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "\n",
    "\n",
    "# Load no-tuned scifact model\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "\n",
    "\n",
    "#remove duplicates from the original dataset\n",
    "df_org_claims_by_scifact = df_org_claims_by_scifact.drop_duplicates('org_claim', keep='first')\n",
    "#get paraphrased sentences from no-tuned model\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "## Pass fillter\n",
    "\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "# pass all valids to scifact\n",
    "\n",
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "    (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "    \n",
    "]\n",
    "\n",
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_support_major_paraphrased_stat = pd.concat([df_org_support_major, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "\n",
    "report_df_succesfully_attacked_claim(df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat], ignore_index=True)\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/v2/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_successful_filter_attacked, fp)\n",
    "    \n",
    "while(True):\n",
    "    df_fine_tuning_dataset = None    \n",
    "    ## Select dataset for fine-tuning ::either support major or refute major\n",
    "    if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "        df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_sup_to_gen_ref']\n",
    "    elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "        df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_ref_to_gen_sup']\n",
    "    else:\n",
    "        raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "    CUR_NO_OF_EPOCH_FT += 1    \n",
    "\n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                             df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "    model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)        \n",
    "\n",
    "\n",
    "    # ask model to generate paraphrase\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune.model.to(device)\n",
    "\n",
    "    # Ask fine-tuned model to paraphrase\n",
    "    df_paraphrased_selected_model_full = get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                               model_t5 = model_t5_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "\n",
    "    # Filter for entailment check\n",
    "    df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    #Filter for tech terms check\n",
    "    df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "    report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "\n",
    "    df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "        (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "        (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "\n",
    "    ]\n",
    "\n",
    "    #Check majority\n",
    "    df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "    report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "    df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "    df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "    df_org_support_major_paraphrased_stat = pd.concat([df_org_support_major, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "    df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "\n",
    "    df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "    df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "\n",
    "    report_df_succesfully_attacked_claim(df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    df_successful_filter_attacked = pd.concat([df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat], ignore_index=True)\n",
    "\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "    with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/v2/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_successful_filter_attacked, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd2b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d41f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081daa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4749f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433b785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdf497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:03:20.617682Z",
     "start_time": "2022-01-14T10:03:20.611644Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact_majority_original = df_org_claims_by_scifact_majority.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cfcbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:03:38.447699Z",
     "start_time": "2022-01-14T10:03:38.442884Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_org_claims_by_scifact_majority = df_org_claims_by_scifact_majority_original.iloc[200:230, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a63f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:17:33.281827Z",
     "start_time": "2022-01-14T10:05:52.016628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load experiment setup\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "\n",
    "\n",
    "# Load no-tuned scifact model\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "\n",
    "\n",
    "#remove duplicates from the original dataset\n",
    "df_org_claims_by_scifact = df_org_claims_by_scifact.drop_duplicates('org_claim', keep='first')\n",
    "#get paraphrased sentences from no-tuned model\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "## Pass fillter\n",
    "\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "# pass all valids to scifact\n",
    "\n",
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "    (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "    \n",
    "]\n",
    "\n",
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_support_major_paraphrased_stat = pd.concat([df_org_support_major, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "\n",
    "report_df_succesfully_attacked_claim(df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57129959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:29:19.479102Z",
     "start_time": "2022-01-14T10:28:25.019242Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fine_tuning_dataset = None    \n",
    "## Select dataset for fine-tuning ::either support major or refute major\n",
    "if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "    df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_sup_to_gen_ref']\n",
    "elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "    df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_ref_to_gen_sup']\n",
    "else:\n",
    "    raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "CUR_NO_OF_EPOCH_FT += 1    \n",
    "\n",
    "## Train model with fine-tuning dataset\n",
    "df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                         num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                         df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)        \n",
    "\n",
    "\n",
    "# ask model to generate paraphrase\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = trainer_model_t5_fine_tune.model.to(device)\n",
    "\n",
    "# Ask fine-tuned model to paraphrase\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                           model_t5 = model_t5_fine_tuned, \n",
    "                                                                           tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                           model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "# pass all valids to scifact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fddba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:40:52.914035Z",
     "start_time": "2022-01-14T10:31:11.132800Z"
    }
   },
   "outputs": [],
   "source": [
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "        (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "        (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "\n",
    "    ]\n",
    "\n",
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = pd.concat([df_org_support_major, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d23c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffe822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60732e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dc74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb0f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c247b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T07:44:43.183208Z",
     "start_time": "2022-01-14T07:43:21.674158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check attacks by scifact\n",
    "df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "df_org_support_gen_refute['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_gen_support['attack_type'] = 'org_ref_to_gen_sup'\n",
    "report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_gen_refute, df_org_refute_gen_support], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c8181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae56c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T07:44:43.191262Z",
     "start_time": "2022-01-14T07:44:43.185751Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.shape, df_org_support_major.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccb2d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T07:44:43.201023Z",
     "start_time": "2022-01-14T07:44:43.193013Z"
    }
   },
   "outputs": [],
   "source": [
    "xx = pd.concat([df_org_support_major, x], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ec7c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T07:44:48.920375Z",
     "start_time": "2022-01-14T07:44:48.913358Z"
    }
   },
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf07c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T08:21:24.981198Z",
     "start_time": "2022-01-14T08:21:24.976943Z"
    }
   },
   "outputs": [],
   "source": [
    "df_paraphrased_filtered_original = df_paraphrased_filtered.copy()\n",
    "df_paraphrased_filtered = df_paraphrased_filtered.iloc[:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b57d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T08:25:20.928972Z",
     "start_time": "2022-01-14T08:21:29.562645Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "\n",
    "report_df_succesfully_attacked_claim(df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat], ignore_index=True)\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_successful_filter_attacked, fp)\n",
    "\n",
    "# Fine tune model\n",
    "df_fine_tuning_dataset = None    \n",
    "## Select dataset for fine-tuning ::either support major or refute major\n",
    "if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "    df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_sup_to_gen_ref']\n",
    "elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "    df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_refute_gen_sup']\n",
    "else:\n",
    "    raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "CUR_NO_OF_EPOCH_FT += 1    \n",
    "\n",
    "## Train model with fine-tuning dataset\n",
    "\n",
    "df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                         num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                         df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "\n",
    "model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)        \n",
    "\n",
    "\n",
    "# ask model to generate paraphrase\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = trainer_model_t5_fine_tune.model.to(device)\n",
    "\n",
    "# Ask fine-tuned model to paraphrase\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                           model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                           tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                           model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "# pass all valids to scifact\n",
    "\n",
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "    (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "\n",
    "]\n",
    "\n",
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_major_paraphrased_stat['attack_type'] = 'org_ref_to_gen_sup'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471082b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.894315Z",
     "start_time": "2022-01-14T03:19:46.842671Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_org_support_major, df_org_refute_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2daa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.896339Z",
     "start_time": "2022-01-14T03:19:46.085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check attacks by scifact\n",
    "df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "df_org_support_gen_refute['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_gen_support['attack_type'] = 'org_ref_to_gen_sup'\n",
    "report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_gen_refute, df_org_refute_gen_support], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d145eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.897530Z",
     "start_time": "2022-01-14T03:19:46.091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load experiment setup\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "\n",
    "\n",
    "# Load no-tuned scifact model\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "\n",
    "\n",
    "#remove duplicates from the original dataset\n",
    "df_org_claims_by_scifact = df_org_claims_by_scifact.drop_duplicates('org_claim', keep='first')\n",
    "#get paraphrased sentences from no-tuned model\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "## Pass fillter\n",
    "\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "# pass all valids to scifact\n",
    "\n",
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "    (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "#Check majority\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "# Check attacks by scifact\n",
    "df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "df_org_support_gen_refute['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_gen_support['attack_type'] = 'org_ref_to_gen_sup'\n",
    "report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_gen_refute, df_org_refute_gen_support], ignore_index=True)\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_successful_filter_attacked, fp)\n",
    "    \n",
    "while(True):\n",
    "    \n",
    "# Fine tune model\n",
    "    df_fine_tuning_dataset = None\n",
    "\n",
    "                                        \n",
    "    ## Select dataset for fine-tuning ::either support major or refute major\n",
    "    if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "        df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_sup_to_gen_ref']\n",
    "    elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "        df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_refute_gen_sup']\n",
    "    else:\n",
    "        raise ValueError('Select a direction of fine tuning dataset')\n",
    "        \n",
    "    CUR_NO_OF_EPOCH_FT += 1\n",
    "                                        \n",
    "                                        \n",
    "\n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                             df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "    model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)        \n",
    "    \n",
    "                                        \n",
    "    # ask model to generate paraphrase\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune.model.to(device)\n",
    "    ## Ask fine-tuned model to paraphrase\n",
    "\n",
    "    ## Evaluate paraphrased dataset\n",
    "    df_paraphrased_fine_tuned = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_org_claims_by_scifact.iloc[:, :], \n",
    "                                              model_t5 = model_t5_fine_tuned, \n",
    "                                              tokenizer_t5 = tokenizer_t5, \n",
    "                                              model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "    # Apply filter\n",
    "    # Filter for entailment check\n",
    "    df_paraphrased_fine_tuned[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_fine_tuned.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    #Filter for tech terms check\n",
    "    df_paraphrased_fine_tuned['passed_ner_abr_filter_ic'] = df_paraphrased_fine_tuned.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "    # pass all valids to scifact\n",
    "\n",
    "    df_paraphrased_filtered_fine_tuned = df_paraphrased_fine_tuned[\n",
    "        (df_paraphrased_fine_tuned['passed_ner_abr_filter_ic'] == True) &\n",
    "        (df_paraphrased_fine_tuned['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_paraphrased_fine_tuned['mlnli_label_gen_org'] == 'entailment')\n",
    "\n",
    "    ]\n",
    "    #Check majority\n",
    "    df_org_support_major, df_org_refute_major, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "    report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "    # Check attacks by scifact\n",
    "    df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "    df_org_support_gen_refute['attack_type'] = 'org_sup_to_gen_ref'\n",
    "    df_org_refute_gen_support['attack_type'] = 'org_ref_to_gen_sup'\n",
    "    report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    df_successful_filter_attacked = pd.concat([df_org_support_gen_refute, df_org_refute_gen_support], ignore_index=True)\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "    with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_successful_filter_attacked, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eab34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da6c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.898595Z",
     "start_time": "2022-01-14T03:19:46.102Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all model's no_fine_tuned dataset\n",
    "#df_paraphrased_all_model_full = get_paraphrased_dataframe_all_model_no_fine_tuned()\n",
    "\n",
    "#Filter and select dataset only for the selected model\n",
    "\n",
    "'''\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "'''\n",
    "# df_paraphrased_selected_model_full = get_paraphrased_dataframe_selected_models(df_paraphrased_all_model_full, \n",
    "#                                                                                       list_paraphrase_model_names)\n",
    "\n",
    "#df_org_claims_by_scifact= df_org_claims_by_scifact.iloc[:50, :].copy()\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_org_success= get_dataframes_by_majority_org_claim(df_org_claims_by_scifact)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_org_success)\n",
    "\n",
    "df_all_cur_model_org_success = df_all_cur_model_org_success.drop_duplicates('org_claim', keep='first') \n",
    "\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "                                                                \n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_paraphrased_selected_model_full, fp)\n",
    "\n",
    "\n",
    "#df_paraphrased_selected_model_full = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_refute_to_gen_support_0_FT_concat_prev.pkl')\n",
    "\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "#df_all_cur_model_org_success = pd.DataFrame(df_all_cur_model_org_success['columns_for_org_claim'].unique(), columns = ['org_claim'])\n",
    "#split dataframe of support major and refute major\n",
    "while(True):\n",
    "    # Get paraphrased sentences with majority of Support or refute.\n",
    "    df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success= get_dataframes_by_majority_org_claim(df_paraphrased_selected_model_full)\n",
    "    report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success)\n",
    "\n",
    "\n",
    "    # Get successfullt attacked claims after paraphrased\n",
    "    df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "    report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    report_df_filter(df_org_support_gen_refute, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "    report_df_filter(df_org_refute_gen_support, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "    \n",
    "    \n",
    "    df_fine_tuning_dataset = None\n",
    "\n",
    "    ## Select dataset for fine-tuning ::either support major or refute major\n",
    "    if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "        df_fine_tuning_dataset = df_org_support_gen_refute.copy()\n",
    "    elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "        df_fine_tuning_dataset = df_org_refute_gen_support.copy()\n",
    "    else:\n",
    "        raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "    ##Filter tech terms\n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[df_fine_tuning_dataset['passed_ner_abr_filter_ic'] == True]\n",
    "    \n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[(df_fine_tuning_dataset['mlnli_label_org_gen'] == 'entailment') &\n",
    "                                       (df_fine_tuning_dataset['mlnli_label_gen_org'] == 'entailment')]\n",
    "    \n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                             df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "    model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)\n",
    "\n",
    "    CUR_NO_OF_EPOCH_FT += 1\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune.model.to(device)\n",
    "    ## Ask fine-tuned model to paraphrase\n",
    "\n",
    "    ## Evaluate paraphrased dataset\n",
    "    df_paraphrased_fine_tuned = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success.iloc[:, :], \n",
    "                                              model_t5 = model_t5_fine_tuned, \n",
    "                                              tokenizer_t5 = tokenizer_t5, \n",
    "                                              model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "    \n",
    "    df_paraphrased_fine_tuned['passed_ner_abr_filter_ic'] = df_paraphrased_fine_tuned.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "    df_paraphrased_fine_tuned[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_fine_tuned.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "    #list_paraphrase_model_names[0]+'_'+PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION']+'_'+NUM_OF_EPOCH_REQ_FT\n",
    "    with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_paraphrased_fine_tuned, fp)\n",
    "    \n",
    "    print('>> before epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))\n",
    "    df_paraphrased_selected_model_full = pd.concat([df_paraphrased_selected_model_full, df_paraphrased_fine_tuned], ignore_index=True)\n",
    "    print('>> after epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914098a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ef66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d1baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5415be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.931106Z",
     "start_time": "2022-01-14T03:19:46.909491Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all model's no_fine_tuned dataset\n",
    "#df_paraphrased_all_model_full = get_paraphrased_dataframe_all_model_no_fine_tuned()\n",
    "\n",
    "#Filter and select dataset only for the selected model\n",
    "\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "# df_paraphrased_selected_model_full = get_paraphrased_dataframe_selected_models(df_paraphrased_all_model_full, \n",
    "#                                                                                       list_paraphrase_model_names)\n",
    "\n",
    "#df_org_claims_by_scifact= df_org_claims_by_scifact.iloc[:50, :].copy()\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_org_success= get_dataframes_by_majority_org_claim(df_org_claims_by_scifact)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_org_success)\n",
    "\n",
    "df_all_cur_model_org_success = df_all_cur_model_org_success.drop_duplicates('org_claim', keep='first') \n",
    "\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "                                                                \n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_paraphrased_selected_model_full, fp)\n",
    "\n",
    "\n",
    "#df_paraphrased_selected_model_full = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_refute_to_gen_support_0_FT_concat_prev.pkl')\n",
    "\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "#df_all_cur_model_org_success = pd.DataFrame(df_all_cur_model_org_success['columns_for_org_claim'].unique(), columns = ['org_claim'])\n",
    "#split dataframe of support major and refute major\n",
    "while(True):\n",
    "    # Get paraphrased sentences with majority of Support or refute.\n",
    "    df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success= get_dataframes_by_majority_org_claim(df_paraphrased_selected_model_full)\n",
    "    report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success)\n",
    "\n",
    "\n",
    "    # Get successfullt attacked claims after paraphrased\n",
    "    df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "    report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    report_df_filter(df_org_support_gen_refute, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "    report_df_filter(df_org_refute_gen_support, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "    \n",
    "    \n",
    "    df_fine_tuning_dataset = None\n",
    "\n",
    "    ## Select dataset for fine-tuning ::either support major or refute major\n",
    "    if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "        df_fine_tuning_dataset = df_org_support_gen_refute.copy()\n",
    "    elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "        df_fine_tuning_dataset = df_org_refute_gen_support.copy()\n",
    "    else:\n",
    "        raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "    ##Filter tech terms\n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[df_fine_tuning_dataset['passed_ner_abr_filter_ic'] == True]\n",
    "    \n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[(df_fine_tuning_dataset['mlnli_label_org_gen'] == 'entailment') &\n",
    "                                       (df_fine_tuning_dataset['mlnli_label_gen_org'] == 'entailment')]\n",
    "    \n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                             df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "    model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)\n",
    "\n",
    "    CUR_NO_OF_EPOCH_FT += 1\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune.model.to(device)\n",
    "    ## Ask fine-tuned model to paraphrase\n",
    "\n",
    "    ## Evaluate paraphrased dataset\n",
    "    df_paraphrased_fine_tuned = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success.iloc[:, :], \n",
    "                                              model_t5 = model_t5_fine_tuned, \n",
    "                                              tokenizer_t5 = tokenizer_t5, \n",
    "                                              model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "    \n",
    "    df_paraphrased_fine_tuned['passed_ner_abr_filter_ic'] = df_paraphrased_fine_tuned.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "    df_paraphrased_fine_tuned[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_fine_tuned.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "    #list_paraphrase_model_names[0]+'_'+PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION']+'_'+NUM_OF_EPOCH_REQ_FT\n",
    "    with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_paraphrased_fine_tuned, fp)\n",
    "    \n",
    "    print('>> before epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))\n",
    "    df_paraphrased_selected_model_full = pd.concat([df_paraphrased_selected_model_full, df_paraphrased_fine_tuned], ignore_index=True)\n",
    "    print('>> after epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf8599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a37623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477823b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11b144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1b86f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.933318Z",
     "start_time": "2022-01-14T03:19:46.147Z"
    }
   },
   "outputs": [],
   "source": [
    "report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c73afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.934400Z",
     "start_time": "2022-01-14T03:19:46.154Z"
    }
   },
   "outputs": [],
   "source": [
    "report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740873ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.935402Z",
     "start_time": "2022-01-14T03:19:46.160Z"
    }
   },
   "outputs": [],
   "source": [
    "x = df_org_support_gen_refute.progress_apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c73b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.943110Z",
     "start_time": "2022-01-14T03:19:46.939340Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29996079/match-a-whole-word-in-a-string-using-dynamic-regex\n",
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    print('\\n>>')\n",
    "    print(claim_paraphrased)\n",
    "    print(claim_original)\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'(?<!\\S){}(?!\\S)'.format(re.escape(cur_term_row.ner_text))\n",
    "        res_num = re.findall(cur_term_row_formatted, claim_paraphrased)\n",
    "        if res_num == []:\n",
    "            print(cur_term_row_formatted.casefold())\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181660e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.952382Z",
     "start_time": "2022-01-14T03:19:46.946578Z"
    }
   },
   "outputs": [],
   "source": [
    "len(x[x == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dd60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.961543Z",
     "start_time": "2022-01-14T03:19:46.957498Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_term_row = 'HIV'\n",
    "claim_paraphrased = 'HIV Having a main partner worsens HIVd outcomes HIV .'\n",
    "cur_term_row_formatted = r'(?<!\\w){}(?!\\w)'.format(re.escape(cur_term_row.))\n",
    "re.findall(cur_term_row_formatted, claim_paraphrased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102216b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.970365Z",
     "start_time": "2022-01-14T03:19:46.964307Z"
    }
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f6eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.979047Z",
     "start_time": "2022-01-14T03:19:46.973010Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_term_row_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255a71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.987931Z",
     "start_time": "2022-01-14T03:19:46.981827Z"
    }
   },
   "outputs": [],
   "source": [
    "claim_paraphrased.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1287fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.989858Z",
     "start_time": "2022-01-14T03:19:46.223Z"
    }
   },
   "outputs": [],
   "source": [
    "re.escape(cur_term_row).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba84bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.005955Z",
     "start_time": "2022-01-14T03:19:46.995397Z"
    }
   },
   "outputs": [],
   "source": [
    " filter_and_replace_tech_term_paraphrased_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6434026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.020622Z",
     "start_time": "2022-01-14T03:19:47.012740Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_org_support_gen_refute[df_org_support_gen_refute['mlnli_label_org_gen'] == 'entailment'].shape)\n",
    "print(df_org_support_gen_refute[\n",
    "    (df_org_support_gen_refute['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_org_support_gen_refute['mlnli_label_gen_org'] == 'entailment')\n",
    "].shape)\n",
    "print(df_org_support_gen_refute[df_org_support_gen_refute['passed_ner_abr_filter_ic'] == True].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c3e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.023273Z",
     "start_time": "2022-01-14T03:19:46.251Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_support_gen_refute[df_org_support_gen_refute['passed_ner_abr_filter_ic'] == False][['org_claim', 'gen_claim']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f6001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.036894Z",
     "start_time": "2022-01-14T03:19:47.030096Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_org_refute_gen_support[df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment'].shape)\n",
    "\n",
    "\n",
    "print(df_org_refute_gen_support[\n",
    "    (df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_org_refute_gen_support['mlnli_label_gen_org'] == 'entailment')\n",
    "].shape)\n",
    "\n",
    "print(df_org_refute_gen_support[df_org_refute_gen_support['passed_ner_abr_filter_ic'] == True].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035f671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.038961Z",
     "start_time": "2022-01-14T03:19:46.271Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_org_refute_gen_support[df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment']['org_claim'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24448143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.049877Z",
     "start_time": "2022-01-14T03:19:47.045643Z"
    }
   },
   "outputs": [],
   "source": [
    "'IL-1Î²'.lower() == 'IL-1Î²'.casefold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a53f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.061752Z",
     "start_time": "2022-01-14T03:19:47.054609Z"
    }
   },
   "outputs": [],
   "source": [
    "report_df_filter(df_org_support_gen_refute, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f1c1e",
   "metadata": {},
   "source": [
    "## No fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf158b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.514004Z",
     "start_time": "2022-01-14T03:19:47.262982Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769950d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.517399Z",
     "start_time": "2022-01-14T03:19:46.505Z"
    }
   },
   "outputs": [],
   "source": [
    "print(transformers.__version__)\n",
    "print(pytorch_lightning.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c037e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.518495Z",
     "start_time": "2022-01-14T03:19:46.511Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fine_tuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b449a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8bb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.519589Z",
     "start_time": "2022-01-14T03:19:46.521Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"razent/SciFive-large-Pubmed_PMC\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"razent/SciFive-large-Pubmed_PMC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fdce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.520797Z",
     "start_time": "2022-01-14T03:19:46.528Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25a90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.521932Z",
     "start_time": "2022-01-14T03:19:46.534Z"
    }
   },
   "outputs": [],
   "source": [
    "model_t5 = T5ForConditionalGeneration.from_pretrained('Vamsi/T5_Paraphrase_Paws')\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained('razent/SciFive-large-Pubmed_PMC')       \n",
    "_ = model_t5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5fa4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.522968Z",
     "start_time": "2022-01-14T03:19:46.541Z"
    }
   },
   "outputs": [],
   "source": [
    "org_sentence = 'BCL-2 promotes the apoptotic effects of c-Myc.'\n",
    "#'A T helper 2 cell (Th2) environment impedes disease development in patients with systemic lupus erythematosus (SLE)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c5d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.524106Z",
     "start_time": "2022-01-14T03:19:46.547Z"
    }
   },
   "outputs": [],
   "source": [
    "text =  \"paraphrase: \" + org_sentence \n",
    "\n",
    "encoding = tokenizer_t5.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "outputs = model_t5.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.99,\n",
    "    repetition_penalty=3.5,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "gen_sentences_t5 = []\n",
    "for output in outputs:\n",
    "    line = tokenizer_t5.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    gen_sentences_t5.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f4c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.525183Z",
     "start_time": "2022-01-14T03:19:46.553Z"
    }
   },
   "outputs": [],
   "source": [
    "set(gen_sentences_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795a617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.526342Z",
     "start_time": "2022-01-14T03:19:46.559Z"
    }
   },
   "outputs": [],
   "source": [
    "set(gen_sentences_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb98da5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.527419Z",
     "start_time": "2022-01-14T03:19:46.565Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae49a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.528598Z",
     "start_time": "2022-01-14T03:19:46.572Z"
    }
   },
   "outputs": [],
   "source": [
    "target_claim = 'BCL-2 promotes the apoptosis effects of c-myce a4pt gene.'\n",
    "org_term = 'c-Myce A4PT Gene'\n",
    "term_simple_form = org_term.lower()\n",
    "#pattern = re.compile(r'\\b{}\\b'.format(term_simple_form), re.IGNORECASE)\n",
    "regex_to_search = r'\\b(?=\\w)' + re.escape(term_simple_form) + r'\\b(?!\\w)'\n",
    "pattern = re.compile(r'\\b{}\\b'.format(regex_to_search), re.IGNORECASE)\n",
    "s2=pattern.sub(org_term,target_claim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f2302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.529773Z",
     "start_time": "2022-01-14T03:19:46.578Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71d152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.530819Z",
     "start_time": "2022-01-14T03:19:46.585Z"
    }
   },
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877a42f",
   "metadata": {},
   "source": [
    "https://blog.devgenius.io/different-ways-to-replace-occurences-of-a-substring-in-python-strings-2911b1f7bf86\n",
    "\n",
    "https://betterprogramming.pub/5-ways-to-find-the-index-of-a-substring-in-python-13d5293fc76d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6816cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.531849Z",
     "start_time": "2022-01-14T03:19:46.778Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0733787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.532917Z",
     "start_time": "2022-01-14T03:19:46.784Z"
    }
   },
   "outputs": [],
   "source": [
    "!/home/qudratealahyratu/anaconda3/envs/scifact/bin/pip install -U spacy[cuda113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868dd3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.534109Z",
     "start_time": "2022-01-14T03:19:46.791Z"
    }
   },
   "outputs": [],
   "source": [
    "import thinc\n",
    "thinc.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3503a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.535212Z",
     "start_time": "2022-01-14T03:19:46.797Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip help uninstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3090b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.536268Z",
     "start_time": "2022-01-14T03:19:46.806Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_paraphrased_selected_model_full = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_refute_to_gen_support_0_FT_concat_prev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a31ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.537545Z",
     "start_time": "2022-01-14T03:19:46.814Z"
    }
   },
   "outputs": [],
   "source": [
    "df_paraphrased_selected_model_full[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9099efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.538656Z",
     "start_time": "2022-01-14T03:19:46.821Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_support_to_gen_refute_0_FT_concat_prev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f472d98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.539839Z",
     "start_time": "2022-01-14T03:19:46.827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deb8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.540916Z",
     "start_time": "2022-01-14T03:19:46.836Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp['passed_ner_abr_filter_ic'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f6aadad",
   "metadata": {},
   "source": [
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Succesfully attacked Claim Stat\n",
    "# of toal org refute to gen support : 28\n",
    "# of total org support to gen refute : 31\n",
    "# of unique org refute to support : 19\n",
    "# of unique org support to refute : 22\n",
    "\n",
    "---------------------------------------------------\n",
    "Epoch 1\n",
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Succesfully attacked Claim Stat\n",
    "# of toal org refute to gen support : 197\n",
    "# of total org support to gen refute : 214\n",
    "# of unique org refute to support : 67\n",
    "# of unique org support to refute : 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec860a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.542067Z",
     "start_time": "2022-01-14T03:19:47.006Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp['passed_ner_abr_filter_ic'] = df_tmp.progress_apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154166ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.543111Z",
     "start_time": "2022-01-14T03:19:47.013Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(df_tmp[df_tmp['passed_ner_abr_filter_ic'] == True][['org_claim', 'gen_claim']].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8e91c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.544123Z",
     "start_time": "2022-01-14T03:19:47.020Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_pickle('../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ce62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.545184Z",
     "start_time": "2022-01-14T03:19:47.027Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_pickle('../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl')[['org_claim', 'ground_label', 'ground_list_rationales', 'source',\n",
    "       'org_count_support', 'org_count_refute', 'org_list_supported_ids',\n",
    "       'org_list_refuted_ids', 'org_list_supported_confidence',\n",
    "       'org_list_refuted_confidence', 'org_list_supported_confidence_mean',\n",
    "       'org_list_refuted_confidence_mean', 'org_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b0cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.546274Z",
     "start_time": "2022-01-14T03:19:47.033Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = df_tmp.drop_duplicates('org_claim', keep='first')\n",
    "df_tmp.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026d227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.547260Z",
     "start_time": "2022-01-14T03:19:47.039Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp.to_pickle('../../dfs_generated/scifact/org_claim_ext_roberta_roberta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8c065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scifact] *",
   "language": "python",
   "name": "conda-env-scifact-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
