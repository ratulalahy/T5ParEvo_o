{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfdb9c8",
   "metadata": {},
   "source": [
    "## Setting up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215d7bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:49.773778Z",
     "start_time": "2022-02-06T23:53:45.529409Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f27280a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:50.977343Z",
     "start_time": "2022-02-06T23:53:49.775982Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "from random import choices\n",
    "import os\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import traceback\n",
    "import gc\n",
    "from enum import Enum \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "# from trl.ppo import PPOTrainer\n",
    "#from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "from IPython.core.display import Markdown,display, HTML, Latex\n",
    "import qgrid\n",
    "\n",
    "from verisci.covid import AbstractRetriever, RationaleSelector, LabelPredictor\n",
    "from verisci.evaluate.lib.data import GoldDataset\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53bd19a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:50.984899Z",
     "start_time": "2022-02-06T23:53:50.979928Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "cur_date_time = datetime.today().strftime('%Y_%m_%d_%H_%M')\n",
    "loc_target_root = '../../dfs_generated/paraphrased/paws/'\n",
    "#log_dir = project_opt_location+'logs/'\n",
    "project_name = 'separate_t5_for_majority_tech_term_mlnli'\n",
    "version = 'v1'\n",
    "\n",
    "loc_project_opt_location = loc_target_root+project_name+'/'+version+'/'\n",
    "log_dir = loc_project_opt_location+'logs/'\n",
    "log_file_dir_name = log_dir+'log_all.log'\n",
    "Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import logging\n",
    "  \n",
    "#Create and configure logger\n",
    "logging.basicConfig(filename=log_file_dir_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "log_file_fine_tune_callback = log_dir+'log_call_back.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2404d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.015395Z",
     "start_time": "2022-02-06T23:53:50.987719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/ratulalahy/scifact-paraphrase-T5-evo/e/SCIF3-110\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "nep_run = neptune.init(\n",
    "    project=\"ratulalahy/scifact-paraphrase-T5-evo\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2NWQwMGIyZi1mNzM5LTRiMjEtOTg2MC1mNTc4ODRiMWU2ZGYifQ==\",\n",
    "    tags=['separate_t5_for_majority', 'tech_term_2', 'mlnli'],\n",
    "    source_files=[\"T5_filter_tech_term_no_threshold_filter_first_separate_t5_for_majority.ipynb\"],#[\"**/*.ipynb\", \"*.yaml\"]\n",
    "\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ef31ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.022053Z",
     "start_time": "2022-02-06T23:53:54.018544Z"
    }
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"Scifact_paraphrase_T5_per_evo_general_parasci_sup_to_ref_2_ft_0.0.1\", entity=\"qratulalahy\")\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256b2fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.395154Z",
     "start_time": "2022-02-06T23:53:54.024913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Endpoint: <samp>https://notify.run/Tx9SgrDnUDTX9XMsmFSz</samp></p>\n",
       "<p>To subscribe, open: <a href=\"https://notify.run/c/Tx9SgrDnUDTX9XMsmFSz\">https://notify.run/c/Tx9SgrDnUDTX9XMsmFSz</a></p>\n",
       "<p>Or scan this QR code:</p>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"222\" width=\"222\" class=\"pyqrcode\"><path transform=\"scale(6)\" stroke=\"#000\" class=\"pyqrline\" d=\"M4 4.5h7m2 0h1m4 0h1m1 0h2m1 0h2m1 0h7m-29 1h1m5 0h1m1 0h2m1 0h3m1 0h2m1 0h1m3 0h1m5 0h1m-29 1h1m1 0h3m1 0h1m2 0h3m2 0h3m1 0h1m1 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m1 0h1m1 0h2m2 0h2m1 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m3 0h1m1 0h2m1 0h3m2 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h1m1 0h1m3 0h2m2 0h1m1 0h1m1 0h1m5 0h1m-29 1h7m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h7m-17 1h1m1 0h2m3 0h2m-21 1h5m1 0h5m1 0h1m1 0h1m1 0h1m2 0h1m1 0h1m1 0h1m1 0h1m1 0h1m-28 1h2m6 0h2m5 0h2m2 0h1m1 0h4m3 0h1m-29 1h3m2 0h3m1 0h1m1 0h8m1 0h1m2 0h2m-25 1h1m1 0h1m1 0h1m2 0h5m3 0h1m3 0h1m1 0h2m2 0h1m1 0h1m-25 1h2m1 0h4m1 0h1m2 0h1m1 0h2m7 0h2m-25 1h2m3 0h2m1 0h1m1 0h1m1 0h1m3 0h3m1 0h3m3 0h1m-28 1h1m1 0h1m2 0h1m1 0h3m2 0h1m1 0h1m1 0h1m2 0h3m1 0h3m-27 1h1m1 0h1m7 0h1m1 0h1m1 0h2m3 0h2m2 0h2m2 0h1m-28 1h1m1 0h1m1 0h7m1 0h3m1 0h2m1 0h1m1 0h1m1 0h1m1 0h2m-27 1h1m1 0h2m3 0h3m3 0h4m2 0h1m1 0h4m1 0h1m1 0h1m-29 1h1m1 0h1m1 0h3m1 0h1m1 0h4m1 0h1m1 0h2m3 0h2m2 0h1m-27 1h1m1 0h4m5 0h1m3 0h2m1 0h6m3 0h1m-28 1h1m1 0h6m1 0h3m3 0h2m2 0h6m1 0h3m-21 1h1m3 0h2m1 0h2m1 0h1m1 0h1m3 0h5m-29 1h7m1 0h3m2 0h3m3 0h2m1 0h1m1 0h3m-27 1h1m5 0h1m3 0h1m1 0h2m1 0h2m1 0h3m3 0h1m-25 1h1m1 0h3m1 0h1m1 0h1m3 0h1m1 0h2m1 0h1m2 0h5m1 0h3m-29 1h1m1 0h3m1 0h1m1 0h2m5 0h5m5 0h4m-29 1h1m1 0h3m1 0h1m1 0h2m1 0h2m2 0h3m1 0h1m2 0h6m-28 1h1m5 0h1m1 0h4m2 0h3m1 0h4m3 0h1m1 0h1m-28 1h7m1 0h4m3 0h2m2 0h1m1 0h3m2 0h1\"/></svg>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "Endpoint: https://notify.run/Tx9SgrDnUDTX9XMsmFSz\n",
       "To subscribe, open: https://notify.run/c/Tx9SgrDnUDTX9XMsmFSz\n",
       "Or scan this QR code:\n",
       "\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notify_run import Notify\n",
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6823c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.400531Z",
     "start_time": "2022-02-06T23:53:54.396944Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2902c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.406773Z",
     "start_time": "2022-02-06T23:53:54.404090Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22bd4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.414109Z",
     "start_time": "2022-02-06T23:53:54.409126Z"
    }
   },
   "outputs": [],
   "source": [
    "class ParaphraseTargetDirection(Enum):\n",
    "    org_support_to_gen_refute = 0\n",
    "    org_refute_to_gen_support = 1\n",
    "    both_majority_and_inverse_majority = 2\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(37)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5cb4bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.421693Z",
     "start_time": "2022-02-06T23:53:54.415791Z"
    }
   },
   "outputs": [],
   "source": [
    "PARAPHRASE_PROJECT_SETTINGS={\n",
    "    'file_and_dirs': {\n",
    "        'file_paraphrased_no_tune_all_model_full' : '../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl', # can be deleted\n",
    "        'file_org_claims_by_scifact' : '../../dfs_generated/scifact/org_claim_ext_label_roberta_large_fever.pkl',\n",
    "    },\n",
    "    'config_scifact' : {\n",
    "        'cls_model_name': '../../scifact/model/label_roberta_large_fever_scifact',\n",
    "        'rationale_model_name': '../../scifact/model/rationale_roberta_large_fever_scifact',\n",
    "        'loc_gold_ds_corpus' : '../../scifact/data/corpus.jsonl', \n",
    "        'loc_gold_ds_train' : '../../scifact/data/claims_train.jsonl', \n",
    "        'loc_gold_ds_dev' : '../../scifact/data/claims_dev.jsonl', \n",
    "\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'paraphrase_model' :\n",
    "    {\n",
    "        'list_potential_paraphrase_models' : \n",
    "            [\n",
    "                {'model_name' : 'parasci_base_no_fine_tune' , 'model_path_or_url' : 'HelloRusk/t5-base-parasci', 'is_selected' : False},\n",
    "                {'model_name' : 'parrot_base_no_fine_tune' , 'model_path_or_url' : 'prithivida/parrot_paraphraser_on_T5', 'is_selected' : False},\n",
    "                {'model_name' : 'parrot_div_base_no_fine_tune' , 'model_path_or_url' : 'prithivida/parrot_paraphraser_on_T5', 'is_selected' : False},\n",
    "                {'model_name' : 'pegasus_base_no_fine_tune' , 'model_path_or_url' : 'tuner007/pegasus_paraphrase', 'is_selected' : False},\n",
    "                {'model_name' : 'paws_base_no_fine_tune' , 'model_path_or_url' : 'Vamsi/T5_Paraphrase_Paws', 'is_selected' : True},\n",
    "                {'model_name' : 'tapaco_base_no_fine_tune' , 'model_path_or_url' : 'hetpandya/t5-base-tapaco', 'is_selected' : False},\n",
    "                {'model_name' : 'sci_five_pubmed' , 'model_path_or_url' : 'razent/SciFive-large-Pubmed_PMC', 'is_selected' : False}\n",
    "            ],\n",
    "        't5_paraphrase_model_params':\n",
    "        {\n",
    "            'max_length':256,\n",
    "            'do_sample':True,\n",
    "            'top_k':50,\n",
    "            'top_p': 0.99,\n",
    "            'repetition_penalty':3.5,\n",
    "            'early_stopping':True,\n",
    "            'num_return_sequences':10\n",
    "        }\n",
    "    },\n",
    "    'entailment_model':\n",
    "    {\n",
    "        'model_path' : 'pytorch/fairseq',\n",
    "        'model_name' : 'roberta.large.mnli',\n",
    "    },\n",
    "    'labels_multi_nli' :\n",
    "    {\n",
    "        0: 'contradiction', \n",
    "        1 : 'neutral', \n",
    "        2 : 'entailment'\n",
    "    },\n",
    "    \n",
    "    'run_settings':\n",
    "    {\n",
    "        'PARAPHRASE_FT_TRAIN_SPLIT' : 0.1,\n",
    "        'PARAPHRASE_FT_DATASET_DIRECTION' : ParaphraseTargetDirection.both_majority_and_inverse_majority,#ParaphraseTargetDirection.org_support_to_gen_refute,#ParaphraseTargetDirection.org_support_to_gen_refute,#ParaphraseTargetDirection.org_refute_to_gen_support,\n",
    "        'NUM_OF_EPOCH_REQ_FT' : 2,\n",
    "        'FILTER_BY' : 'TECH_TERMS',\n",
    "        'SIMILARITY_THRESHOLD' : -100\n",
    "        #'CUR_MODEL_NAME_PATHS' : (lambda: [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True])(),\n",
    "    },\n",
    "}\n",
    "\n",
    "CUR_NO_OF_EPOCH_FT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb26e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.426073Z",
     "start_time": "2022-02-06T23:53:54.423696Z"
    }
   },
   "outputs": [],
   "source": [
    "loc_df_scispacy_sentence_word_unq_ner_abr_filtered ='../../dfs_generated/linguistic/df_scispacy_sentence_word_unq_ner_abr_cust_2.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ee541",
   "metadata": {},
   "source": [
    "##  Load paraphrased dataset for selected `paraphrase_models`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e7353a",
   "metadata": {},
   "source": [
    "def gen_paraphrased_dataframe_selected_model_no_fine_tuned(df_org_claim_evid_label, paraphrase_model_name):\n",
    "    \n",
    "    dic_key_gen_sent = paraphrase_model_name+\"_sentences_info\"\n",
    "    for index_df, cur_row in tqdm(df_org_claim_evid_label.iloc[:,:].iterrows(), total=df_org_claim_evid_label.shape[0]):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim\n",
    "\n",
    "        try:\n",
    "            list_pws_sentences = get_t5_pws_gen_sentences(cur_row[\"claim\"])\n",
    "            list_dic_pws_info = []\n",
    "            for cur_pws_sent in tqdm(list_pws_sentences, desc = paraphrase_model_name):\n",
    "                cur_dic_pws_info = get_results_by_gen_claim(cur_pws_sent, dic_info_org_claim)\n",
    "                cur_dic_pws_info[\"model\"] = paraphrase_model_name\n",
    "                list_dic_pws_info.append(cur_dic_pws_info)\n",
    "            \n",
    "            cur_res[dic_key_gen_sent] = list_dic_pws_info\n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception pws genereted claim >>> \")\n",
    "            logging.info(cur_row[\"claim\"])\n",
    "            logging.info(e)\n",
    "    \n",
    "    result_as_proper_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ecc8565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.432889Z",
     "start_time": "2022-02-06T23:53:54.430336Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_dataframe_all_model_no_fine_tuned():\n",
    "    return pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_paraphrased_no_tune_all_model_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3e450a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.437632Z",
     "start_time": "2022-02-06T23:53:54.434708Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_dataframe_selected_models(df_all_model_paraphrased, list_model_names):\n",
    "    return df_all_model_paraphrased[df_all_model_paraphrased['model'].isin(list_model_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f440b6",
   "metadata": {},
   "source": [
    "## Prepare dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2322136b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.442971Z",
     "start_time": "2022-02-06T23:53:54.439603Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframes_by_majority_org_claim(df_all_paraphrased_org_claim):\n",
    "    df_all_paraphrased_org_success = df_all_paraphrased_org_claim[df_all_paraphrased_org_claim['org_comment'] == 'success']\n",
    "    \n",
    "    # Select claims with majority\n",
    "    df_paraphrased_org_support_major = df_all_paraphrased_org_success[\n",
    "        df_all_paraphrased_org_success['org_count_support'] > df_all_paraphrased_org_success['org_count_refute']\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df_paraphrased_org_refute_major = df_all_paraphrased_org_success[\n",
    "        df_all_paraphrased_org_success['org_count_support'] < df_all_paraphrased_org_success['org_count_refute']\n",
    "    ]\n",
    "    \n",
    "    return df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_org_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45682cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.447259Z",
     "start_time": "2022-02-06T23:53:54.444980Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_html_tag = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be0e01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.453231Z",
     "start_time": "2022-02-06T23:53:54.449236Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_org_success):\n",
    "    count_org_claim_support_major = len(df_paraphrased_org_support_major['org_claim'].unique())\n",
    "    count_org_claim_refute_major = len(df_paraphrased_org_refute_major['org_claim'].unique())\n",
    "    count_successful_org_claim = len(df_all_paraphrased_org_success['org_claim'].unique())\n",
    "    ## Report majority\n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#0080ff\">' + 'Original Claim Stat for current SciFact model' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim with Support majority : '+str(count_org_claim_support_major)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim with Refute majority : '+str(count_org_claim_refute_major)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique successful claim : '+str(count_successful_org_claim)+'</h4>'\n",
    "    display(HTML(tmp_html_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6198d60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.458151Z",
     "start_time": "2022-02-06T23:53:54.455184Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_succesfully_attacked_claim(df_paraphrased_support_major, df_paraphrased_refute_major):\n",
    "    df_org_refute_gen_support = df_paraphrased_refute_major[\n",
    "    df_paraphrased_refute_major['gen_count_support'] > df_paraphrased_refute_major['gen_count_refute']\n",
    "    ]\n",
    "\n",
    "    df_org_support_gen_refute = df_paraphrased_support_major[\n",
    "        df_paraphrased_support_major['gen_count_support'] < df_paraphrased_support_major['gen_count_refute']\n",
    "    ]\n",
    "    \n",
    "    return df_org_support_gen_refute, df_org_refute_gen_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04407ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.462698Z",
     "start_time": "2022-02-06T23:53:54.459962Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_succesfully_attacked_claim_refute_major(df_paraphrased_refute_major):\n",
    "    df_org_refute_gen_support = df_paraphrased_refute_major[\n",
    "    df_paraphrased_refute_major['gen_count_support'] > df_paraphrased_refute_major['gen_count_refute']\n",
    "    ]\n",
    "    return df_org_refute_gen_support\n",
    "        \n",
    "def get_df_succesfully_attacked_claim_support_major(df_paraphrased_support_major):\n",
    "    df_org_support_gen_refute = df_paraphrased_support_major[\n",
    "        df_paraphrased_support_major['gen_count_support'] < df_paraphrased_support_major['gen_count_refute']\n",
    "    ]\n",
    "    return df_org_support_gen_refute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15628496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.469539Z",
     "start_time": "2022-02-06T23:53:54.464668Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch):\n",
    "    \n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#0080ff\">' + 'Succesfully attacked Claim Stat ' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of toal org refute to gen support : '+str(len(df_org_refute_gen_support))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of total org support to gen refute : '+str(len(df_org_support_gen_refute))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique org refute to support : '+str(len(df_org_refute_gen_support['org_claim'].unique()))+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#0080ff\">'+'# of unique org support to refute : '+str(len(df_org_support_gen_refute['org_claim'].unique()))+'</h4>'\n",
    "    display(HTML(tmp_html_tag))\n",
    "    \n",
    "    dict_no_ft_org_gen_count = {\n",
    "    '# of toal org refute to gen support': len(df_org_refute_gen_support),\n",
    "    '# of total org support to gen refute' : len(df_org_support_gen_refute), \n",
    "    '# of unique org refute to support' : len(df_org_refute_gen_support['org_claim'].unique()),\n",
    "    '# of unique org support to refute' : len(df_org_support_gen_refute['org_claim'].unique()),\n",
    "    }\n",
    "\n",
    "    nep_run['no_ft_org_gen_count'] = dict_no_ft_org_gen_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08bafe0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.474961Z",
     "start_time": "2022-02-06T23:53:54.471242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org_sup_gen_ref'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'org SUP gen REF'.lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95e70563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.484904Z",
     "start_time": "2022-02-06T23:53:54.476497Z"
    }
   },
   "outputs": [],
   "source": [
    "def report_df_filter(df_cur_analyze_filter, name_analyzed_df ,cur_epoch):\n",
    "    \n",
    "    name_analyzed_df_formatted = name_analyzed_df.lower().replace(' ', '_')\n",
    "    num_mlnli_ent_org_gen = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment'])\n",
    "    num_mlnli_ent_gen_org = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment'])\n",
    "    num_mlnli_ent_both = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment')])\n",
    "    num_passed_ner_abr_filter_ic = len(df_cur_analyze_filter[df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True])\n",
    "    \n",
    "    num_both_ent_ner_passed = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True)\n",
    "    ])\n",
    "    #unique \n",
    "    num_unique_mlnli_ent_org_gen = len(df_cur_analyze_filter[df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment']['org_claim'].unique())\n",
    "    \n",
    "    num_unique_mlnli_ent_both = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment')]['org_claim'].unique())\n",
    "    \n",
    "    num_unique_both_ent_ner_passed = len(df_cur_analyze_filter[\n",
    "        (df_cur_analyze_filter['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['mlnli_label_gen_org'] == 'entailment') &\n",
    "        (df_cur_analyze_filter['passed_ner_abr_filter_ic'] == True)\n",
    "    ]['org_claim'].unique())\n",
    "    \n",
    "    tmp_html_tag = ''\n",
    "    tmp_html_tag += '<h3 style=\"color:#004f11\">' + 'Filtered for '+name_analyzed_df+' ' +'</h3>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_org_gen : '+str(num_mlnli_ent_org_gen)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_gen_org : '+str(num_mlnli_ent_gen_org)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# mlnli_ent_both : '+str(num_mlnli_ent_both)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# passed_ner_abr_filter_ic : '+str(num_passed_ner_abr_filter_ic)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#004f11\">'+'# both_ent_ner_passed : '+str(num_both_ent_ner_passed)+'</h4>'\n",
    "    \n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_mlnli_ent_org_gen : '+str(num_unique_mlnli_ent_org_gen)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_mlnli_ent_both : '+str(num_unique_mlnli_ent_both)+'</h4>'\n",
    "    tmp_html_tag += '<h4 style=\"color:#7700a6\">'+'# unique_both_ent_ner_passed : '+str(num_unique_both_ent_ner_passed)+'</h4>'\n",
    "    display(HTML(tmp_html_tag))\n",
    "    \n",
    "    dict_no_ft_org_gen_count = {\n",
    "    name_analyzed_df_formatted+'_mlnli_ent_org_gen': num_mlnli_ent_org_gen,\n",
    "    name_analyzed_df_formatted+'_mlnli_ent_gen_org' : num_mlnli_ent_gen_org, \n",
    "    name_analyzed_df_formatted+'_mlnli_ent_both' : num_mlnli_ent_both,\n",
    "    name_analyzed_df_formatted+'_passed_ner_abr_filter_ic' : num_passed_ner_abr_filter_ic,\n",
    "    name_analyzed_df_formatted+'_both_ent_ner_passed' : num_both_ent_ner_passed,\n",
    "    name_analyzed_df_formatted+'_unique_mlnli_ent_org_gen' : num_unique_mlnli_ent_org_gen,\n",
    "    name_analyzed_df_formatted+'_unique_mlnli_ent_both' : num_unique_mlnli_ent_both,\n",
    "    name_analyzed_df_formatted+'_unique_both_ent_ner_passed' : num_unique_both_ent_ner_passed,\n",
    "    }\n",
    "\n",
    "    nep_run['no_ft_org_gen_count'] = dict_no_ft_org_gen_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c529f",
   "metadata": {},
   "source": [
    "## Load Abbraviation NER dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fcef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-01T02:09:42.609626Z",
     "start_time": "2022-01-01T02:09:42.607487Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d95e5ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.491701Z",
     "start_time": "2022-02-06T23:53:54.486690Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered = pd.read_pickle(loc_df_scispacy_sentence_word_unq_ner_abr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2abba92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.502922Z",
     "start_time": "2022-02-06T23:53:54.495702Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text_stripped'] = df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text'].apply(lambda x: re.sub('[^a-z]+', ' ', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25108f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.532065Z",
     "start_time": "2022-02-06T23:53:54.505440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_text</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>ner_model</th>\n",
       "      <th>claim</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "      <th>org_label</th>\n",
       "      <th>list_rationales</th>\n",
       "      <th>data_source</th>\n",
       "      <th>term_in_claim</th>\n",
       "      <th>ner_text_stripped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK</td>\n",
       "      <td>GENE_OR_GENE_PRODUCT</td>\n",
       "      <td>en_ner_bionlp13cg_md</td>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "      <td>UK</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PrP</td>\n",
       "      <td>GENE_OR_GENE_PRODUCT</td>\n",
       "      <td>en_ner_bionlp13cg_md</td>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "      <td>PrP</td>\n",
       "      <td>prp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genomes</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>genomes</td>\n",
       "      <td>genomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genetic sequence</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>genetic sequence</td>\n",
       "      <td>genetic sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>variants</td>\n",
       "      <td>SO</td>\n",
       "      <td>en_ner_craft_md</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>139</td>\n",
       "      <td>147</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[In conclusion, uncommon or rare genetic varia...</td>\n",
       "      <td>dev</td>\n",
       "      <td>variants</td>\n",
       "      <td>variants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>Î²-sheet</td>\n",
       "      <td>PROTEIN</td>\n",
       "      <td>en_ner_jnlpba_md</td>\n",
       "      <td>Î²-sheet opening occurs during pleurotolysin po...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The major conformational changes in PlyB are ...</td>\n",
       "      <td>train</td>\n",
       "      <td>Î²-sheet</td>\n",
       "      <td>sheet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>pleurotolysin</td>\n",
       "      <td>PROTEIN</td>\n",
       "      <td>en_ner_jnlpba_md</td>\n",
       "      <td>Î²-sheet opening occurs during pleurotolysin po...</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The major conformational changes in PlyB are ...</td>\n",
       "      <td>train</td>\n",
       "      <td>pleurotolysin pore</td>\n",
       "      <td>pleurotolysin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>cRCT</td>\n",
       "      <td>EXPERIMENT</td>\n",
       "      <td>custom</td>\n",
       "      <td>Cost effectiveness evaluations based on cRCT d...</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[CONCLUSIONS The published cost-effectiveness ...</td>\n",
       "      <td>train</td>\n",
       "      <td>cRCT</td>\n",
       "      <td>crct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>embryonic stem cells (ESCs)</td>\n",
       "      <td>TAXON</td>\n",
       "      <td>custom</td>\n",
       "      <td>Androgenetic haploid mouse embryonic stem cell...</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Our results demonstrate that AG-haESCs can be...</td>\n",
       "      <td>train</td>\n",
       "      <td>embryonic stem cells (ESCs)</td>\n",
       "      <td>embryonic stem cells escs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>embryonic fibroblasts</td>\n",
       "      <td>EXPERIMENT</td>\n",
       "      <td>custom</td>\n",
       "      <td>Ectopic expression of Sall4, Nanog, Esrrb, and...</td>\n",
       "      <td>105</td>\n",
       "      <td>126</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Based on tetraploid complementation, we found...</td>\n",
       "      <td>train</td>\n",
       "      <td>embryonic fibroblasts</td>\n",
       "      <td>embryonic fibroblasts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1660 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ner_text             ner_label             ner_model  \\\n",
       "0                              UK  GENE_OR_GENE_PRODUCT  en_ner_bionlp13cg_md   \n",
       "1                             PrP  GENE_OR_GENE_PRODUCT  en_ner_bionlp13cg_md   \n",
       "2                         genomes                    SO       en_ner_craft_md   \n",
       "3                genetic sequence                    SO       en_ner_craft_md   \n",
       "4                        variants                    SO       en_ner_craft_md   \n",
       "...                           ...                   ...                   ...   \n",
       "1727                      Î²-sheet               PROTEIN      en_ner_jnlpba_md   \n",
       "1728                pleurotolysin               PROTEIN      en_ner_jnlpba_md   \n",
       "1729                         cRCT            EXPERIMENT                custom   \n",
       "1730  embryonic stem cells (ESCs)                 TAXON                custom   \n",
       "1731        embryonic fibroblasts            EXPERIMENT                custom   \n",
       "\n",
       "                                                  claim start_char end_char  \\\n",
       "0     1 in 5 million in UK have abnormal PrP positiv...         18       20   \n",
       "1     1 in 5 million in UK have abnormal PrP positiv...         35       38   \n",
       "2     1,000 genomes project enables mapping of genet...          6       13   \n",
       "3     1,000 genomes project enables mapping of genet...         41       57   \n",
       "4     1,000 genomes project enables mapping of genet...        139      147   \n",
       "...                                                 ...        ...      ...   \n",
       "1727  Î²-sheet opening occurs during pleurotolysin po...          0        7   \n",
       "1728  Î²-sheet opening occurs during pleurotolysin po...         30       48   \n",
       "1729  Cost effectiveness evaluations based on cRCT d...         40       44   \n",
       "1730  Androgenetic haploid mouse embryonic stem cell...         27       54   \n",
       "1731  Ectopic expression of Sall4, Nanog, Esrrb, and...        105      126   \n",
       "\n",
       "     org_label                                    list_rationales data_source  \\\n",
       "0      REFUTES  [RESULTS Of the 32,441 appendix samples 16 wer...       train   \n",
       "1      REFUTES  [RESULTS Of the 32,441 appendix samples 16 wer...       train   \n",
       "2     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "3     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "4     SUPPORTS  [In conclusion, uncommon or rare genetic varia...         dev   \n",
       "...        ...                                                ...         ...   \n",
       "1727  SUPPORTS  [The major conformational changes in PlyB are ...       train   \n",
       "1728  SUPPORTS  [The major conformational changes in PlyB are ...       train   \n",
       "1729   REFUTES  [CONCLUSIONS The published cost-effectiveness ...       train   \n",
       "1730  SUPPORTS  [Our results demonstrate that AG-haESCs can be...       train   \n",
       "1731  SUPPORTS  [Based on tetraploid complementation, we found...       train   \n",
       "\n",
       "                    term_in_claim           ner_text_stripped  \n",
       "0                              UK                          uk  \n",
       "1                             PrP                         prp  \n",
       "2                         genomes                     genomes  \n",
       "3                genetic sequence            genetic sequence  \n",
       "4                        variants                    variants  \n",
       "...                           ...                         ...  \n",
       "1727                      Î²-sheet                       sheet  \n",
       "1728           pleurotolysin pore               pleurotolysin  \n",
       "1729                         cRCT                        crct  \n",
       "1730  embryonic stem cells (ESCs)  embryonic stem cells escs   \n",
       "1731        embryonic fibroblasts       embryonic fibroblasts  \n",
       "\n",
       "[1660 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scispacy_sentence_word_unq_ner_abr_filtered"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24cd6abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:54:43.868737Z",
     "start_time": "2021-12-20T01:54:42.026813Z"
    }
   },
   "source": [
    "for cur_tmp in df_scispacy_sentence_word_unq_ner_abr_filtered['ner_text'].values:\n",
    "    print(cur_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941162a",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07b484",
   "metadata": {},
   "source": [
    "### Setting up Fine tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fecb2011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.540181Z",
     "start_time": "2022-02-06T23:53:54.535011Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_fine_tune, val_fine_tune = train_test_split(df_paraphrase_fine_tune_dataset[['org_claim', 'gen_claim']], \n",
    "#                                                   test_size=paraphraser_split_size)\n",
    "\n",
    "def get_train_test_dataset(df_to_be_splitted, split_size): \n",
    "    df_train, df_val = train_test_split(df_to_be_splitted, \n",
    "                                                      test_size=split_size)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_val.reset_index(drop=True, inplace=True)\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd7679a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.551008Z",
     "start_time": "2022-02-06T23:53:54.543173Z"
    }
   },
   "outputs": [],
   "source": [
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, target_dataframe, max_len=512, truncation=True):\n",
    "        #self.path = os.path.join(data_dir, type_path + '.csv')\n",
    "\n",
    "        self.source_column = \"org_claim\"\n",
    "        self.target_column = \"gen_claim\"\n",
    "        self.data = target_dataframe#pd.read_csv(self.path)\n",
    "        #print(self.data)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
    "\n",
    "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
    "            target = target + \" </s>\"\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8c58239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.555255Z",
     "start_time": "2022-02-06T23:53:54.552806Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, target_dataframe, args):\n",
    "    return ParaphraseDataset(tokenizer=tokenizer, target_dataframe = target_dataframe,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0af1167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.572313Z",
     "start_time": "2022-02-06T23:53:54.557287Z"
    }
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        # Calling the super constructer\n",
    "        super(T5FineTuner,self).__init__()\n",
    "\n",
    "        self.hparams.update(vars(hparams))\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None):        \n",
    "        return self.model(input_ids, attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                labels=labels,)\n",
    "     \n",
    "    def is_logger(self):\n",
    "        return True\n",
    "        #return self.trainer.proc_rank <= 0        \n",
    "    \n",
    "    def _step(self, batch):\n",
    "        labels = batch[\"target_ids\"]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100 #########\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return None#{\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]    \n",
    "    \n",
    "#     def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=None):\n",
    "#         if self.trainer.use_tpu:\n",
    "#             print('why tpu!')\n",
    "#             xm.optimizer_step(optimizer)\n",
    "#         else:\n",
    "#             print('here!')\n",
    "#             optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         self.lr_scheduler.step()    \n",
    "     \n",
    "    def optimizer_step(self, epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None,\n",
    "                       optimizer_closure=None, on_tpu=None, using_native_amp=None, using_lbfgs=None):\n",
    "        optimizer.step(closure=optimizer_closure) # remove 'closure=optimizer_closure' here\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, target_dataframe=self.hparams.df_train, args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, target_dataframe=self.hparams.df_val, args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db259cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.580456Z",
     "start_time": "2022-02-06T23:53:54.574688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            output_test_results_file = log_file_fine_tune_callback\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe0bd76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.588735Z",
     "start_time": "2022-02-06T23:53:54.582300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "class FineTuneHyperParams:\n",
    "    def __init__(self,model_name_path, num_train_epochs, df_train, df_val, df_train_val):\n",
    "        self.args_dict_fine_tune = dict(\n",
    "            #data_dir='./tmp_data/', # path for data files\n",
    "            #output_dir='./tmp_data/', # path to save the checkpoints\n",
    "            #temp_train_file_name = 'train.csv',\n",
    "            #temp_validation_file_name = 'val.csv',\n",
    "            #temp_train_val_file_name = 'all.csv',\n",
    "            df_train = df_train,\n",
    "            df_val = df_val,\n",
    "            df_train_val = df_train_val,\n",
    "            model_name_or_path= model_name_path,#'HelloRusk/t5-base-parasci',\n",
    "            tokenizer_name_or_path= model_name_path,#'HelloRusk/t5-base-parasci',\n",
    "            max_seq_length=512,\n",
    "            learning_rate=3e-4,\n",
    "            weight_decay=0.0,\n",
    "            adam_epsilon=1e-8,\n",
    "            warmup_steps=0,\n",
    "            train_batch_size=4,\n",
    "            eval_batch_size=4,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            gradient_accumulation_steps=16,\n",
    "            n_gpu=1,\n",
    "            early_stop_callback=False,\n",
    "            fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "            opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "            max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "            seed=37,\n",
    "        )\n",
    "\n",
    "        self.args_fine_tune_ns = argparse.Namespace(**self.args_dict_fine_tune)\n",
    "\n",
    "        self.checkpoint_callback_fine_tune = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=5,\n",
    "            verbose=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\")\n",
    "\n",
    "        self.train_params_fine_tune = dict(\n",
    "            accumulate_grad_batches=self.args_fine_tune_ns.gradient_accumulation_steps,\n",
    "            gpus=self.args_fine_tune_ns.n_gpu,\n",
    "            max_epochs=self.args_fine_tune_ns.num_train_epochs,\n",
    "            #early_stop_callback=False, #\n",
    "            precision=32,\n",
    "            #amp_level=self.args_fine_tune_ns.opt_level, #\n",
    "            gradient_clip_val=self.args_fine_tune_ns.max_grad_norm,\n",
    "            #logger=wandb_logger,\n",
    "            callbacks=[self.checkpoint_callback_fine_tune, LoggingCallback()],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c9407",
   "metadata": {},
   "source": [
    "## Scifact Functinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2f14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249e379e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-26T02:58:23.723272Z",
     "start_time": "2021-12-26T02:58:23.720406Z"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7258213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.594074Z",
     "start_time": "2022-02-06T23:53:54.590576Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_claim_label_from_jsonl(dataset_jsonl):\n",
    "    claim_label_list_train = []\n",
    "\n",
    "\n",
    "    for cur_claim in dataset_jsonl:\n",
    "        claim_txt = cur_claim.claim\n",
    "\n",
    "        for doc_id, evidence in cur_claim.evidence.items():\n",
    "\n",
    "            ev_doc = cur_claim.release.corpus.get_document(doc_id)\n",
    "\n",
    "            claim_label = evidence.label.name\n",
    "\n",
    "            tmp_dic = {\"claim\" : claim_txt, \"label\" : claim_label}\n",
    "\n",
    "            claim_label_list_train.append(tmp_dic)\n",
    "    return claim_label_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9e198ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:54.600377Z",
     "start_time": "2022-02-06T23:53:54.595877Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_claim_label_evidence_from_jsonl(dataset_jsonl, source):\n",
    "    claim_label_list_train = []\n",
    "\n",
    "\n",
    "    for cur_claim in dataset_jsonl:\n",
    "        claim_txt = cur_claim.claim\n",
    "\n",
    "        for doc_id, evidence in cur_claim.evidence.items():\n",
    "\n",
    "            ev_doc = claim_train.release.corpus.get_document(doc_id)\n",
    "\n",
    "            claim_label = evidence.label.name\n",
    "            \n",
    "            list_rationales = []\n",
    "            for i, sents in enumerate(evidence.rationales):\n",
    "                list_rationales = [sent for i, sent in enumerate(ev_doc.sentences) if i in sents]\n",
    "\n",
    "            tmp_dic = {\"claim\" : claim_txt, \"label\" : claim_label, \"list_rationales\" :list_rationales, \"source\" :source}\n",
    "\n",
    "            claim_label_list_train.append(tmp_dic)\n",
    "    return claim_label_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fec0f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:55.065528Z",
     "start_time": "2022-02-06T23:53:54.602078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 39: A diminished ovarian reserve does not solely indicate infertility in an a priori non-infertile population.\n",
      "\n",
      "Evidence sets:\n",
      "\n",
      "####################\n",
      "\n",
      "13497630: SUPPORTS\n",
      "Set 0:\n",
      "\t- After adjusting for age, body mass index, race, current smoking status, and recent hormonal contraceptive use, women with low AMH values (<0.7 ng/mL [n = 84]) did not have a significantly different predicted probability of conceiving by 6 cycles of attempt (65%; 95% CI, 50%-75%) compared with women (n = 579) with normal values (62%; 95% CI, 57%-66%) or by 12 cycles of attempt (84% [95% CI, 70%-91%] vs 75% [95% CI, 70%-79%], respectively).\n",
      "Set 1:\n",
      "\t- Women with high serum FSH values (>10 mIU/mL [n = 83]) did not have a significantly different predicted probability of conceiving after 6 cycles of attempt (63%; 95% CI, 50%-73%) compared with women (n = 654) with normal values (62%; 95% CI, 57%-66%) or after 12 cycles of attempt (82% [95% CI, 70%-89%] vs 75% [95% CI, 70%-78%], respectively).\n",
      "Set 2:\n",
      "\t- Women with high urinary FSH values (>11.5 mIU/mg creatinine [n = 69]) did not have a significantly different predicted probability of conceiving after 6 cycles of attempt (61%; 95% CI, 46%-74%) compared with women (n = 660) with normal values (62%; 95% CI, 58%-66%) or after 12 cycles of attempt (70% [95% CI, 54%-80%] vs 76% [95% CI, 72%-80%], respectively).\n",
      "Set 3:\n",
      "\t- Inhibin B levels (n = 737) were not associated with the probability of conceiving in a given cycle (hazard ratio per 1-pg/mL increase, 0.999; 95% CI, 0.997-1.001).\n",
      "Set 4:\n",
      "\t- Conclusions and Relevance Among women aged 30 to 44 years without a history of infertility who had been trying to conceive for 3 months or less, biomarkers indicating diminished ovarian reserve compared with normal ovarian reserve were not associated with reduced fertility.\n"
     ]
    }
   ],
   "source": [
    "ds_train = GoldDataset(PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_corpus'],\n",
    "                       PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_train'])\n",
    "claim_train = ds_train.get_claim(39)\n",
    "claim_train.pretty_print()\n",
    "\n",
    "dic_train = get_claim_label_evidence_from_jsonl(ds_train, source = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "098083b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:55.275299Z",
     "start_time": "2022-02-06T23:53:55.068002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 42: A high microerythrocyte count raises vulnerability to severe anemia in homozygous alpha (+)- thalassemia trait subjects.\n",
      "\n",
      "Evidence sets:\n",
      "\n",
      "####################\n",
      "\n",
      "18174210: REFUTES\n",
      "Set 0:\n",
      "\t- Individuals homozygous for alpha(+)-thalassaemia have microcytosis and an increased erythrocyte count.\n",
      "\t- We estimated that the haematological profile in children homozygous for alpha(+)-thalassaemia reduces the risk of SMA during acute malaria compared to children of normal genotype (relative risk 0.52; 95% confidence interval [CI] 0.24-1.12, p = 0.09).   \n",
      "\n",
      "Set 1:\n",
      "\t- CONCLUSIONS The increased erythrocyte count and microcytosis in children homozygous for alpha(+)-thalassaemia may contribute substantially to their protection against SMA.\n"
     ]
    }
   ],
   "source": [
    "ds_valid = GoldDataset(PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_corpus'],\n",
    "                       PARAPHRASE_PROJECT_SETTINGS['config_scifact']['loc_gold_ds_dev'])\n",
    "claim_valid = ds_valid.get_claim(42)\n",
    "claim_valid.pretty_print()\n",
    "\n",
    "dic_valid = get_claim_label_evidence_from_jsonl(ds_valid, source = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d35d8f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:55.283881Z",
     "start_time": "2022-02-06T23:53:55.277177Z"
    }
   },
   "outputs": [],
   "source": [
    "df_claim_evid_label = pd.concat([pd.DataFrame(dic_train), pd.DataFrame(dic_valid)], ignore_index=True)\n",
    "\n",
    "#df_claim_evid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10f77b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:55.305107Z",
     "start_time": "2022-02-06T23:53:55.287513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>list_rationales</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 in 5 million in UK have abnormal PrP positiv...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[RESULTS Of the 32,441 appendix samples 16 wer...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32% of liver transplantation programs required...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Policies requiring discontinuation of methado...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40mg/day dosage of folic acid and 2mg/day dosa...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[CONCLUSION Treatment with high doses of folic...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Although disorder severity was correlated wit...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A T helper 2 cell (Th2) environment impedes di...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Thus, in Lyn(-/-) mice, basophils and IgE aut...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Women with a higher birth weight are more like...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Increased risk of breast cancer was noted wit...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Women with a higher birth weight are more like...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[RESULTS We found that heavier birth weights w...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>aPKCz causes tumour enhancement by affecting g...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Taken together, this demonstrates that PKCÎ¶ i...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>cSMAC formation enhances weak ligand signalling.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[This conclusion was supported by experiments ...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>mTORC2 regulates intracellular cysteine levels...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[mTORC2 phosphorylates serine 26 at the cytoso...</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 claim     label  \\\n",
       "0    1 in 5 million in UK have abnormal PrP positiv...   REFUTES   \n",
       "1    32% of liver transplantation programs required...  SUPPORTS   \n",
       "2    40mg/day dosage of folic acid and 2mg/day dosa...  SUPPORTS   \n",
       "3    76-85% of people with severe mental disorder r...  SUPPORTS   \n",
       "4    A T helper 2 cell (Th2) environment impedes di...   REFUTES   \n",
       "..                                                 ...       ...   \n",
       "768  Women with a higher birth weight are more like...  SUPPORTS   \n",
       "769  Women with a higher birth weight are more like...  SUPPORTS   \n",
       "770  aPKCz causes tumour enhancement by affecting g...   REFUTES   \n",
       "771   cSMAC formation enhances weak ligand signalling.  SUPPORTS   \n",
       "772  mTORC2 regulates intracellular cysteine levels...  SUPPORTS   \n",
       "\n",
       "                                       list_rationales source  \n",
       "0    [RESULTS Of the 32,441 appendix samples 16 wer...  train  \n",
       "1    [Policies requiring discontinuation of methado...  train  \n",
       "2    [CONCLUSION Treatment with high doses of folic...  train  \n",
       "3    [Although disorder severity was correlated wit...  train  \n",
       "4    [Thus, in Lyn(-/-) mice, basophils and IgE aut...  train  \n",
       "..                                                 ...    ...  \n",
       "768  [Increased risk of breast cancer was noted wit...    dev  \n",
       "769  [RESULTS We found that heavier birth weights w...    dev  \n",
       "770  [Taken together, this demonstrates that PKCÎ¶ i...    dev  \n",
       "771  [This conclusion was supported by experiments ...    dev  \n",
       "772  [mTORC2 phosphorylates serine 26 at the cytoso...    dev  \n",
       "\n",
       "[773 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claim_evid_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed2043",
   "metadata": {},
   "source": [
    "### Scifact Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc9d048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:53:55.316051Z",
     "start_time": "2022-02-06T23:53:55.306691Z"
    }
   },
   "outputs": [],
   "source": [
    "class ArgsScifact:\n",
    "    def __init__(self, claim):\n",
    "        self.claim = claim\n",
    "        self.report_file = \"../../scifact/results/covid/report\" #not needed\n",
    "        self.n_documents = 100\n",
    "        self.rationale_selection_method = \"topk\"\n",
    "        self.output_format = \"markdown\"\n",
    "        self.rationale_threshold = 0.5\n",
    "        self.label_threshold = 0.5\n",
    "        self.keep_nei = False\n",
    "        self.full_abstract = True\n",
    "        self.verbose = True\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        ##\n",
    "class PretrainedModelsForScifact:\n",
    "    def __init__(self, args):\n",
    "        if args.device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(args.device)\n",
    "            \n",
    "        #self.rationale_selection_model = '/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/model/rationale_roberta_large_scifact'\n",
    "        self.rationale_selection_model = PARAPHRASE_PROJECT_SETTINGS['config_scifact']['rationale_model_name']\n",
    "        self.label_prediction_model = PARAPHRASE_PROJECT_SETTINGS['config_scifact']['cls_model_name']\n",
    "        self.abstract_retriever = AbstractRetriever()\n",
    "        self.rationale_selector = RationaleSelector(self.rationale_selection_model,\n",
    "                                               args.rationale_selection_method,\n",
    "                                               args.rationale_threshold,\n",
    "                                               self.device)\n",
    "        self.label_predictor = LabelPredictor(self.label_prediction_model,\n",
    "                                         args.keep_nei,\n",
    "                                         args.label_threshold,\n",
    "                                         self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1fa2fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:08.621306Z",
     "start_time": "2022-02-06T23:53:55.317875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../scifact/model/rationale_roberta_large_fever_scifact were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../../scifact/model/label_roberta_large_fever_scifact were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args_sci = ArgsScifact(\"\")\n",
    "\n",
    "pretrained_models_config = PretrainedModelsForScifact(args_sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ad4cd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:08.631428Z",
     "start_time": "2022-02-06T23:54:08.623942Z"
    }
   },
   "outputs": [],
   "source": [
    "log_failed_claim = []\n",
    "def inference(args, pretraind_models_config):\n",
    "\n",
    "    try:\n",
    "#         if args.verbose:\n",
    "#         print(\"Retrieving abstracts. inference > \", args.claim)\n",
    "        results = pretraind_models_config.abstract_retriever(args.claim, k=args.n_documents)\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'abstract retrival', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "        #print(\"abstract_retriever >> \", results)\n",
    "\n",
    "#         if args.verbose:\n",
    "#             print(\"Selecting rationales. inference > \", args.claim)\n",
    "        results = pretraind_models_config.rationale_selector(args.claim, results)\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'Rationale selection', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "#         if args.verbose:\n",
    "#             print(\"Label predictions. inference > \", args.claim)\n",
    "        results = pretraind_models_config.label_predictor(args.claim, results)\n",
    "\n",
    "        if len(results) == 0:\n",
    "            temp_dic = {'failed_in' : 'Label Prediction', 'claim': args.claim}\n",
    "            log_failed_claim.append(temp_dic)\n",
    "            return []\n",
    "        \n",
    "        results.sort(key=lambda r: r['label_confidence'], reverse=True)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"Exception :: Inference cant retrive info for >> \", args.claim)\n",
    "        print(sys.exc_info()[0])\n",
    "        print(traceback.format_exc())\n",
    "        temp_dic = {'failed_in' : sys.exc_info()[0], 'claim': args.claim}\n",
    "        log_failed_claim.append(temp_dic)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0456c634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:08.640534Z",
     "start_time": "2022-02-06T23:54:08.634175Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_result(result, full_abstract):\n",
    "    all_msg = \"\"\n",
    "    all_msg = f\"#### [{result['title']}]({result['url']}) \\n\"\n",
    "    #print(msg, file=f)\n",
    "    #all_msg = all_msg+msg\n",
    "    ev_scores = [f\"{x:0.2f}\" for x in result[\"evidence_confidence\"]]\n",
    "    ev_scores = \", \".join(ev_scores)\n",
    "    if result['label'].lower() == \"support\":\n",
    "        msg = f\"ðŸŸ© **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    elif result['label'].lower() == \"refute\":\n",
    "        msg = f\"ðŸŸ¥ **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    else:\n",
    "        msg = f\"âº **Decision** : {result['label']} (score={result['label_confidence']:0.2f}, evidence scores={ev_scores})\\n\"\n",
    "    #print(msg, file=f)\n",
    "    all_msg = all_msg+msg \n",
    "    \n",
    "    for i, line in enumerate(result[\"abstract\"]):\n",
    "        # If we're showing the full abstract, show evidence in green.\n",
    "        if full_abstract:\n",
    "            if result['label'].lower() == \"support\":\n",
    "                msg = (f\"- <span style='color:green'>{line}</span>\"\n",
    "                       if i in result[\"evidence\"]\n",
    "                       else f\"- {line}\")\n",
    "            elif result['label'].lower() == \"refute\":\n",
    "                msg = (f\"- <span style='color:red'>{line}</span>\"\n",
    "                       if i in result[\"evidence\"]\n",
    "                       else f\"- {line}\")                \n",
    "            #print(msg, file=f)\n",
    "            all_msg = all_msg+msg + \" \\n\"\n",
    "        else:\n",
    "            if i in result[\"evidence\"]:\n",
    "                msg = f\"- {line}\"\n",
    "                #print(msg, file=f)\n",
    "                all_msg = all_msg+msg + \" \\n\" \n",
    "    \n",
    "    #print(file=f)\n",
    "    #print(40 * \"-\", file=f)\n",
    "    #print(file=f)\n",
    "    all_msg = all_msg+msg \n",
    "    return all_msg + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96b77196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:08.648279Z",
     "start_time": "2022-02-06T23:54:08.642667Z"
    }
   },
   "outputs": [],
   "source": [
    "def export(args, results):\n",
    "    all_msg = \"\"\n",
    "    claim = args.claim\n",
    "    #report_file = args.report_file\n",
    "    #f = open(f\"{report_file}.md\", \"w\")\n",
    "    msg = f\"### Claim \\n > **{claim}** \\n \"\n",
    "    #print(msg, file=f)\n",
    "    #print(file=f)\n",
    "    all_msg = all_msg +msg\n",
    "    \n",
    "    #support_confs = [], refute_confs = []\n",
    "    confs = []\n",
    "    for result in results:\n",
    "        if result['label'].lower() == \"support\":\n",
    "            tmp_dic = {'label' : 'Support', 'label_confidence' : result[\"label_confidence\"], \"no_of_evidence\" : len(result['evidence_confidence'])}\n",
    "            confs.append(tmp_dic)\n",
    "        elif result['label'].lower() == \"refute\":\n",
    "            tmp_dic = {'label' : 'Refute', 'label_confidence' : -result[\"label_confidence\"], \"no_of_evidence\" : len(result['evidence_confidence'])}\n",
    "            confs.append(tmp_dic)\n",
    "        \n",
    "    \n",
    "    tpm_df = pd.DataFrame(confs)\n",
    "    #HTML(tpm_df.style.bar(align='mid', color=['#d65f5f', '#5fba7d']))\n",
    "    display(HTML(tpm_df.style.bar(subset=[\"label_confidence\"], align='mid', color=['#ffa1a1', '#bfffcf']).render()))\n",
    "    \n",
    "    msg = \"### Evidence \\n \"\n",
    "    all_msg = all_msg +msg\n",
    "    for result in results:\n",
    "        cur_msg = write_result(result, args.full_abstract)\n",
    "        all_msg = all_msg +cur_msg+\"\\n\"\n",
    "\n",
    "    return all_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfa5914e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:15.108413Z",
     "start_time": "2022-02-06T23:54:08.650634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_786fb_row0_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 100.0%, transparent 100.0%);\n",
       "}\n",
       "#T_786fb_row1_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 92.9%, transparent 92.9%);\n",
       "}\n",
       "#T_786fb_row2_col1 {\n",
       "  width: 10em;\n",
       "  height: 80%;\n",
       "  background: linear-gradient(90deg,#bfffcf 78.5%, transparent 78.5%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_786fb_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >label</th>\n",
       "      <th class=\"col_heading level0 col1\" >label_confidence</th>\n",
       "      <th class=\"col_heading level0 col2\" >no_of_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_786fb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_786fb_row0_col0\" class=\"data row0 col0\" >Support</td>\n",
       "      <td id=\"T_786fb_row0_col1\" class=\"data row0 col1\" >0.69</td>\n",
       "      <td id=\"T_786fb_row0_col2\" class=\"data row0 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_786fb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_786fb_row1_col0\" class=\"data row1 col0\" >Support</td>\n",
       "      <td id=\"T_786fb_row1_col1\" class=\"data row1 col1\" >0.64</td>\n",
       "      <td id=\"T_786fb_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_786fb_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_786fb_row2_col0\" class=\"data row2 col0\" >Support</td>\n",
       "      <td id=\"T_786fb_row2_col1\" class=\"data row2 col1\" >0.54</td>\n",
       "      <td id=\"T_786fb_row2_col2\" class=\"data row2 col2\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claim \n",
       " > **ART substantially reduces infectiveness of HIV-positive people.** \n",
       " ### Evidence \n",
       " #### [Autonomous Targeting of Infectious Superspreaders Using Engineered Transmissible Therapies](https://api.semanticscholar.org/10.1371/journal.pcbi.1002015) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.69, evidence scores=0.11, 0.06, 0.01)\n",
       "- Infectious disease treatments, both pharmaceutical and vaccine, face three universal challenges: the difficulty of targeting treatments to high-risk â€˜superspreaderâ€™ populations who drive the great majority of disease spread, behavioral barriers in the host population (such as poor compliance and risk disinhibition), and the evolution of pathogen resistance. \n",
       "- Here, we describe a proposed intervention that would overcome these challenges by capitalizing upon Therapeutic Interfering Particles (TIPs) that are engineered to replicate conditionally in the presence of the pathogen and spread between individuals â€” analogous to â€˜transmissible immunizationâ€™ that occurs with live-attenuated vaccines (but without the potential for reversion to virulence). \n",
       "- Building on analyses of HIV field data from sub-Saharan Africa, we construct a multi-scale model, beginning at the single-cell level, to predict the effect of TIPs on individual patient viral loads and ultimately population-level disease prevalence. \n",
       "- <span style='color:green'>Our results show that a TIP, engineered with properties based on a recent HIV gene-therapy trial, could stably lower HIV/AIDS prevalence by âˆ¼30-fold within 50 years and could complement current therapies.</span> \n",
       "- <span style='color:green'>In contrast, optimistic antiretroviral therapy or vaccination campaigns alone could only lower HIV/AIDS prevalence by <2-fold over 50 years.</span> \n",
       "- The TIP's efficacy arises from its exploitation of the same risk factors as the pathogen, allowing it to autonomously penetrate superspreader populations, maintain efficacy despite behavioral disinhibition, and limit viral resistance. \n",
       "- <span style='color:green'>While demonstrated here for HIV, the TIP concept could apply broadly to many viral infectious diseases and would represent a new paradigm for disease control, away from pathogen eradication but toward robust disease suppression.</span> \n",
       "- <span style='color:green'>While demonstrated here for HIV, the TIP concept could apply broadly to many viral infectious diseases and would represent a new paradigm for disease control, away from pathogen eradication but toward robust disease suppression.</span>\n",
       "\n",
       "#### [HIV: Biology to Treatment](https://api.semanticscholar.org/10.1007/978-981-32-9898-9_7) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.64, evidence scores=0.67, 0.01, 0.37)\n",
       "- AIDS is one of the most dreaded diseases of the twenty-first century caused by human immunodeficiency virus (HIV). \n",
       "- <span style='color:green'>Recently, there are reports which show decline in new infections due to better access to anti-retroviral drugs.</span> \n",
       "- <span style='color:green'>Still on a daily basis, ~2356 new HIV infections are being reported globally.</span> \n",
       "- New treatments and anti-HIV drugs are being continuously developed with the aim to control and cure AIDS. \n",
       "- The anti-HIV drugs that are in use usually target HIV entry and replication inside the host cells. \n",
       "- <span style='color:green'>However, these drugs are only partially effective in slowing the rate of HIV replication.</span> \n",
       "- Nevertheless, the virus manages to replicate at much slower rates even when anti-retroviral treatment is ongoing. \n",
       "- The HIV seropositives who are on anti-retroviral treatment for long periods of time are now developing different kinds of other complications including neuroAIDS. \n",
       "- The latest development in HIV therapy is a novel kind of bone marrow transplantation from donors who have a homozygous mutation in CCR5 gene. \n",
       "- The latest development in HIV therapy is a novel kind of bone marrow transplantation from donors who have a homozygous mutation in CCR5 gene.\n",
       "\n",
       "#### [Human Immunodeficiency Virus-Associated Diarrhea: Still an Issue in the Era of Antiretroviral Therapy](https://api.semanticscholar.org/10.1007/s10620-015-3615-y) \n",
       "ðŸŸ© **Decision** : SUPPORT (score=0.54, evidence scores=0.01, 0.99, 0.02)\n",
       "- <span style='color:green'>Over half of patients with human immunodeficiency virus (HIV) experience diarrhea that contributes negatively to quality of life and adherence to antiretroviral therapy (ART).</span> \n",
       "- Opportunistic infectious agents that cause diarrhea in patients with HIV span the array of protozoa, fungi, viruses, and bacteria. \n",
       "- <span style='color:green'>With global use of ART, the incidence of diarrhea because of opportunistic infections has decreased; however, the incidence of noninfectious diarrhea has increased.</span> \n",
       "- <span style='color:green'>The etiology of noninfectious diarrhea in patients with HIV is multifactorial and includes ART-associated diarrhea and gastrointestinal damage related to HIV infection (i.e., HIV enteropathy).</span> \n",
       "- A basic algorithm for the diagnosis of diarrhea in patients with HIV includes physical examination, a review of medical history, assessment of HIV viral load and CD4+ T cell count, stool microbiologic assessment, and endoscopic evaluation, if needed. \n",
       "- For patients with negative diagnostic results, the diagnosis of noninfectious diarrhea may be considered. \n",
       "- Pharmacologic options for the treatment of noninfectious diarrhea are primarily supportive; however, the use of many unapproved agents is based on unstudied and anecdotal information. \n",
       "- In addition, these agents can be associated with treatment-limiting adverse events (AEs), such as drugâ€“drug interactions with ART regimens, abuse liability, and additional gastrointestinal AEs. \n",
       "- Currently, crofelemer, an antisecretory agent, is the only therapy approved in the USA for the symptomatic relief of noninfectious diarrhea in patients with HIV on ART. \n",
       "- Currently, crofelemer, an antisecretory agent, is the only therapy approved in the USA for the symptomatic relief of noninfectious diarrhea in patients with HIV on ART.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claim_to_check = \"ART substantially reduces infectiveness of HIV-positive people.\"#df_claim_evid_label.iloc[18, :][\"claim\"]\n",
    "args_sci = ArgsScifact(claim_to_check)\n",
    "\n",
    "#pretrained_models_config = pretrained_models_for_scifact(args_sci)\n",
    "\n",
    "results_raw = inference(args_sci, pretrained_models_config)\n",
    "\n",
    "if results_raw!= []:\n",
    "    result_md = export(args_sci, results_raw)\n",
    "    #result_md = export(args_sci, results_raw)\n",
    "    display(Markdown(result_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83ad2623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:15.113211Z",
     "start_time": "2022-02-06T23:54:15.110472Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(pretraind_models_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56524194",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895bab2",
   "metadata": {},
   "source": [
    "### Tech term"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78c3110c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:50:57.373595Z",
     "start_time": "2022-01-12T03:50:57.366209Z"
    }
   },
   "source": [
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    print('\\n\\n\\n>>>>> ')\n",
    "    print('claim_original : ', claim_original)\n",
    "    print('claim_paraphrased : ', claim_paraphrased)\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'\\b' + re.escape(cur_term_row.ner_text) + r'\\b'\n",
    "        if cur_term_row_formatted not in claim_paraphrased:\n",
    "            print('# [ ' , cur_term_row_formatted, '  ] not found ' )\n",
    "            return False        \n",
    "#         if cur_term_row.ner_text.lower() not in claim_paraphrased.lower():\n",
    "#             return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4311b512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:15.118694Z",
     "start_time": "2022-02-06T23:54:15.115245Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29996079/match-a-whole-word-in-a-string-using-dynamic-regex\n",
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'(?<!\\S){}(?!\\S)'.format(re.escape(cur_term_row.ner_text))\n",
    "        res_num = re.findall(cur_term_row_formatted, claim_paraphrased)\n",
    "        if res_num == []:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fbe2b",
   "metadata": {},
   "source": [
    "### Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68dac4c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.297655Z",
     "start_time": "2022-02-06T23:54:15.120632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/qudratealahyratu/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (mnli): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_neg_checker_roberta = torch.hub.load(PARAPHRASE_PROJECT_SETTINGS['entailment_model']['model_path'], \n",
    "                                           PARAPHRASE_PROJECT_SETTINGS['entailment_model']['model_name'])\n",
    "\n",
    "model_neg_checker_roberta.to(device)\n",
    "#model_neg_checker_roberta.cuda()\n",
    "\n",
    "model_neg_checker_roberta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3ec0b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.304421Z",
     "start_time": "2022-02-06T23:54:31.299882Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mlnli_label(org_claim, gen_claim):    \n",
    "    tokens_sentences_org_gen = model_neg_checker_roberta.encode(org_claim, gen_claim)\n",
    "    logprobs_sentences_org_gen = model_neg_checker_roberta.predict('mnli', tokens_sentences_org_gen)      \n",
    "    cal_val_mlnli_org_gen = logprobs_sentences_org_gen.argmax(dim=1).item()\n",
    "    cal_label_mlnli_org_gen = PARAPHRASE_PROJECT_SETTINGS['labels_multi_nli'][cal_val_mlnli_org_gen]\n",
    "    \n",
    "    tokens_sentences_gen_org = model_neg_checker_roberta.encode(gen_claim, org_claim)\n",
    "    logprobs_sentences_gen_org = model_neg_checker_roberta.predict('mnli', tokens_sentences_gen_org)      \n",
    "    cal_val_mlnli_gen_org = logprobs_sentences_gen_org.argmax(dim=1).item()\n",
    "    cal_label_mlnli_gen_org = PARAPHRASE_PROJECT_SETTINGS['labels_multi_nli'][cal_val_mlnli_gen_org]    \n",
    "#     return {'val_mlnli_org_gen' : cal_val_mlnli_org_gen, \n",
    "#             'label_mlnli_org_gen': cal_label_mlnli_org_gen, \n",
    "#             'val_mlnli_gen_org': cal_val_mlnli_gen_org, \n",
    "#             'label_mlnli_gen_org': cal_label_mlnli_gen_org}\n",
    "    \n",
    "    return pd.Series([cal_val_mlnli_org_gen, cal_label_mlnli_org_gen, cal_val_mlnli_gen_org, cal_label_mlnli_gen_org])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd46de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d13eb3",
   "metadata": {},
   "source": [
    "## Apply Finetuned Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e2de9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T09:11:48.857656Z",
     "start_time": "2021-11-11T09:11:48.822407Z"
    }
   },
   "source": [
    "tokenizer_t5 = AutoTokenizer.from_pretrained(model_name_or_path)  \n",
    "_ = model_t5_fine_tuned.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffc7509b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.315014Z",
     "start_time": "2022-02-06T23:54:31.306527Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_t5_gen_sentences(org_sentence, model_t5, tokenizer_t5):\n",
    "    text =  \"paraphrase: \" + org_sentence + \" </s>\"\n",
    "\n",
    "    encoding = tokenizer_t5.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    #PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']\n",
    "    outputs = []\n",
    "    if type(model_t5) == T5ForConditionalGeneration:\n",
    "        outputs = model_t5.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['max_length'],\n",
    "            do_sample=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['do_sample'],\n",
    "            top_k=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_k'],\n",
    "            top_p=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_p'],\n",
    "            repetition_penalty=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['repetition_penalty'],\n",
    "            early_stopping=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['early_stopping'],\n",
    "            num_return_sequences=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['num_return_sequences']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        outputs = model_t5.model.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['max_length'],\n",
    "            do_sample=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['do_sample'],\n",
    "            top_k=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_k'],\n",
    "            top_p=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['top_p'],\n",
    "            repetition_penalty=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['repetition_penalty'],\n",
    "            early_stopping=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['early_stopping'],\n",
    "            num_return_sequences=PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['t5_paraphrase_model_params']['num_return_sequences']\n",
    "        )\n",
    "        \n",
    "    gen_sentences_t5 = []\n",
    "    for output in outputs:\n",
    "        line = tokenizer_t5.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "        gen_sentences_t5.append(line)\n",
    "    \n",
    "    #print('gen_sentences_t5_tapaco >> ', gen_sentences_t5_tapaco)\n",
    "    return list(set(gen_sentences_t5))#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23e2a7f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.325249Z",
     "start_time": "2022-02-06T23:54:31.317137Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stat_of_original_claim(row_org_claim):\n",
    "    claim =  row_org_claim[\"org_claim\"]\n",
    "    logging.info(\"#### \\n\\n>>> Original claim >>> \")\n",
    "    logging.info(claim)\n",
    "\n",
    "    args_sci = ArgsScifact(claim)\n",
    "    dic_info = {}\n",
    "    \n",
    "    dic_info[\"org_claim\"] = row_org_claim[\"org_claim\"]\n",
    "    dic_info[\"ground_label\"] = row_org_claim[\"ground_label\"]\n",
    "    dic_info[\"ground_list_rationales\"] = row_org_claim[\"ground_list_rationales\"]\n",
    "    dic_info[\"source\"] = row_org_claim[\"source\"]\n",
    "    dic_info[\"org_count_support\"] = 0\n",
    "    dic_info[\"org_count_refute\"] = 0\n",
    "    dic_info[\"org_list_supported_ids\"] = []\n",
    "    dic_info[\"org_list_refuted_ids\"] = []\n",
    "    dic_info[\"org_list_supported_confidence\"] = []\n",
    "    dic_info[\"org_list_refuted_confidence\"] = []\n",
    "    dic_info[\"org_list_supported_confidence_mean\"] = 0.0\n",
    "    dic_info[\"org_list_refuted_confidence_mean\"] = 0.0\n",
    "    dic_info[\"org_comment\"] = \"\"\n",
    "    \n",
    "    try:\n",
    "        results_raw_org = inference(args_sci, pretrained_models_config)  \n",
    "        \n",
    "        if results_raw_org == []:\n",
    "            dic_info[\"org_comment\"] = \"no result\"\n",
    "            \n",
    "        else:\n",
    "            list_supported_ids = [cur_result['id'] for cur_result in results_raw_org if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_ids = [cur_result['id'] for cur_result in results_raw_org if cur_result['label'] == 'REFUTE']\n",
    "            list_supported_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_org if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_org if cur_result['label'] == 'REFUTE']\n",
    "            \n",
    "            \n",
    "            dic_info[\"org_count_support\"] = len(list_supported_ids)\n",
    "            dic_info[\"org_count_refute\"] = len(list_refuted_ids)\n",
    "            dic_info[\"org_list_supported_ids\"] = list_supported_ids\n",
    "            dic_info[\"org_list_refuted_ids\"] = list_refuted_ids\n",
    "            dic_info[\"org_list_supported_confidence\"] = list_supported_label_confidence\n",
    "            dic_info[\"org_list_refuted_confidence\"] = list_refuted_label_confidence\n",
    "            if len(list_supported_label_confidence) > 0:\n",
    "                dic_info[\"org_list_supported_confidence_mean\"] = mean(list_supported_label_confidence)\n",
    "            if len(list_refuted_label_confidence) > 0:\n",
    "                dic_info[\"org_list_refuted_confidence_mean\"] = mean(list_refuted_label_confidence)\n",
    "            dic_info[\"org_comment\"] = \"success\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        dic_info[\"org_comment\"] = \"exception : \"+e\n",
    "        logging.info(\">>> Exception original claim >>> \") \n",
    "        logging.info(claim) \n",
    "        logging.info(e)\n",
    "    \n",
    "    finally:\n",
    "        return dic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b73fe3ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.338030Z",
     "start_time": "2022-02-06T23:54:31.327020Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results_by_gen_claim(gen_claim, dic_original_claim_info):\n",
    "    logging.info(\":: Generated claim :: \")\n",
    "    logging.info(gen_claim)\n",
    "    #print(gen_claim)\n",
    "    \n",
    "    args_gen = ArgsScifact(gen_claim)\n",
    "    gen_dic_info = {}\n",
    "    \n",
    "    #gen_dic_info[\"gen_claim\"] = gen_claim\n",
    "    gen_dic_info[\"gen_count_support\"] = 0\n",
    "    gen_dic_info[\"gen_count_refute\"] = 0\n",
    "    gen_dic_info[\"gen_list_supported_ids\"] = []\n",
    "    gen_dic_info[\"gen_list_refuted_ids\"] = []\n",
    "    gen_dic_info[\"gen_list_supported_confidence\"] = []\n",
    "    gen_dic_info[\"gen_list_refuted_confidence\"] = []\n",
    "    gen_dic_info[\"gen_list_supported_confidence_mean\"] = 0.0\n",
    "    gen_dic_info[\"gen_list_refuted_confidence_mean\"] = 0.0\n",
    "    gen_dic_info[\"gen_comment\"] = \"\"    \n",
    "    \n",
    "    gen_dic_info[\"common_all\"] = 0\n",
    "    gen_dic_info[\"common_support_refute\"] = 0\n",
    "    gen_dic_info[\"common_refute_support\"] = 0\n",
    "    gen_dic_info[\"common_support_support\"] = 0\n",
    "    gen_dic_info[\"common_refute_refute\"] = 0\n",
    "    \n",
    "    try:\n",
    "        results_raw_gen = inference(args_gen, pretrained_models_config)  \n",
    "\n",
    "        if results_raw_gen == []:\n",
    "            gen_dic_info[\"gen_comment\"] = \"no result\"\n",
    "            \n",
    "        else:\n",
    "            list_supported_ids = [cur_result['id'] for cur_result in results_raw_gen if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_ids = [cur_result['id'] for cur_result in results_raw_gen if cur_result['label'] == 'REFUTE']\n",
    "            list_supported_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_gen if cur_result['label'] == 'SUPPORT']\n",
    "            list_refuted_label_confidence =  [cur_result['label_confidence'] for cur_result in results_raw_gen if cur_result['label'] == 'REFUTE']\n",
    "            \n",
    "            \n",
    "            gen_dic_info[\"gen_count_support\"] = len(list_supported_ids)\n",
    "            gen_dic_info[\"gen_count_refute\"] = len(list_refuted_ids)\n",
    "            gen_dic_info[\"gen_list_supported_ids\"] = list_supported_ids\n",
    "            gen_dic_info[\"gen_list_refuted_ids\"] = list_refuted_ids\n",
    "            gen_dic_info[\"gen_list_supported_confidence\"] = list_supported_label_confidence\n",
    "            gen_dic_info[\"gen_list_refuted_confidence\"] = list_refuted_label_confidence\n",
    "            if len(list_supported_label_confidence) > 0 :\n",
    "                gen_dic_info[\"gen_list_supported_confidence_mean\"] = mean(list_supported_label_confidence)\n",
    "            if len(list_refuted_label_confidence) > 0:\n",
    "                gen_dic_info[\"gen_list_refuted_confidence_mean\"] = mean(list_refuted_label_confidence)\n",
    "            gen_dic_info[\"gen_comment\"] = \"success\"      \n",
    "            \n",
    "            \n",
    "            common_all = (set(gen_dic_info[\"gen_list_supported_ids\"]) | set(gen_dic_info[\"gen_list_refuted_ids\"])) & \\\n",
    "                (set(dic_original_claim_info[\"org_list_supported_ids\"]) | set(dic_original_claim_info[\"org_list_refuted_ids\"]))\n",
    "            \n",
    "            common_support_refute = set(dic_original_claim_info[\"org_list_supported_ids\"]) & set(gen_dic_info[\"gen_list_refuted_ids\"])\n",
    "            common_refute_support = set(dic_original_claim_info[\"org_list_refuted_ids\"]) & set(gen_dic_info[\"gen_list_supported_ids\"])\n",
    "            common_support_support = set(dic_original_claim_info[\"org_list_supported_ids\"]) & set(gen_dic_info[\"gen_list_supported_ids\"])\n",
    "            common_refute_refute = set(dic_original_claim_info[\"org_list_refuted_ids\"]) & set(gen_dic_info[\"gen_list_refuted_ids\"])\n",
    "            \n",
    "            gen_dic_info[\"common_all\"] = len(common_all)\n",
    "            gen_dic_info[\"common_support_refute\"] = len(common_support_refute)\n",
    "            gen_dic_info[\"common_refute_support\"] = len(common_refute_support)\n",
    "            gen_dic_info[\"common_support_support\"] = len(common_support_support)\n",
    "            gen_dic_info[\"common_refute_refute\"] = len(common_refute_refute)\n",
    "            \n",
    "            gen_dic_info[\"gen_comment\"] = \"success\" \n",
    "            \n",
    "    except Exception as e:\n",
    "        dic_info[\"gen_comment\"] = \"exception : \"+e\n",
    "        logging.info(\">>> Exception gen claim >>> \") \n",
    "        logging.info(claim) \n",
    "        logging.info(e)\n",
    "        \n",
    "    finally:\n",
    "        return gen_dic_info           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bb8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c483efe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.346108Z",
     "start_time": "2022-02-06T23:54:31.340103Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_no_ft_with_detail_stat(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "\n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info\n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "294c1060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.354728Z",
     "start_time": "2022-02-06T23:54:31.348167Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            list_paraphrased_claims_with_sim_threshold = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                ## Enable if need to measure similarity score                \n",
    "                #                 cur_similarity_score = get_sentence_similarity_score(model_sim_diltillroberta_base,\n",
    "                #                                                                     cur_row['org_claim'], \n",
    "                #                                                                     cur_paraphrased_sent)\n",
    "                cur_similarity_score = 1.0\n",
    "                if cur_similarity_score >= PARAPHRASE_PROJECT_SETTINGS['run_settings']['SIMILARITY_THRESHOLD']:\n",
    "                    list_paraphrased_claims_with_sim_threshold.append(cur_paraphrased_sent)\n",
    "            #Filter paraphrased sentences with tech terms\n",
    "                \n",
    "            #for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info      \n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2856f43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.362264Z",
     "start_time": "2022-02-06T23:54:31.356628Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact = pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_org_claims_by_scifact'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8b64a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:43:13.538419Z",
     "start_time": "2022-01-11T21:43:13.532482Z"
    }
   },
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9247214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d401e519",
   "metadata": {},
   "source": [
    "### Filter first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98dff6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.369221Z",
     "start_time": "2022-02-06T23:54:31.364472Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in df_dataset_to_be_paraphrased.iloc[:,:].iterrows():\n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                cur_tmp_dict = {'gen_claim' : cur_paraphrased_sent,\n",
    "                                               'model_paraphrase' : model_name_t5}\n",
    "                dict_cur_row = cur_row.to_dict()\n",
    "                cur_tmp_dict.update(dict_cur_row)\n",
    "                list_results_fine_tuned.append(cur_tmp_dict)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)      \n",
    "            \n",
    "    return pd.DataFrame(list_results_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87be8218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.374403Z",
     "start_time": "2022-02-06T23:54:31.371566Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_stat_no_ft(cur_row_org_detail_paraphrased_sent):\n",
    "    dict_cur_org_claim = cur_row_org_detail_paraphrased_sent.to_dict()\n",
    "    cur_paraphrased_sent = cur_row_org_detail_paraphrased_sent['gen_claim']\n",
    "    dict_all_results = get_results_by_gen_claim(cur_paraphrased_sent, dict_cur_org_claim)\n",
    "    return pd.Series(dict_all_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cef49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.380562Z",
     "start_time": "2022-02-06T23:54:31.375964Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in df_dataset_to_be_paraphrased.iloc[:,:].iterrows():\n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                cur_tmp_dict = {'gen_claim' : cur_paraphrased_sent,\n",
    "                                               'model_paraphrase' : model_name_t5}\n",
    "                dict_cur_row = cur_row.to_dict()\n",
    "                cur_tmp_dict.update(dict_cur_row)\n",
    "                list_results_fine_tuned.append(cur_tmp_dict)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)      \n",
    "            \n",
    "    return pd.DataFrame(list_results_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4ef76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf5cff06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.384915Z",
     "start_time": "2022-02-06T23:54:31.382592Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_org_support_major[10:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0735aa00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.393395Z",
     "start_time": "2022-02-06T23:54:31.386789Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_sentence(df_dataset_to_be_paraphrased, model_t5, tokenizer_t5, model_name_t5):\n",
    "    '''\n",
    "    args:\n",
    "    df_dataset_to_be_paraphrased : Dataset, those were successfully retrived by current scifact model\n",
    "    '''\n",
    "    list_results_fine_tuned = []\n",
    "    dic_key_sentence_info = model_name_t5+\"_sentences_info\"\n",
    "    \n",
    "    for index_df, cur_row in tqdm(df_dataset_to_be_paraphrased.iloc[:,:].iterrows(), total=len(df_dataset_to_be_paraphrased)):\n",
    "        cur_res = {}\n",
    "\n",
    "        dic_info_org_claim = get_stat_of_original_claim(cur_row)\n",
    "        cur_res[\"org_claim_info\"] = dic_info_org_claim    \n",
    "        #print(cur_res)\n",
    "        \n",
    "        try:\n",
    "            list_paraphrased_claims = get_t5_gen_sentences(org_sentence = cur_row[\"org_claim\"], \n",
    "                                                          model_t5 = model_t5, tokenizer_t5 = tokenizer_t5)#get_t5_gen_sentences(cur_row[\"org_claim\"])\n",
    "            \n",
    "            list_paraphrased_claims_with_sim_threshold = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims:\n",
    "                ## Enable if need to measure similarity score                \n",
    "                #                 cur_similarity_score = get_sentence_similarity_score(model_sim_diltillroberta_base,\n",
    "                #                                                                     cur_row['org_claim'], \n",
    "                #                                                                     cur_paraphrased_sent)\n",
    "                cur_similarity_score = 1.0\n",
    "                if cur_similarity_score >= PARAPHRASE_PROJECT_SETTINGS['run_settings']['SIMILARITY_THRESHOLD']:\n",
    "                    list_paraphrased_claims_with_sim_threshold.append(cur_paraphrased_sent)\n",
    "            #Filter paraphrased sentences with tech terms\n",
    "                \n",
    "            #for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                \n",
    "            \n",
    "            list_dic_paraphrased_info = []\n",
    "            for cur_paraphrased_sent in list_paraphrased_claims_with_sim_threshold:\n",
    "                cur_dic_paraphraased_claim_info = get_results_by_gen_claim(cur_paraphrased_sent, dic_info_org_claim)\n",
    "                cur_dic_paraphraased_claim_info[\"model\"] = model_name_t5\n",
    "                \n",
    "#                 cur_dic_paraphraased_claim_info['passed_ner_abr_filter_ic'] = filter_and_replace_tech_term_paraphrased_claim(cur_paraphrased_sent, \n",
    "#                                                                                                                              dic_info_org_claim['org_claim'])\n",
    "#                 dict_mlnli_labels = get_mlnli_label(dic_info_org_claim['org_claim'], cur_paraphrased_sent)\n",
    "#                 cur_dic_paraphraased_claim_info.update(dict_mlnli_labels)\n",
    "                \n",
    "                list_dic_paraphrased_info.append(cur_dic_paraphraased_claim_info)                 \n",
    "                \n",
    "            cur_res[dic_key_sentence_info] = list_dic_paraphrased_info      \n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\">>> Exception genereted claim >>> \")\n",
    "            logging.info(cur_row[\"org_claim\"])\n",
    "            logging.info(e)     \n",
    "            print('exc : ', e)\n",
    "        #print(cur_res)\n",
    "        list_results_fine_tuned.append(cur_res)\n",
    "            \n",
    "    #print(len(list_results_fine_tuned))\n",
    "    ## Formatting dataframe\n",
    "    result_as_dict = []\n",
    "    for cur_claim in list_results_fine_tuned:\n",
    "        #print(cur_claim.keys())\n",
    "        for cur_gen_paraphrased_claim in cur_claim[dic_key_sentence_info]:\n",
    "            cur_merged_dict = {**cur_claim[\"org_claim_info\"], **cur_gen_paraphrased_claim}\n",
    "            result_as_dict.append(cur_merged_dict)\n",
    "            #print('cur_merged_dict : ', cur_merged_dict)\n",
    "    #print(len(result_as_dict))\n",
    "    return pd.DataFrame(result_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5084305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1b55b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.404066Z",
     "start_time": "2022-02-06T23:54:31.395126Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact = pd.read_pickle(PARAPHRASE_PROJECT_SETTINGS['file_and_dirs']['file_org_claims_by_scifact'])\n",
    "df_org_claims_by_scifact_support_major, df_org_claims_by_scifact_refute_major, df_org_claims_by_scifact_sci_success = get_dataframes_by_majority_org_claim(df_org_claims_by_scifact)\n",
    "df_org_claims_by_scifact_majority = pd.concat([df_org_claims_by_scifact_support_major, df_org_claims_by_scifact_refute_major], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbfaaa9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.418607Z",
     "start_time": "2022-02-06T23:54:31.406268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 13)\n",
      "(118, 13)\n"
     ]
    }
   ],
   "source": [
    "df_org_claims_by_scifact_support_major.to_csv(log_dir+'df_org_claims_by_scifact_support_major.csv')\n",
    "df_org_claims_by_scifact_refute_major.to_csv(log_dir+'df_org_claims_by_scifact_refute_major.csv')\n",
    "\n",
    "print(df_org_claims_by_scifact_support_major.shape)\n",
    "print(df_org_claims_by_scifact_refute_major.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4734052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.423707Z",
     "start_time": "2022-02-06T23:54:31.420138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../dfs_generated/paraphrased/paws/separate_t5_for_majority_tech_term_mlnli/v1/logs/'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04ebffc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T23:54:31.428817Z",
     "start_time": "2022-02-06T23:54:31.425378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load experiment setup\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84271573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T01:24:05.572807Z",
     "start_time": "2022-02-06T23:54:31.430732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#004f11\">Filtered for both dataset </h3><h4 style=\"color:#004f11\"># mlnli_ent_org_gen : 2387</h4><h4 style=\"color:#004f11\"># mlnli_ent_gen_org : 2344</h4><h4 style=\"color:#004f11\"># mlnli_ent_both : 2226</h4><h4 style=\"color:#004f11\"># passed_ner_abr_filter_ic : 1054</h4><h4 style=\"color:#004f11\"># both_ent_ner_passed : 944</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_org_gen : 322</h4><h4 style=\"color:#7700a6\"># unique_mlnli_ent_both : 320</h4><h4 style=\"color:#7700a6\"># unique_both_ent_ner_passed : 217</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:#0080ff\">Original Claim Stat for current SciFact model</h3><h4 style=\"color:#0080ff\"># of unique successful claim with Support majority : 136</h4><h4 style=\"color:#0080ff\"># of unique successful claim with Refute majority : 81</h4><h4 style=\"color:#0080ff\"># of unique successful claim : 217</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd055beb460419c94892e2279379bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/research/nlp/fact_checking/my_work/scifact/verisci/covid/abstract_retriever.py:53: UserWarning: abstract selector :: returning empty list \n",
      "  warnings.warn(\"abstract selector :: returning empty list \")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c01554db7c4e8bb4e8747a1b0f8568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/.local/lib/python3.7/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/qudratealahyratu/.local/lib/python3.7/site-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Load no-tuned scifact model\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "\n",
    "\n",
    "logging.info('### PARAPHRASE WITH NO-FINE-TUNED MODEL -> ###')\n",
    "#remove duplicates from the original dataset\n",
    "df_org_claims_by_scifact = df_org_claims_by_scifact.drop_duplicates('org_claim', keep='first')\n",
    "df_org_claims_by_scifact.to_csv(log_dir+'df_paraphrased_org_majority_unique.csv')\n",
    "#get paraphrased sentences from no-tuned model\n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_no_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_majority, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "## Pass fillter\n",
    "logging.info('### FILTER ALL WITH NO-FINE-TUNED MODEL -> ###')\n",
    "# Filter for entailment check\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "#Filter for tech terms check\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "# pass all valids to scifact\n",
    "\n",
    "df_paraphrased_filtered = df_paraphrased_selected_model_full[\n",
    "    (df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] == True) &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_paraphrased_selected_model_full['mlnli_label_gen_org'] == 'entailment')\n",
    "    \n",
    "]\n",
    "\n",
    "#Check majority\n",
    "logging.info('### GET SUPPORT or REFUTE MAJORITY FILTERED ORG WITH NO-FINE-TUNED MODEL -> ###')\n",
    "df_org_support_major_filtered, df_org_refute_major_filtered, df_all_cur_model_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_filtered)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major_filtered, df_org_refute_major_filtered, df_all_cur_model_filtered)\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat = df_org_support_major_filtered.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "df_org_refute_major_paraphrased_stat = df_org_refute_major_filtered.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "df_org_support_major_paraphrased_stat = pd.concat([df_org_support_major_filtered, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major_filtered, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "\n",
    "df_org_support_major_paraphrased_stat.to_csv(log_dir+'df_org_support_major_paraphrased_stat.csv')\n",
    "df_org_refute_major_paraphrased_stat.to_csv(log_dir+'df_org_refute_major_paraphrased_stat.csv')\n",
    "# Get successfullt attacked claims after paraphrased\n",
    "logging.info('### GET SUCCESSFULL ATTACK WITH NO-FINE-TUNED MODEL -> ###')\n",
    "df_org_support_gen_refute_stat, df_org_refute_gen_support_stat = get_df_succesfully_attacked_claim(df_org_support_major_paraphrased_stat, df_org_refute_major_paraphrased_stat)\n",
    "##report_df_succesfully_attacked_claim(df_org_support_gen_refute_stat, df_org_refute_gen_support_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "##report_df_filter(df_org_support_gen_refute_stat, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "##report_df_filter(df_org_refute_gen_support_stat, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_org_support_gen_refute_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "df_org_refute_gen_support_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "\n",
    "df_org_support_gen_refute_stat.to_csv(log_dir+'df_org_support_gen_refute_stat.csv')\n",
    "df_org_refute_gen_support_stat.to_csv(log_dir+'df_org_refute_gen_support_stat.csv')\n",
    "#report_df_succesfully_attacked_claim(df_org_support_gen_refute_stat, df_org_refute_gen_support_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "df_successful_filter_attacked = pd.concat([df_org_support_gen_refute_stat, df_org_refute_gen_support_stat], ignore_index=True)\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open(loc_project_opt_location+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_successful_filter_attacked, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b729c8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-06T23:53:48.580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:86: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding`LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch(rather, they are called on every optimization step).\n",
      "  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee56bad36d564b1a935d0094f191846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:406: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4c118737554a56a607455160cf2fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/.local/lib/python3.7/site-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabc7685157541a3bd54c868f6738025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a37735365423780c80f9374223e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qudratealahyratu/.local/lib/python3.7/site-packages/ipykernel_launcher.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa3c96343f44f92a2c65e36118824c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c5733f09014fe48158ac5074dcdee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0c633f3974494085f44e04f1e84ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797a3a51165b4799be2ef4a5b7a3ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657b86439e1e4be78043793af914b036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fbf31fa87c4e478b9b9b7f2a7423f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bdee268baf4da080875ecbd2d3b7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4950bb830a39466bb594134d4f0b8763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e091f1d768434fb02e652ad88cbe84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a91e2d3f164a22a0f5ebe7a9d7c941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4435e00c69d04c39b4991cd58d6df371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f00e6e76b1041929b57bfefa290d50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33a6f8ebd4549c195b86e43de39b859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cf1329740a4ed5b51043efb6540502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2ae5d28e6a43f49d1a4848cdb6ad22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f51c5240804a5480b1055cf09678ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff7f486fcb04d3ba2dcf625f3ea08f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba8e67650ae4761be74f2238c6f8ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18d54b31a144f47ace1ff3a30d2fdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b84b4ac3854b27b986310091c057a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54231354256843b2992a79928be946be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b670aa7e842d48a980e48807c6747013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2564d699462472c820c7feeb75f4569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0535fdc2463b44cca9c3fb00aa038593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/qudratealahyratu/anaconda3/envs/scifact/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:327: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ccd158f2394c69abee22d3cb95d4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ca6bcf114e4459ae5036d1652ca077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3542c587f5d4b33ab60f816c9160d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss was not in top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad89ee6618946d88453108c302ae5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "while(True):\n",
    "#     df_fine_tuning_dataset = None    \n",
    "#     ## Select dataset for fine-tuning ::either support major or refute major\n",
    "#     if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "#         df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_sup_to_gen_ref']\n",
    "#     elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "#         df_fine_tuning_dataset = df_successful_filter_attacked[df_successful_filter_attacked['attack_type'] == 'org_ref_to_gen_sup']\n",
    "#     else:\n",
    "#         raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "######################################## SUP INPUT #####################################\n",
    "    df_org_refute_gen_support_basic = df_org_refute_gen_support_stat[[ 'org_claim','gen_claim','attack_type']]\n",
    "    df_org_refute_gen_support_basic_inv = df_org_refute_gen_support_stat[['gen_claim', 'org_claim', 'attack_type']].rename(columns={'gen_claim': 'org_claim', 'org_claim': 'gen_claim'})\n",
    "    df_org_refute_gen_support_basic_inv['attack_type'] = 'org_ref_gen_sup_inv'\n",
    "    \n",
    "    df_org_support_gen_refute_basic = df_org_support_gen_refute_stat[[ 'org_claim','gen_claim','attack_type']]\n",
    "    df_org_support_gen_refute_basic_inv = df_org_support_gen_refute_stat[['gen_claim', 'org_claim', 'attack_type']].rename(columns={'gen_claim': 'org_claim', 'org_claim': 'gen_claim'})\n",
    "    df_org_support_gen_refute_basic_inv['attack_type'] = 'org_sup_gen_ref_inv'\n",
    "       \n",
    "    df_fine_tuning_dataset_sup_to_ref = pd.concat([df_org_support_gen_refute_basic, df_org_refute_gen_support_basic_inv], ignore_index=True)\n",
    "    df_fine_tuning_dataset_ref_to_sup = pd.concat([df_org_refute_gen_support_basic, df_org_support_gen_refute_basic_inv], ignore_index=True)\n",
    "    \n",
    "    CUR_NO_OF_EPOCH_FT += 1   \n",
    "    \n",
    "    notify.send(str(CUR_NO_OF_EPOCH_FT))\n",
    "    logging.info('### FINE TUNING MODEL with SUP MAJOR-> ###')\n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset_sup_to_ref.reset_index(drop=True, inplace=True)\n",
    "    df_fine_tuning_dataset_sup_to_ref.to_csv(log_dir+'df_fine_tuning_dataset_sup_to_ref_'+str(CUR_NO_OF_EPOCH_FT)+'.csv')\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune_sup_to_ref, df_validate_fine_tune_sup_to_ref = get_train_test_dataset(df_fine_tuning_dataset_sup_to_ref, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam_sup_to_ref = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune_sup_to_ref, \n",
    "                                             df_val = df_validate_fine_tune_sup_to_ref, df_train_val = df_fine_tuning_dataset_sup_to_ref)\n",
    "\n",
    "    model_t5_fine_tuned_sup_to_ref = T5FineTuner(fineTuneHyperParam_sup_to_ref.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune_sup_to_ref = pl.Trainer(**fineTuneHyperParam_sup_to_ref.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune_sup_to_ref.fit(model_t5_fine_tuned_sup_to_ref)        \n",
    "\n",
    "\n",
    "    # ask model to generate paraphrase\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune_sup_to_ref.model.to(device)\n",
    "\n",
    "    # Ask fine-tuned model to paraphrase\n",
    "    df_paraphrased_sup_major = get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_support_major, \n",
    "                                                                               model_t5 = model_t5_fine_tuned_sup_to_ref, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "\n",
    "    # Filter for entailment check\n",
    "    df_paraphrased_sup_major[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_sup_major.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    #Filter for tech terms check\n",
    "    df_paraphrased_sup_major['passed_ner_abr_filter_ic'] = df_paraphrased_sup_major.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "    #report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "\n",
    "    df_paraphrased_sup_major_filtered = df_paraphrased_sup_major[\n",
    "        (df_paraphrased_sup_major['passed_ner_abr_filter_ic'] == True) &\n",
    "        (df_paraphrased_sup_major['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_paraphrased_sup_major['mlnli_label_gen_org'] == 'entailment')\n",
    "    ]\n",
    "\n",
    "    #Check majority\n",
    "    logging.info('### GET SUPPORT or REFUTE MAJORITY FILTERED ORG WITH FINE-TUNED MODEL -> ###')\n",
    "    df_org_support_major_filtered, df_org_refute_major_filtered, df_all_success_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_sup_major_filtered)\n",
    "    #report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "    logging.info('### GET DETAIL and STAT of GEN WITH FINE-TUNED MODEL -> ###')\n",
    "    df_org_support_major_paraphrased_stat = df_org_support_major_filtered.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "#    #df_org_refute_major_paraphrased_stat = df_org_refute_major.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "    df_org_support_major_paraphrased_stat = pd.concat([df_paraphrased_sup_major_filtered, df_org_support_major_paraphrased_stat], axis='columns')\n",
    "    #df_org_empty_refute_major_paraphrased_stat = pd.DataFrame(columns = df_org_support_major_paraphrased_stat.columns)\n",
    "#    #df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "#report\n",
    "    # Get successfullt attacked claims after paraphrased\n",
    "    logging.info('### GET SUCCESSFULL ATTACK WITH FINE-TUNED MODEL -> ###')\n",
    "    df_org_support_gen_refute_stat= get_df_succesfully_attacked_claim_support_major(df_org_support_major_paraphrased_stat)\n",
    "#    report_df_succesfully_attacked_claim(df_org_support_gen_refute_stat, df_org_refute_gen_support_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    #report_df_filter(df_org_support_gen_refute_stat, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "    #report_df_filter(df_org_refute_gen_support_stat, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    df_org_support_gen_refute_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "    df_org_support_gen_refute_stat.to_csv(log_dir+'df_org_support_gen_refute_stat_'+str(CUR_NO_OF_EPOCH_FT)+'.csv')\n",
    "#    #df_org_refute_gen_support_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "    model_t5_fine_tuned_sup_to_ref = model_t5_fine_tuned_sup_to_ref.cpu()    \n",
    "    del model_t5_fine_tuned_sup_to_ref\n",
    "    \n",
    "################################ refute major ####################################\n",
    "\n",
    "    df_fine_tuning_dataset_ref_to_sup.reset_index(drop=True, inplace=True)\n",
    "    df_fine_tuning_dataset_ref_to_sup.to_csv(log_dir+'df_fine_tuning_dataset_ref_to_sup_'+str(CUR_NO_OF_EPOCH_FT)+'.csv')\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune_ref_to_sup, df_validate_fine_tune_ref_to_sup = get_train_test_dataset(df_fine_tuning_dataset_ref_to_sup, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam_ref_to_sup = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune_ref_to_sup, \n",
    "                                             df_val = df_validate_fine_tune_ref_to_sup, df_train_val = df_fine_tuning_dataset_ref_to_sup)\n",
    "\n",
    "    model_t5_fine_tuned_ref_to_sup = T5FineTuner(fineTuneHyperParam_ref_to_sup.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune_ref_to_sup = pl.Trainer(**fineTuneHyperParam_ref_to_sup.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune_ref_to_sup.fit(model_t5_fine_tuned_ref_to_sup)        \n",
    "\n",
    "\n",
    "    # ask model to generate paraphrase\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune_ref_to_sup.model.to(device)\n",
    "\n",
    "    # Ask fine-tuned model to paraphrase\n",
    "    df_paraphrased_ref_major = get_paraphrased_sentence_with_ft(df_dataset_to_be_paraphrased = df_org_claims_by_scifact_refute_major, \n",
    "                                                                               model_t5 = model_t5_fine_tuned_ref_to_sup, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "\n",
    "    # Filter for entailment check\n",
    "    df_paraphrased_ref_major[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_ref_major.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    #Filter for tech terms check\n",
    "    df_paraphrased_ref_major['passed_ner_abr_filter_ic'] = df_paraphrased_ref_major.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "    #report_df_filter(df_paraphrased_selected_model_full, 'both dataset' ,0)\n",
    "\n",
    "    df_paraphrased_ref_major_filtered = df_paraphrased_ref_major[\n",
    "        (df_paraphrased_ref_major['passed_ner_abr_filter_ic'] == True) &\n",
    "        (df_paraphrased_ref_major['mlnli_label_org_gen'] == 'entailment') &\n",
    "        (df_paraphrased_ref_major['mlnli_label_gen_org'] == 'entailment')\n",
    "    ]\n",
    "\n",
    "    #Check majority\n",
    "    logging.info('### GET REFUTE or SUPPORT MAJORITY FILTERED ORG WITH FINE-TUNED MODEL -> ###')\n",
    "    df_org_support_major_filtered, df_org_refute_major_filtered, df_all_success_filtered = get_dataframes_by_majority_org_claim(df_paraphrased_ref_major_filtered)\n",
    "    #report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_filtered)\n",
    "\n",
    "    logging.info('### GET DETAIL and STAT of GEN WITH FINE-TUNED MODEL -> ###')\n",
    "    #df_org_support_major_paraphrased_stat = df_org_support_major_filtered.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "    df_org_refute_major_paraphrased_stat = df_org_refute_major_filtered.iloc[:, :].progress_apply(lambda x: get_paraphrased_sentence_stat_no_ft(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\n",
    "    df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major_filtered, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "    #df_org_empty_support_major_paraphrased_stat = pd.DataFrame(columns = df_org_refute_major_paraphrased_stat.columns)\n",
    "#    #df_org_refute_major_paraphrased_stat = pd.concat([df_org_refute_major, df_org_refute_major_paraphrased_stat], axis='columns')\n",
    "\n",
    "#report\n",
    "    # Get successfullt attacked claims after paraphrased\n",
    "    logging.info('### GET SUCCESSFULL ATTACK WITH FINE-TUNED MODEL -> ###')\n",
    "    df_org_refute_gen_support_stat = get_df_succesfully_attacked_claim_refute_major(df_org_refute_major_paraphrased_stat)\n",
    "#    report_df_succesfully_attacked_claim(df_org_support_gen_refute_stat, df_org_refute_gen_support_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    #report_df_filter(df_org_support_gen_refute_stat, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "    #report_df_filter(df_org_refute_gen_support_stat, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    #df_org_support_gen_refute_stat['attack_type'] = 'org_sup_to_gen_ref'\n",
    "    df_org_refute_gen_support_stat['attack_type'] = 'org_ref_to_gen_sup'\n",
    "    df_org_refute_gen_support_stat.to_csv(log_dir+'df_org_refute_gen_support_stat_'+str(CUR_NO_OF_EPOCH_FT)+'.csv')\n",
    "    model_t5_fine_tuned_ref_to_sup = model_t5_fine_tuned_ref_to_sup.cpu()    \n",
    "    del model_t5_fine_tuned_ref_to_sup\n",
    "\n",
    "    #report_df_succesfully_attacked_claim(df_org_support_gen_refute_stat, df_org_refute_gen_support_stat, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    df_successful_filter_attacked = pd.concat([df_org_support_gen_refute_stat, df_org_refute_gen_support_stat], ignore_index=True)\n",
    "\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "    #fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "    with open(loc_project_opt_location+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_successful_filter_attacked, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d41f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31708ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "081daa27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-05T23:29:46.542327Z",
     "start_time": "2022-02-05T23:29:46.524488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_claim</th>\n",
       "      <th>gen_claim</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Adult tissue-resident macrophages are seeded b...</td>\n",
       "      <td>Adult tissue-resident macrophages are cultivat...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Adult tissue-resident macrophages are seeded b...</td>\n",
       "      <td>Adult tissue-resident macrophages are planted ...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Bariatric surgery leads to negative outcomes i...</td>\n",
       "      <td>The bariatric surgery leads to negative outcom...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Genetic deficiency of mast cells leads to decr...</td>\n",
       "      <td>Genetic deficiency of mast cells leads to decr...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>Having a main partner improves HIV outcomes.</td>\n",
       "      <td>One main partner improves HIV outcomes.</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Having a main partner worsens HIV outcomes.</td>\n",
       "      <td>Having a principal partner worsens HIV outcomes.</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Having a main partner worsens HIV outcomes.</td>\n",
       "      <td>Having a main partner worsens HIV health outco...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Having a main partner worsens HIV outcomes.</td>\n",
       "      <td>Being a main partner worsens HIV results.</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Having a main partner worsens HIV outcomes.</td>\n",
       "      <td>A main partner worsens HIV outcomes.</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Having a main partner worsens HIV outcomes.</td>\n",
       "      <td>Having a principal partner worsens the HIV out...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Immune responses in immune cells are geographi...</td>\n",
       "      <td>Immune responses in immune cells are geographi...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Incidence of sepsis has fallen substantially f...</td>\n",
       "      <td>From 2009 to 2014, the incidence of sepsis dro...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Medications to treat obesity are highly effect...</td>\n",
       "      <td>Medications for the treatment of obesity are h...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>The most prevalent adverse events to Semagluti...</td>\n",
       "      <td>The most common adverse reactions to Semagluti...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>The risk of female prisoners harming themselve...</td>\n",
       "      <td>The chance of female prisoners harming themsel...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>Dexamethasone decreases risk of postoperative ...</td>\n",
       "      <td>Dexamethasone decreases the postoperative blee...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>High levels of CRP reduces the risk of exacerb...</td>\n",
       "      <td>In chronic obstructive pulmonary disease (COPD...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>PD-1 triggering on monocytes reduces IL-10 pro...</td>\n",
       "      <td>PD-1 triggering on monocytes reduce the produc...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>PD-1 triggering on monocytes reduces IL-10 pro...</td>\n",
       "      <td>PD-1 stimulation on monocytes reduce the produ...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>PD-1 triggering on monocytes reduces IL-10 pro...</td>\n",
       "      <td>PD-1 activated on monocytes reduce the IL-10 p...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>PD-1 triggering on monocytes reduces IL-10 pro...</td>\n",
       "      <td>PD-1 - trigger on monocytes reduce increases i...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>Teaching hospitals do not provide better care ...</td>\n",
       "      <td>Teaching hospitals do not provide better care ...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>The severity of cardiac involvement in amyloid...</td>\n",
       "      <td>The severity of cardiac involvement in amyloid...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>Vitamin D deficiency is unrelated to birth wei...</td>\n",
       "      <td>Vitamin D deficiency is non-related to the bir...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>Vitamin D deficiency is unrelated to birth wei...</td>\n",
       "      <td>Vitamin D deficiency is independent of its rel...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>Vitamin D deficiency is unrelated to birth wei...</td>\n",
       "      <td>Vitamin D deficiency is not related to birth w...</td>\n",
       "      <td>org_sup_to_gen_ref</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              org_claim  \\\n",
       "66    Adult tissue-resident macrophages are seeded b...   \n",
       "67    Adult tissue-resident macrophages are seeded b...   \n",
       "160   Bariatric surgery leads to negative outcomes i...   \n",
       "371   Genetic deficiency of mast cells leads to decr...   \n",
       "400        Having a main partner improves HIV outcomes.   \n",
       "409         Having a main partner worsens HIV outcomes.   \n",
       "410         Having a main partner worsens HIV outcomes.   \n",
       "411         Having a main partner worsens HIV outcomes.   \n",
       "412         Having a main partner worsens HIV outcomes.   \n",
       "414         Having a main partner worsens HIV outcomes.   \n",
       "502   Immune responses in immune cells are geographi...   \n",
       "538   Incidence of sepsis has fallen substantially f...   \n",
       "648   Medications to treat obesity are highly effect...   \n",
       "1049  The most prevalent adverse events to Semagluti...   \n",
       "1097  The risk of female prisoners harming themselve...   \n",
       "1318  Dexamethasone decreases risk of postoperative ...   \n",
       "1381  High levels of CRP reduces the risk of exacerb...   \n",
       "1480  PD-1 triggering on monocytes reduces IL-10 pro...   \n",
       "1481  PD-1 triggering on monocytes reduces IL-10 pro...   \n",
       "1485  PD-1 triggering on monocytes reduces IL-10 pro...   \n",
       "1486  PD-1 triggering on monocytes reduces IL-10 pro...   \n",
       "1568  Teaching hospitals do not provide better care ...   \n",
       "1601  The severity of cardiac involvement in amyloid...   \n",
       "1628  Vitamin D deficiency is unrelated to birth wei...   \n",
       "1629  Vitamin D deficiency is unrelated to birth wei...   \n",
       "1630  Vitamin D deficiency is unrelated to birth wei...   \n",
       "\n",
       "                                              gen_claim         attack_type  \n",
       "66    Adult tissue-resident macrophages are cultivat...  org_sup_to_gen_ref  \n",
       "67    Adult tissue-resident macrophages are planted ...  org_sup_to_gen_ref  \n",
       "160   The bariatric surgery leads to negative outcom...  org_sup_to_gen_ref  \n",
       "371   Genetic deficiency of mast cells leads to decr...  org_sup_to_gen_ref  \n",
       "400             One main partner improves HIV outcomes.  org_sup_to_gen_ref  \n",
       "409    Having a principal partner worsens HIV outcomes.  org_sup_to_gen_ref  \n",
       "410   Having a main partner worsens HIV health outco...  org_sup_to_gen_ref  \n",
       "411           Being a main partner worsens HIV results.  org_sup_to_gen_ref  \n",
       "412                A main partner worsens HIV outcomes.  org_sup_to_gen_ref  \n",
       "414   Having a principal partner worsens the HIV out...  org_sup_to_gen_ref  \n",
       "502   Immune responses in immune cells are geographi...  org_sup_to_gen_ref  \n",
       "538   From 2009 to 2014, the incidence of sepsis dro...  org_sup_to_gen_ref  \n",
       "648   Medications for the treatment of obesity are h...  org_sup_to_gen_ref  \n",
       "1049  The most common adverse reactions to Semagluti...  org_sup_to_gen_ref  \n",
       "1097  The chance of female prisoners harming themsel...  org_sup_to_gen_ref  \n",
       "1318  Dexamethasone decreases the postoperative blee...  org_sup_to_gen_ref  \n",
       "1381  In chronic obstructive pulmonary disease (COPD...  org_sup_to_gen_ref  \n",
       "1480  PD-1 triggering on monocytes reduce the produc...  org_sup_to_gen_ref  \n",
       "1481  PD-1 stimulation on monocytes reduce the produ...  org_sup_to_gen_ref  \n",
       "1485  PD-1 activated on monocytes reduce the IL-10 p...  org_sup_to_gen_ref  \n",
       "1486  PD-1 - trigger on monocytes reduce increases i...  org_sup_to_gen_ref  \n",
       "1568  Teaching hospitals do not provide better care ...  org_sup_to_gen_ref  \n",
       "1601  The severity of cardiac involvement in amyloid...  org_sup_to_gen_ref  \n",
       "1628  Vitamin D deficiency is non-related to the bir...  org_sup_to_gen_ref  \n",
       "1629  Vitamin D deficiency is independent of its rel...  org_sup_to_gen_ref  \n",
       "1630  Vitamin D deficiency is not related to birth w...  org_sup_to_gen_ref  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org_support_gen_refute_stat[['org_claim', 'gen_claim', 'attack_type']]\n",
    "#df_org_refute_gen_support_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57ded035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-05T23:35:53.204386Z",
     "start_time": "2022-02-05T23:35:53.184708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_claim</th>\n",
       "      <th>gen_claim</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>76-85% of people with severe mental disorder h...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>In low and middle income countries, 76-85% of ...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Antimicrobial agents are less effective due to...</td>\n",
       "      <td>Antimicrobial agents are less effective due to...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Autophagy deficiency in the liver increases vu...</td>\n",
       "      <td>Autophagy deficit in the liver increases vulne...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>Birth-weight is negatively associated with bre...</td>\n",
       "      <td>Birth weight is associated with breast cancer ...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>Birth-weight is negatively associated with bre...</td>\n",
       "      <td>A negative association with breast cancer is b...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>General exercise therapy is more effective tha...</td>\n",
       "      <td>Exercise therapy is more effective at reducing...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>MafA phosphorylation enhances its ubiquitination.</td>\n",
       "      <td>MafA phosphorylation increases its ubiquitina...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>Risk-adjusted mortality rates are similar in t...</td>\n",
       "      <td>Risk-adjusteable mortality rates are similar i...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>Sepsis related mortality has remained stable b...</td>\n",
       "      <td>Between 2009-2014, mortality due to Sepsis rem...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>ALDH1 expression is associated with better bre...</td>\n",
       "      <td>Involving ALDH1 in breast cancer is associated...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>ALDH1 expression is associated with better bre...</td>\n",
       "      <td>Expression of ALDH1 is associated with improve...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>Bariatric surgery has, as they say positively ...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>Bariatric surgery has â€“ have positive effects ...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>Bariatric surgery has influenced positive affe...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Birth-weight is positively associated with bre...</td>\n",
       "      <td>Birth weight is associated with breast cancer ...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>Non-invasive ventilation use should be decreas...</td>\n",
       "      <td>If there is inadequate response to conventiona...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              org_claim  \\\n",
       "1647  76-85% of people with severe mental disorder r...   \n",
       "1649  76-85% of people with severe mental disorder r...   \n",
       "1678  Antimicrobial agents are less effective due to...   \n",
       "1703  Autophagy deficiency in the liver increases vu...   \n",
       "1730  Birth-weight is negatively associated with bre...   \n",
       "1732  Birth-weight is negatively associated with bre...   \n",
       "1915  General exercise therapy is more effective tha...   \n",
       "1981  MafA phosphorylation enhances its ubiquitination.   \n",
       "2132  Risk-adjusted mortality rates are similar in t...   \n",
       "2139  Sepsis related mortality has remained stable b...   \n",
       "2335  ALDH1 expression is associated with better bre...   \n",
       "2341  ALDH1 expression is associated with better bre...   \n",
       "2384  Bariatric surgery has a positive impact on men...   \n",
       "2385  Bariatric surgery has a positive impact on men...   \n",
       "2393  Bariatric surgery has a positive impact on men...   \n",
       "2399  Birth-weight is positively associated with bre...   \n",
       "2556  Non-invasive ventilation use should be decreas...   \n",
       "\n",
       "                                              gen_claim         attack_type  \n",
       "1647  76-85% of people with severe mental disorder h...  org_ref_to_gen_sup  \n",
       "1649  In low and middle income countries, 76-85% of ...  org_ref_to_gen_sup  \n",
       "1678  Antimicrobial agents are less effective due to...  org_ref_to_gen_sup  \n",
       "1703  Autophagy deficit in the liver increases vulne...  org_ref_to_gen_sup  \n",
       "1730  Birth weight is associated with breast cancer ...  org_ref_to_gen_sup  \n",
       "1732  A negative association with breast cancer is b...  org_ref_to_gen_sup  \n",
       "1915  Exercise therapy is more effective at reducing...  org_ref_to_gen_sup  \n",
       "1981   MafA phosphorylation increases its ubiquitina...  org_ref_to_gen_sup  \n",
       "2132  Risk-adjusteable mortality rates are similar i...  org_ref_to_gen_sup  \n",
       "2139  Between 2009-2014, mortality due to Sepsis rem...  org_ref_to_gen_sup  \n",
       "2335  Involving ALDH1 in breast cancer is associated...  org_ref_to_gen_sup  \n",
       "2341  Expression of ALDH1 is associated with improve...  org_ref_to_gen_sup  \n",
       "2384  Bariatric surgery has, as they say positively ...  org_ref_to_gen_sup  \n",
       "2385  Bariatric surgery has â€“ have positive effects ...  org_ref_to_gen_sup  \n",
       "2393  Bariatric surgery has influenced positive affe...  org_ref_to_gen_sup  \n",
       "2399  Birth weight is associated with breast cancer ...  org_ref_to_gen_sup  \n",
       "2556  If there is inadequate response to conventiona...  org_ref_to_gen_sup  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org_refute_gen_support_stat[[ 'org_claim','gen_claim','attack_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a4749f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-05T23:37:29.066943Z",
     "start_time": "2022-02-05T23:37:29.050357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_claim</th>\n",
       "      <th>gen_claim</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>76-85% of people with severe mental disorder h...</td>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>In low and middle income countries, 76-85% of ...</td>\n",
       "      <td>76-85% of people with severe mental disorder r...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Antimicrobial agents are less effective due to...</td>\n",
       "      <td>Antimicrobial agents are less effective due to...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Autophagy deficit in the liver increases vulne...</td>\n",
       "      <td>Autophagy deficiency in the liver increases vu...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>Birth weight is associated with breast cancer ...</td>\n",
       "      <td>Birth-weight is negatively associated with bre...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>A negative association with breast cancer is b...</td>\n",
       "      <td>Birth-weight is negatively associated with bre...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Exercise therapy is more effective at reducing...</td>\n",
       "      <td>General exercise therapy is more effective tha...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>MafA phosphorylation increases its ubiquitina...</td>\n",
       "      <td>MafA phosphorylation enhances its ubiquitination.</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>Risk-adjusteable mortality rates are similar i...</td>\n",
       "      <td>Risk-adjusted mortality rates are similar in t...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>Between 2009-2014, mortality due to Sepsis rem...</td>\n",
       "      <td>Sepsis related mortality has remained stable b...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>Involving ALDH1 in breast cancer is associated...</td>\n",
       "      <td>ALDH1 expression is associated with better bre...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>Expression of ALDH1 is associated with improve...</td>\n",
       "      <td>ALDH1 expression is associated with better bre...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>Bariatric surgery has, as they say positively ...</td>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>Bariatric surgery has â€“ have positive effects ...</td>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Bariatric surgery has influenced positive affe...</td>\n",
       "      <td>Bariatric surgery has a positive impact on men...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Birth weight is associated with breast cancer ...</td>\n",
       "      <td>Birth-weight is positively associated with bre...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>If there is inadequate response to conventiona...</td>\n",
       "      <td>Non-invasive ventilation use should be decreas...</td>\n",
       "      <td>org_ref_to_gen_sup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              org_claim  \\\n",
       "1647  76-85% of people with severe mental disorder h...   \n",
       "1649  In low and middle income countries, 76-85% of ...   \n",
       "1678  Antimicrobial agents are less effective due to...   \n",
       "1703  Autophagy deficit in the liver increases vulne...   \n",
       "1730  Birth weight is associated with breast cancer ...   \n",
       "1732  A negative association with breast cancer is b...   \n",
       "1915  Exercise therapy is more effective at reducing...   \n",
       "1981   MafA phosphorylation increases its ubiquitina...   \n",
       "2132  Risk-adjusteable mortality rates are similar i...   \n",
       "2139  Between 2009-2014, mortality due to Sepsis rem...   \n",
       "2335  Involving ALDH1 in breast cancer is associated...   \n",
       "2341  Expression of ALDH1 is associated with improve...   \n",
       "2384  Bariatric surgery has, as they say positively ...   \n",
       "2385  Bariatric surgery has â€“ have positive effects ...   \n",
       "2393  Bariatric surgery has influenced positive affe...   \n",
       "2399  Birth weight is associated with breast cancer ...   \n",
       "2556  If there is inadequate response to conventiona...   \n",
       "\n",
       "                                              gen_claim         attack_type  \n",
       "1647  76-85% of people with severe mental disorder r...  org_ref_to_gen_sup  \n",
       "1649  76-85% of people with severe mental disorder r...  org_ref_to_gen_sup  \n",
       "1678  Antimicrobial agents are less effective due to...  org_ref_to_gen_sup  \n",
       "1703  Autophagy deficiency in the liver increases vu...  org_ref_to_gen_sup  \n",
       "1730  Birth-weight is negatively associated with bre...  org_ref_to_gen_sup  \n",
       "1732  Birth-weight is negatively associated with bre...  org_ref_to_gen_sup  \n",
       "1915  General exercise therapy is more effective tha...  org_ref_to_gen_sup  \n",
       "1981  MafA phosphorylation enhances its ubiquitination.  org_ref_to_gen_sup  \n",
       "2132  Risk-adjusted mortality rates are similar in t...  org_ref_to_gen_sup  \n",
       "2139  Sepsis related mortality has remained stable b...  org_ref_to_gen_sup  \n",
       "2335  ALDH1 expression is associated with better bre...  org_ref_to_gen_sup  \n",
       "2341  ALDH1 expression is associated with better bre...  org_ref_to_gen_sup  \n",
       "2384  Bariatric surgery has a positive impact on men...  org_ref_to_gen_sup  \n",
       "2385  Bariatric surgery has a positive impact on men...  org_ref_to_gen_sup  \n",
       "2393  Bariatric surgery has a positive impact on men...  org_ref_to_gen_sup  \n",
       "2399  Birth-weight is positively associated with bre...  org_ref_to_gen_sup  \n",
       "2556  Non-invasive ventilation use should be decreas...  org_ref_to_gen_sup  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org_refute_gen_support_stat[['gen_claim', 'org_claim', 'attack_type']].rename(columns={'gen_claim': 'org_claim', 'org_claim': 'gen_claim'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433b785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdf497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:03:20.617682Z",
     "start_time": "2022-01-14T10:03:20.611644Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_claims_by_scifact_majority_original = df_org_claims_by_scifact_majority.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cfcbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T10:03:38.447699Z",
     "start_time": "2022-01-14T10:03:38.442884Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_org_claims_by_scifact_majority = df_org_claims_by_scifact_majority_original.iloc[200:230, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d1baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5415be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.931106Z",
     "start_time": "2022-01-14T03:19:46.909491Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all model's no_fine_tuned dataset\n",
    "#df_paraphrased_all_model_full = get_paraphrased_dataframe_all_model_no_fine_tuned()\n",
    "\n",
    "#Filter and select dataset only for the selected model\n",
    "\n",
    "paraphrase_model_path_url = [_x['model_path_or_url'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_path_url = paraphrase_model_path_url[0]\n",
    "list_paraphrase_model_names = [_x['model_name'] for  _x in PARAPHRASE_PROJECT_SETTINGS['paraphrase_model']['list_potential_paraphrase_models'] if _x['is_selected'] == True]\n",
    "paraphrase_model_name = list_paraphrase_model_names[0]\n",
    "# df_paraphrased_selected_model_full = get_paraphrased_dataframe_selected_models(df_paraphrased_all_model_full, \n",
    "#                                                                                       list_paraphrase_model_names)\n",
    "\n",
    "#df_org_claims_by_scifact= df_org_claims_by_scifact.iloc[:50, :].copy()\n",
    "df_org_support_major, df_org_refute_major, df_all_cur_model_org_success= get_dataframes_by_majority_org_claim(df_org_claims_by_scifact)\n",
    "report_dataframes_by_majority_org_claim(df_org_support_major, df_org_refute_major, df_all_cur_model_org_success)\n",
    "\n",
    "df_all_cur_model_org_success = df_all_cur_model_org_success.drop_duplicates('org_claim', keep='first') \n",
    "\n",
    "model_t5_not_fine_tuned = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_path_url)\n",
    "tokenizer_t5_not_fine_tuned = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "_ = model_t5_not_fine_tuned.to(device)\n",
    "                                                                \n",
    "df_paraphrased_selected_model_full = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success, \n",
    "                                                                               model_t5 = model_t5_not_fine_tuned, \n",
    "                                                                               tokenizer_t5 = tokenizer_t5_not_fine_tuned, \n",
    "                                                                               model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "df_paraphrased_selected_model_full['passed_ner_abr_filter_ic'] = df_paraphrased_selected_model_full.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "df_paraphrased_selected_model_full[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_selected_model_full.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "\n",
    "fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "\n",
    "with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_paraphrased_selected_model_full, fp)\n",
    "\n",
    "\n",
    "#df_paraphrased_selected_model_full = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_refute_to_gen_support_0_FT_concat_prev.pkl')\n",
    "\n",
    "\n",
    "model_t5_not_fine_tuned = model_t5_not_fine_tuned.cpu()    \n",
    "del model_t5_not_fine_tuned\n",
    "\n",
    "#df_all_cur_model_org_success = pd.DataFrame(df_all_cur_model_org_success['columns_for_org_claim'].unique(), columns = ['org_claim'])\n",
    "#split dataframe of support major and refute major\n",
    "while(True):\n",
    "    # Get paraphrased sentences with majority of Support or refute.\n",
    "    df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success= get_dataframes_by_majority_org_claim(df_paraphrased_selected_model_full)\n",
    "    report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success)\n",
    "\n",
    "\n",
    "    # Get successfullt attacked claims after paraphrased\n",
    "    df_org_support_gen_refute, df_org_refute_gen_support = get_df_succesfully_attacked_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major)\n",
    "    report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)\n",
    "\n",
    "    report_df_filter(df_org_support_gen_refute, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)\n",
    "    report_df_filter(df_org_refute_gen_support, 'org REF gen SUP' ,CUR_NO_OF_EPOCH_FT)\n",
    "    \n",
    "    \n",
    "    df_fine_tuning_dataset = None\n",
    "\n",
    "    ## Select dataset for fine-tuning ::either support major or refute major\n",
    "    if PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_support_to_gen_refute:\n",
    "        df_fine_tuning_dataset = df_org_support_gen_refute.copy()\n",
    "    elif PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'] == ParaphraseTargetDirection.org_refute_to_gen_support:\n",
    "        df_fine_tuning_dataset = df_org_refute_gen_support.copy()\n",
    "    else:\n",
    "        raise ValueError('Select a direction of fine tuning dataset')\n",
    "\n",
    "    ##Filter tech terms\n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[df_fine_tuning_dataset['passed_ner_abr_filter_ic'] == True]\n",
    "    \n",
    "    df_fine_tuning_dataset = df_fine_tuning_dataset[(df_fine_tuning_dataset['mlnli_label_org_gen'] == 'entailment') &\n",
    "                                       (df_fine_tuning_dataset['mlnli_label_gen_org'] == 'entailment')]\n",
    "    \n",
    "    ## Train model with fine-tuning dataset\n",
    "    df_fine_tuning_dataset.reset_index(drop=True, inplace=True)\n",
    "    train_split_size = PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_TRAIN_SPLIT']\n",
    "    df_train_fine_tune, df_validate_fine_tune = get_train_test_dataset(df_fine_tuning_dataset, train_split_size)\n",
    "\n",
    "    num_train_epochs = PARAPHRASE_PROJECT_SETTINGS['run_settings']['NUM_OF_EPOCH_REQ_FT']\n",
    "    fineTuneHyperParam = FineTuneHyperParams(model_name_path = paraphrase_model_path_url, \n",
    "                                             num_train_epochs = num_train_epochs, df_train = df_train_fine_tune, \n",
    "                                             df_val = df_validate_fine_tune, df_train_val = df_fine_tuning_dataset)\n",
    "\n",
    "    model_t5_fine_tuned = T5FineTuner(fineTuneHyperParam.args_fine_tune_ns)\n",
    "    trainer_model_t5_fine_tune = pl.Trainer(**fineTuneHyperParam.train_params_fine_tune)\n",
    "    trainer_model_t5_fine_tune.fit(model_t5_fine_tuned)\n",
    "\n",
    "    CUR_NO_OF_EPOCH_FT += 1\n",
    "    tokenizer_t5 = AutoTokenizer.from_pretrained(paraphrase_model_path_url)  \n",
    "    _ = trainer_model_t5_fine_tune.model.to(device)\n",
    "    ## Ask fine-tuned model to paraphrase\n",
    "\n",
    "    ## Evaluate paraphrased dataset\n",
    "    df_paraphrased_fine_tuned = get_paraphrased_sentence_with_detail_stat(df_dataset_to_be_paraphrased = df_all_cur_model_org_success.iloc[:, :], \n",
    "                                              model_t5 = model_t5_fine_tuned, \n",
    "                                              tokenizer_t5 = tokenizer_t5, \n",
    "                                              model_name_t5 = paraphrase_model_name)\n",
    "\n",
    "    \n",
    "    df_paraphrased_fine_tuned['passed_ner_abr_filter_ic'] = df_paraphrased_fine_tuned.apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)\n",
    "\n",
    "\n",
    "    df_paraphrased_fine_tuned[['mlnli_val_org_gen', 'mlnli_label_org_gen', 'mlnli_val_gen_org', 'mlnli_label_gen_org']] = df_paraphrased_fine_tuned.apply(lambda cur_row : get_mlnli_label (cur_row['org_claim'], cur_row['gen_claim']), axis=1)\n",
    "\n",
    "    fle_dataframe_to_save = paraphrase_model_name+'_'+str(PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION'])+'_'+str(CUR_NO_OF_EPOCH_FT)+'_FT'\n",
    "    #list_paraphrase_model_names[0]+'_'+PARAPHRASE_PROJECT_SETTINGS['run_settings']['PARAPHRASE_FT_DATASET_DIRECTION']+'_'+NUM_OF_EPOCH_REQ_FT\n",
    "    with open('../../dfs_generated/paraphrased/paws/tech_term_ner_mlnli/'+fle_dataframe_to_save+'_concat_prev.pkl', 'wb') as fp:\n",
    "        pickle.dump(df_paraphrased_fine_tuned, fp)\n",
    "    \n",
    "    print('>> before epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))\n",
    "    df_paraphrased_selected_model_full = pd.concat([df_paraphrased_selected_model_full, df_paraphrased_fine_tuned], ignore_index=True)\n",
    "    print('>> after epoch '+str(CUR_NO_OF_EPOCH_FT)+' size was '+str(len(df_paraphrased_selected_model_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf8599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a37623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477823b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11b144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1b86f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.933318Z",
     "start_time": "2022-01-14T03:19:46.147Z"
    }
   },
   "outputs": [],
   "source": [
    "report_dataframes_by_majority_org_claim(df_paraphrased_org_support_major, df_paraphrased_org_refute_major, df_all_paraphrased_cur_model_org_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c73afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.934400Z",
     "start_time": "2022-01-14T03:19:46.154Z"
    }
   },
   "outputs": [],
   "source": [
    "report_df_succesfully_attacked_claim(df_org_support_gen_refute, df_org_refute_gen_support, cur_epoch = CUR_NO_OF_EPOCH_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740873ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.935402Z",
     "start_time": "2022-01-14T03:19:46.160Z"
    }
   },
   "outputs": [],
   "source": [
    "x = df_org_support_gen_refute.progress_apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c73b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.943110Z",
     "start_time": "2022-01-14T03:19:46.939340Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29996079/match-a-whole-word-in-a-string-using-dynamic-regex\n",
    "def filter_and_replace_tech_term_paraphrased_claim(claim_paraphrased, claim_original):\n",
    "    print('\\n>>')\n",
    "    print(claim_paraphrased)\n",
    "    print(claim_original)\n",
    "    #claim_para_trimmed = re.sub('[^a-z]+', ' ', claim_paraphrased.lower())\n",
    "    df_cur_sentence_word_unq_ner_abr_filtered = df_scispacy_sentence_word_unq_ner_abr_filtered[\n",
    "        df_scispacy_sentence_word_unq_ner_abr_filtered['claim'] == claim_original\n",
    "    ]\n",
    "    for cur_term_row in df_cur_sentence_word_unq_ner_abr_filtered.itertuples(index=False):\n",
    "        cur_term_row_formatted = r'(?<!\\S){}(?!\\S)'.format(re.escape(cur_term_row.ner_text))\n",
    "        res_num = re.findall(cur_term_row_formatted, claim_paraphrased)\n",
    "        if res_num == []:\n",
    "            print(cur_term_row_formatted.casefold())\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181660e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.952382Z",
     "start_time": "2022-01-14T03:19:46.946578Z"
    }
   },
   "outputs": [],
   "source": [
    "len(x[x == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dd60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.961543Z",
     "start_time": "2022-01-14T03:19:46.957498Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_term_row = 'HIV'\n",
    "claim_paraphrased = 'HIV Having a main partner worsens HIVd outcomes HIV .'\n",
    "cur_term_row_formatted = r'(?<!\\w){}(?!\\w)'.format(re.escape(cur_term_row.))\n",
    "re.findall(cur_term_row_formatted, claim_paraphrased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102216b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.970365Z",
     "start_time": "2022-01-14T03:19:46.964307Z"
    }
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f6eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.979047Z",
     "start_time": "2022-01-14T03:19:46.973010Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_term_row_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255a71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.987931Z",
     "start_time": "2022-01-14T03:19:46.981827Z"
    }
   },
   "outputs": [],
   "source": [
    "claim_paraphrased.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1287fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:46.989858Z",
     "start_time": "2022-01-14T03:19:46.223Z"
    }
   },
   "outputs": [],
   "source": [
    "re.escape(cur_term_row).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba84bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.005955Z",
     "start_time": "2022-01-14T03:19:46.995397Z"
    }
   },
   "outputs": [],
   "source": [
    " filter_and_replace_tech_term_paraphrased_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6434026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.020622Z",
     "start_time": "2022-01-14T03:19:47.012740Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_org_support_gen_refute[df_org_support_gen_refute['mlnli_label_org_gen'] == 'entailment'].shape)\n",
    "print(df_org_support_gen_refute[\n",
    "    (df_org_support_gen_refute['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_org_support_gen_refute['mlnli_label_gen_org'] == 'entailment')\n",
    "].shape)\n",
    "print(df_org_support_gen_refute[df_org_support_gen_refute['passed_ner_abr_filter_ic'] == True].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c3e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.023273Z",
     "start_time": "2022-01-14T03:19:46.251Z"
    }
   },
   "outputs": [],
   "source": [
    "df_org_support_gen_refute[df_org_support_gen_refute['passed_ner_abr_filter_ic'] == False][['org_claim', 'gen_claim']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f6001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.036894Z",
     "start_time": "2022-01-14T03:19:47.030096Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_org_refute_gen_support[df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment'].shape)\n",
    "\n",
    "\n",
    "print(df_org_refute_gen_support[\n",
    "    (df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment') &\n",
    "    (df_org_refute_gen_support['mlnli_label_gen_org'] == 'entailment')\n",
    "].shape)\n",
    "\n",
    "print(df_org_refute_gen_support[df_org_refute_gen_support['passed_ner_abr_filter_ic'] == True].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035f671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.038961Z",
     "start_time": "2022-01-14T03:19:46.271Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_org_refute_gen_support[df_org_refute_gen_support['mlnli_label_org_gen'] == 'entailment']['org_claim'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24448143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.049877Z",
     "start_time": "2022-01-14T03:19:47.045643Z"
    }
   },
   "outputs": [],
   "source": [
    "'IL-1Î²'.lower() == 'IL-1Î²'.casefold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a53f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:47.061752Z",
     "start_time": "2022-01-14T03:19:47.054609Z"
    }
   },
   "outputs": [],
   "source": [
    "report_df_filter(df_org_support_gen_refute, 'org SUP gen REF' ,CUR_NO_OF_EPOCH_FT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f1c1e",
   "metadata": {},
   "source": [
    "## No fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf158b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.514004Z",
     "start_time": "2022-01-14T03:19:47.262982Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769950d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.517399Z",
     "start_time": "2022-01-14T03:19:46.505Z"
    }
   },
   "outputs": [],
   "source": [
    "print(transformers.__version__)\n",
    "print(pytorch_lightning.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c037e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.518495Z",
     "start_time": "2022-01-14T03:19:46.511Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fine_tuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b449a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8bb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.519589Z",
     "start_time": "2022-01-14T03:19:46.521Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"razent/SciFive-large-Pubmed_PMC\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"razent/SciFive-large-Pubmed_PMC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fdce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.520797Z",
     "start_time": "2022-01-14T03:19:46.528Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25a90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.521932Z",
     "start_time": "2022-01-14T03:19:46.534Z"
    }
   },
   "outputs": [],
   "source": [
    "model_t5 = T5ForConditionalGeneration.from_pretrained('Vamsi/T5_Paraphrase_Paws')\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained('razent/SciFive-large-Pubmed_PMC')       \n",
    "_ = model_t5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5fa4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.522968Z",
     "start_time": "2022-01-14T03:19:46.541Z"
    }
   },
   "outputs": [],
   "source": [
    "org_sentence = 'BCL-2 promotes the apoptotic effects of c-Myc.'\n",
    "#'A T helper 2 cell (Th2) environment impedes disease development in patients with systemic lupus erythematosus (SLE)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c5d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.524106Z",
     "start_time": "2022-01-14T03:19:46.547Z"
    }
   },
   "outputs": [],
   "source": [
    "text =  \"paraphrase: \" + org_sentence \n",
    "\n",
    "encoding = tokenizer_t5.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "outputs = model_t5.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.99,\n",
    "    repetition_penalty=3.5,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "gen_sentences_t5 = []\n",
    "for output in outputs:\n",
    "    line = tokenizer_t5.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    gen_sentences_t5.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f4c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.525183Z",
     "start_time": "2022-01-14T03:19:46.553Z"
    }
   },
   "outputs": [],
   "source": [
    "set(gen_sentences_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795a617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.526342Z",
     "start_time": "2022-01-14T03:19:46.559Z"
    }
   },
   "outputs": [],
   "source": [
    "set(gen_sentences_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb98da5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.527419Z",
     "start_time": "2022-01-14T03:19:46.565Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae49a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.528598Z",
     "start_time": "2022-01-14T03:19:46.572Z"
    }
   },
   "outputs": [],
   "source": [
    "target_claim = 'BCL-2 promotes the apoptosis effects of c-myce a4pt gene.'\n",
    "org_term = 'c-Myce A4PT Gene'\n",
    "term_simple_form = org_term.lower()\n",
    "#pattern = re.compile(r'\\b{}\\b'.format(term_simple_form), re.IGNORECASE)\n",
    "regex_to_search = r'\\b(?=\\w)' + re.escape(term_simple_form) + r'\\b(?!\\w)'\n",
    "pattern = re.compile(r'\\b{}\\b'.format(regex_to_search), re.IGNORECASE)\n",
    "s2=pattern.sub(org_term,target_claim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f2302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.529773Z",
     "start_time": "2022-01-14T03:19:46.578Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71d152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.530819Z",
     "start_time": "2022-01-14T03:19:46.585Z"
    }
   },
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877a42f",
   "metadata": {},
   "source": [
    "https://blog.devgenius.io/different-ways-to-replace-occurences-of-a-substring-in-python-strings-2911b1f7bf86\n",
    "\n",
    "https://betterprogramming.pub/5-ways-to-find-the-index-of-a-substring-in-python-13d5293fc76d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6816cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.531849Z",
     "start_time": "2022-01-14T03:19:46.778Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0733787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.532917Z",
     "start_time": "2022-01-14T03:19:46.784Z"
    }
   },
   "outputs": [],
   "source": [
    "!/home/qudratealahyratu/anaconda3/envs/scifact/bin/pip install -U spacy[cuda113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868dd3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.534109Z",
     "start_time": "2022-01-14T03:19:46.791Z"
    }
   },
   "outputs": [],
   "source": [
    "import thinc\n",
    "thinc.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3503a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.535212Z",
     "start_time": "2022-01-14T03:19:46.797Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip help uninstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3090b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.536268Z",
     "start_time": "2022-01-14T03:19:46.806Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_paraphrased_selected_model_full = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_refute_to_gen_support_0_FT_concat_prev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a31ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.537545Z",
     "start_time": "2022-01-14T03:19:46.814Z"
    }
   },
   "outputs": [],
   "source": [
    "df_paraphrased_selected_model_full[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9099efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.538656Z",
     "start_time": "2022-01-14T03:19:46.821Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_pickle('../../dfs_generated/paraphrased/paws/cumulative_tech_term_ner_ic/paws_base_no_fine_tune_ParaphraseTargetDirection.org_support_to_gen_refute_0_FT_concat_prev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f472d98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.539839Z",
     "start_time": "2022-01-14T03:19:46.827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deb8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.540916Z",
     "start_time": "2022-01-14T03:19:46.836Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp['passed_ner_abr_filter_ic'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f6aadad",
   "metadata": {},
   "source": [
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Succesfully attacked Claim Stat\n",
    "# of toal org refute to gen support : 28\n",
    "# of total org support to gen refute : 31\n",
    "# of unique org refute to support : 19\n",
    "# of unique org support to refute : 22\n",
    "\n",
    "---------------------------------------------------\n",
    "Epoch 1\n",
    "Original Claim Stat for current SciFact model\n",
    "# of unique successful claim with Support majority : 204\n",
    "# of unique successful claim with Refute majority : 118\n",
    "# of unique successful claim : 340\n",
    "Succesfully attacked Claim Stat\n",
    "# of toal org refute to gen support : 197\n",
    "# of total org support to gen refute : 214\n",
    "# of unique org refute to support : 67\n",
    "# of unique org support to refute : 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec860a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.542067Z",
     "start_time": "2022-01-14T03:19:47.006Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp['passed_ner_abr_filter_ic'] = df_tmp.progress_apply(lambda x: filter_and_replace_tech_term_paraphrased_claim(x['gen_claim'], x['org_claim']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154166ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.543111Z",
     "start_time": "2022-01-14T03:19:47.013Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(df_tmp[df_tmp['passed_ner_abr_filter_ic'] == True][['org_claim', 'gen_claim']].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8e91c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.544123Z",
     "start_time": "2022-01-14T03:19:47.020Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_pickle('../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ce62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.545184Z",
     "start_time": "2022-01-14T03:19:47.027Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_pickle('../../dfs_generated/paraphrased/t5_no_fine_tune_generated_claim_all_model_df_full_1.pkl')[['org_claim', 'ground_label', 'ground_list_rationales', 'source',\n",
    "       'org_count_support', 'org_count_refute', 'org_list_supported_ids',\n",
    "       'org_list_refuted_ids', 'org_list_supported_confidence',\n",
    "       'org_list_refuted_confidence', 'org_list_supported_confidence_mean',\n",
    "       'org_list_refuted_confidence_mean', 'org_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b0cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.546274Z",
     "start_time": "2022-01-14T03:19:47.033Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp = df_tmp.drop_duplicates('org_claim', keep='first')\n",
    "df_tmp.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026d227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:19:48.547260Z",
     "start_time": "2022-01-14T03:19:47.039Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tmp.to_pickle('../../dfs_generated/scifact/org_claim_ext_roberta_roberta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8c065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scifact] *",
   "language": "python",
   "name": "conda-env-scifact-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
